{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0 Preparations\n",
    "Before starting, ensure that you have cloned the repository to your Google Drive.\n",
    "We will connect to this:"
   ],
   "id": "89efdde2f316a2ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "repository = 'evaluating_factuality_word_definitions'\n",
    "\n",
    "%cd /content/drive/My Drive/{repository}"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we install the packages and import the modules needed in this notebook:",
   "id": "6231d35ad484e964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install datasets~=2.18.0\n",
    "!pip install openai~=1.35.10"
   ],
   "id": "96b44e8e7b8c3cb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:15:57.457575Z",
     "start_time": "2024-08-19T14:15:57.450587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from pathlib import Path\n",
    "\n",
    "from fetchers.openai import OpenAiFetcher\n",
    "from general_utils.reader import JSONLineReader\n",
    "from general_utils.utils import parse_model_answer, get_openai_prediction\n",
    "from config import FACT_EVULATION_OPENAI_TOKEN, PROJECT_DIR"
   ],
   "id": "fdbe8cbeb93713f4",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 Setup: Define Datasets\n",
    "Now we define our models and datasets we want to evaluate:"
   ],
   "id": "f06ff4f131279207"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:12:20.358447Z",
     "start_time": "2024-08-19T14:12:06.120633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Datasets with language information\n",
    "datasets = {\n",
    "    'german_dpr-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german_dpr-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'german_wiktionary-claim_verification-mini': {\n",
    "        'dataset': load_dataset('lukasellinger/german_wiktionary-claim_verification-mini', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'squad-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/squad-claim_verification', split='test'),\n",
    "        'lang': 'en'\n",
    "    },\n",
    "    # optional (contains 10k entries)\n",
    "    'german_wiktionary-claim_verification-large': {\n",
    "        'dataset': load_dataset('lukasellinger/german_wiktionary-claim_verification-large', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    # outdated\n",
    "    #'german-claim_verification': {\n",
    "    #    'dataset': load_dataset('lukasellinger/german-claim_verification', split='test'),\n",
    "    #    'lang': 'de'\n",
    "    #},\n",
    "}"
   ],
   "id": "58d3ac7722cd6800",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:16:05.487194Z",
     "start_time": "2024-08-19T14:16:05.483812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [\n",
    "    'gpt-3.5-turbo',\n",
    "    'gpt-4o-mini',\n",
    "    'gpt-4o'\n",
    "]"
   ],
   "id": "ff15217b5457a60e",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:51:27.803615Z",
     "start_time": "2024-08-19T12:51:27.715101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "openai_fetcher = OpenAiFetcher(api_key=FACT_EVULATION_OPENAI_TOKEN)\n",
    "fh = JSONLineReader()"
   ],
   "id": "cc825c4d6b0397ab",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:51:29.263996Z",
     "start_time": "2024-08-19T12:51:29.257682Z"
    }
   },
   "cell_type": "code",
   "source": "EVALUATION_DIR = PROJECT_DIR / 'data/evaluation'",
   "id": "efb60ae2050d9965",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:51:29.864326Z",
     "start_time": "2024-08-19T12:51:29.841396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_task(idx, model, content):\n",
    "    return {\n",
    "            \"custom_id\": f\"task-{idx}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model,\n",
    "                \"temperature\": 0.1,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content\n",
    "                    }\n",
    "                ],\n",
    "                \"seed\": 42,\n",
    "                \"logprobs\": True,\n",
    "                \"top_logprobs\": 5\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def create_tasks(dataset, model, file_name, prompt_func):\n",
    "    tasks = [build_task(idx, model, prompt_func(entry)) for idx, entry in enumerate(dataset)]\n",
    "    fh.write(file_name, tasks)\n",
    "    \n",
    "    \n",
    "def create_long_prompt(entry):\n",
    "    return f'Please verify the following statement about {entry[\"word\"]}. Input: {entry[\"claim\"]} True or False?\\nOutput:'\n",
    "\n",
    "def create_long_translation_prompt(entry):\n",
    "    return f'Please verify the following statement about {entry[\"english_word\"]}. Input: {entry[\"english_claim\"]} True or False?\\nOutput:'\n",
    "\n",
    "\n",
    "def create_short_prompt(entry):\n",
    "    return f'Input: {entry[\"word\"]}: {entry[\"claim\"]} True or False?\\nOutput:'\n",
    "    \n",
    "    \n",
    "def process_results(file_name, dataset, translations: bool):\n",
    "    results = fh.read(file_name)\n",
    "    outputs, all_gt_labels, all_pr_labels, wiki_gt_labels, wiki_pr_labels = [], [], [], [], []\n",
    "\n",
    "    for res in results:\n",
    "        task_id = res['custom_id']\n",
    "        index = int(task_id.split('-')[-1])\n",
    "        entry = dataset[index]\n",
    "        claim = entry['claim']\n",
    "        word = entry['word']\n",
    "        predicted = get_openai_prediction(res['response']['body'])\n",
    "        if predicted == 'UNKOWN':\n",
    "            txt_answer = res['response']['body']['choices'][0]['message']['content']\n",
    "            predicted = parse_model_answer(txt_answer)\n",
    "        \n",
    "        output = {\n",
    "            'id': entry['id'],\n",
    "            'word': word,\n",
    "            'claim': claim,\n",
    "            'label': entry['label'],\n",
    "            'predicted': predicted,\n",
    "            'in_wiki': entry['in_wiki']\n",
    "        }\n",
    "        \n",
    "        if translations:\n",
    "            output['translated_word'] = entry['english_word']\n",
    "            output['translated_claim'] = entry['english_claim']\n",
    "        \n",
    "        outputs.append(output)\n",
    "        gt_label = 1 if output['label'] == 'SUPPORTED' else 0\n",
    "        pr_label = 1 if output['predicted'] == 'SUPPORTED' else 0\n",
    "        all_gt_labels.append(gt_label)\n",
    "        all_pr_labels.append(pr_label)\n",
    "        if output['in_wiki'] == 'Yes':\n",
    "            wiki_gt_labels.append(gt_label)\n",
    "            wiki_pr_labels.append(pr_label)\n",
    "        \n",
    "    report = classification_report(all_gt_labels, all_pr_labels, zero_division=0, digits=4)\n",
    "    wiki_report = classification_report(wiki_gt_labels, wiki_pr_labels, zero_division=0, digits=4)\n",
    "    print('Report all entries:')\n",
    "    print(report)\n",
    "    print('Filtered Report for entries with evidence in wikipedia:')\n",
    "    print(wiki_report)\n",
    "    return outputs"
   ],
   "id": "64c17942c1b6ff67",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 Manual Batch Fetching",
   "id": "14a65477f8c45bf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:55:09.232885Z",
     "start_time": "2024-08-19T12:55:09.226641Z"
    }
   },
   "cell_type": "code",
   "source": "batch_jobs = {}",
   "id": "f71f6b1a50ee0e88",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:55:44.391001Z",
     "start_time": "2024-08-19T12:55:10.502701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_file_name = str(EVALUATION_DIR / 'german_wiktionary-claim_verification-large/zero_shot/input/input_zero_shot-german_wiktionary-claim_verification-large-gpt-4o-mini.jsonl')\n",
    "batch_job = openai_fetcher.create_batch_job(input_file_name)\n",
    "batch_jobs[input_file_name] = batch_job"
   ],
   "id": "751caf640f37d0df",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:03:48.069065Z",
     "start_time": "2024-08-19T14:03:47.774549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file_name, batch_job in batch_jobs.items():\n",
    "    batch_job = openai_fetcher.get_batch_update(batch_job)\n",
    "    batch_jobs[file_name] = batch_job\n",
    "    print(file_name)\n",
    "    print(batch_job)\n",
    "    print(\"_______________\")"
   ],
   "id": "8d1713d07602d940",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lukasellinger/PycharmProjects/evaluating_factuality_word_definitions/data/evaluation/german_wiktionary-claim_verification-large/zero_shot/input/input_zero_shot-german_wiktionary-claim_verification-large-gpt-4o-mini.jsonl\n",
      "Batch(id='batch_nr6EVxFa7QR9tdnRIuJdNxpu', completion_window='24h', created_at=1724072144, endpoint='/v1/chat/completions', input_file_id='file-XKxBNu4OyXVTpAe1QolEjpgM', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1724075837, error_file_id=None, errors=None, expired_at=None, expires_at=1724158544, failed_at=None, finalizing_at=1724075192, in_progress_at=1724072152, metadata=None, output_file_id='file-UXCS0Rw5orp1ePpdhWBP8wn0', request_counts=BatchRequestCounts(completed=10000, failed=0, total=10000))\n",
      "_______________\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:10:10.557233Z",
     "start_time": "2024-08-19T14:07:30.924636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file_name, batch_job in batch_jobs.items():\n",
    "    output_file_name = file_name.replace('input', 'raw_output')\n",
    "    openai_fetcher.get_batch_result(output_file_name, batch_job)"
   ],
   "id": "c72b099bc661f41",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 Zero Shot",
   "id": "5a152c88dce51794"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:51:39.056791Z",
     "start_time": "2024-08-19T12:51:39.049635Z"
    }
   },
   "cell_type": "code",
   "source": "file_base_name = str(EVALUATION_DIR / '{dataset}/zero_shot/{type}/{type}_zero_shot-{dataset}-{model}.jsonl')",
   "id": "581980b95c784bd5",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:51:44.532850Z",
     "start_time": "2024-08-19T12:51:41.621792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        create_tasks(\n",
    "            dataset,\n",
    "            model,\n",
    "            file_base_name.format(type='input', dataset=dataset_name, model=model),\n",
    "            create_long_prompt\n",
    "        )"
   ],
   "id": "27e5f48b5824d684",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.90s/it]\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once input files are created, head to 2, to manually fetch your outputs",
   "id": "e127b7d91fc33df8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:16:26.840056Z",
     "start_time": "2024-08-19T14:16:15.535363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    print(f\"{dataset_name}: _________________________\")\n",
    "    for model in models:\n",
    "        file_name = file_base_name.format(type='raw_output', dataset=dataset_name, model=model)\n",
    "        if not Path(file_name).exists():\n",
    "            continue\n",
    "        print(f\"Evaluating {dataset_name} with {model}...\")\n",
    "        outputs = process_results(file_base_name.format(type='raw_output', dataset=dataset_name, model=model), dataset, translations=False)\n",
    "        fh.write(file_base_name.format(type='output', dataset=dataset_name, model=model), outputs)"
   ],
   "id": "a50d27f53a05d16a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_dpr-claim_verification: _________________________\n",
      "Evaluating german_dpr-claim_verification with gpt-3.5-turbo...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9474    0.4286    0.5902        84\n",
      "           1     0.6308    0.9762    0.7664        84\n",
      "\n",
      "    accuracy                         0.7024       168\n",
      "   macro avg     0.7891    0.7024    0.6783       168\n",
      "weighted avg     0.7891    0.7024    0.6783       168\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9375    0.4348    0.5941        69\n",
      "           1     0.6355    0.9714    0.7684        70\n",
      "\n",
      "    accuracy                         0.7050       139\n",
      "   macro avg     0.7865    0.7031    0.6812       139\n",
      "weighted avg     0.7854    0.7050    0.6818       139\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o-mini...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9355    0.6905    0.7945        84\n",
      "           1     0.7547    0.9524    0.8421        84\n",
      "\n",
      "    accuracy                         0.8214       168\n",
      "   macro avg     0.8451    0.8214    0.8183       168\n",
      "weighted avg     0.8451    0.8214    0.8183       168\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9184    0.6522    0.7627        69\n",
      "           1     0.7333    0.9429    0.8250        70\n",
      "\n",
      "    accuracy                         0.7986       139\n",
      "   macro avg     0.8259    0.7975    0.7939       139\n",
      "weighted avg     0.8252    0.7986    0.7941       139\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9383    0.9048    0.9212        84\n",
      "           1     0.9080    0.9405    0.9240        84\n",
      "\n",
      "    accuracy                         0.9226       168\n",
      "   macro avg     0.9232    0.9226    0.9226       168\n",
      "weighted avg     0.9232    0.9226    0.9226       168\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9385    0.8841    0.9104        69\n",
      "           1     0.8919    0.9429    0.9167        70\n",
      "\n",
      "    accuracy                         0.9137       139\n",
      "   macro avg     0.9152    0.9135    0.9136       139\n",
      "weighted avg     0.9150    0.9137    0.9136       139\n",
      "\n",
      "german_wiktionary-claim_verification-mini: _________________________\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-3.5-turbo...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9545    0.2100    0.3443       100\n",
      "           1     0.5562    0.9900    0.7122       100\n",
      "\n",
      "    accuracy                         0.6000       200\n",
      "   macro avg     0.7554    0.6000    0.5282       200\n",
      "weighted avg     0.7554    0.6000    0.5282       200\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9412    0.1975    0.3265        81\n",
      "           1     0.5455    0.9873    0.7027        79\n",
      "\n",
      "    accuracy                         0.5875       160\n",
      "   macro avg     0.7433    0.5924    0.5146       160\n",
      "weighted avg     0.7458    0.5875    0.5123       160\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o-mini...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8772    0.5000    0.6369       100\n",
      "           1     0.6503    0.9300    0.7654       100\n",
      "\n",
      "    accuracy                         0.7150       200\n",
      "   macro avg     0.7638    0.7150    0.7012       200\n",
      "weighted avg     0.7638    0.7150    0.7012       200\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8542    0.5062    0.6357        81\n",
      "           1     0.6429    0.9114    0.7539        79\n",
      "\n",
      "    accuracy                         0.7063       160\n",
      "   macro avg     0.7485    0.7088    0.6948       160\n",
      "weighted avg     0.7498    0.7063    0.6941       160\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8788    0.8700    0.8744       100\n",
      "           1     0.8713    0.8800    0.8756       100\n",
      "\n",
      "    accuracy                         0.8750       200\n",
      "   macro avg     0.8750    0.8750    0.8750       200\n",
      "weighted avg     0.8750    0.8750    0.8750       200\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8537    0.8642    0.8589        81\n",
      "           1     0.8590    0.8481    0.8535        79\n",
      "\n",
      "    accuracy                         0.8562       160\n",
      "   macro avg     0.8563    0.8561    0.8562       160\n",
      "weighted avg     0.8563    0.8562    0.8562       160\n",
      "\n",
      "squad-claim_verification: _________________________\n",
      "Evaluating squad-claim_verification with gpt-3.5-turbo...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8816    0.8481    0.8645        79\n",
      "           1     0.8537    0.8861    0.8696        79\n",
      "\n",
      "    accuracy                         0.8671       158\n",
      "   macro avg     0.8676    0.8671    0.8670       158\n",
      "weighted avg     0.8676    0.8671    0.8670       158\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8852    0.8571    0.8710        63\n",
      "           1     0.8615    0.8889    0.8750        63\n",
      "\n",
      "    accuracy                         0.8730       126\n",
      "   macro avg     0.8734    0.8730    0.8730       126\n",
      "weighted avg     0.8734    0.8730    0.8730       126\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-4o-mini...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8046    0.8861    0.8434        79\n",
      "           1     0.8732    0.7848    0.8267        79\n",
      "\n",
      "    accuracy                         0.8354       158\n",
      "   macro avg     0.8389    0.8354    0.8350       158\n",
      "weighted avg     0.8389    0.8354    0.8350       158\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.8889    0.8421        63\n",
      "           1     0.8750    0.7778    0.8235        63\n",
      "\n",
      "    accuracy                         0.8333       126\n",
      "   macro avg     0.8375    0.8333    0.8328       126\n",
      "weighted avg     0.8375    0.8333    0.8328       126\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-4o...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7453    1.0000    0.8541        79\n",
      "           1     1.0000    0.6582    0.7939        79\n",
      "\n",
      "    accuracy                         0.8291       158\n",
      "   macro avg     0.8726    0.8291    0.8240       158\n",
      "weighted avg     0.8726    0.8291    0.8240       158\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7683    1.0000    0.8690        63\n",
      "           1     1.0000    0.6984    0.8224        63\n",
      "\n",
      "    accuracy                         0.8492       126\n",
      "   macro avg     0.8841    0.8492    0.8457       126\n",
      "weighted avg     0.8841    0.8492    0.8457       126\n",
      "\n",
      "german_wiktionary-claim_verification-large: _________________________\n",
      "Evaluating german_wiktionary-claim_verification-large with gpt-4o-mini...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9015    0.5420    0.6770      5000\n",
      "           1     0.6726    0.9408    0.7844      5000\n",
      "\n",
      "    accuracy                         0.7414     10000\n",
      "   macro avg     0.7871    0.7414    0.7307     10000\n",
      "weighted avg     0.7871    0.7414    0.7307     10000\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8948    0.5226    0.6598      3938\n",
      "           1     0.6600    0.9378    0.7748      3892\n",
      "\n",
      "    accuracy                         0.7290      7830\n",
      "   macro avg     0.7774    0.7302    0.7173      7830\n",
      "weighted avg     0.7781    0.7290    0.7170      7830\n",
      "\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 Ablation Translated Claims",
   "id": "b0f8f3670917df8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:05:37.526374Z",
     "start_time": "2024-08-19T11:05:37.522470Z"
    }
   },
   "cell_type": "code",
   "source": "file_base_name = str(EVALUATION_DIR / '{dataset}/ablation_translated_claims/{type}/{type}_ablation_translated_claims-{dataset}-{model}.jsonl')",
   "id": "68913ba371e2a6e1",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 19.95it/s]\n"
     ]
    }
   ],
   "execution_count": 38,
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    if config['lang'] == 'en':\n",
    "        continue\n",
    "\n",
    "    dataset = config['dataset']    \n",
    "    for model in models:\n",
    "        create_tasks(\n",
    "            dataset,\n",
    "            model,\n",
    "            file_base_name.format(type='input', dataset=dataset_name, model=model),\n",
    "            create_long_translation_prompt\n",
    "        )"
   ],
   "id": "c80afac9e66af1d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once input files are created, head to 2, to manually fetch your outputs",
   "id": "8d2c3ea8259a6a50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:07:03.185256Z",
     "start_time": "2024-08-19T11:07:01.854415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    if config['lang'] == 'en':\n",
    "        continue\n",
    "        \n",
    "    dataset = config['dataset']\n",
    "    print(f\"{dataset_name}: _________________________\")\n",
    "    for model in models:\n",
    "        print(f\"Evaluating {dataset_name} with {model}...\")\n",
    "        outputs = process_results(file_base_name.format(type='raw_output', dataset=dataset_name, model=model), dataset, translations=False)\n",
    "        fh.write(file_base_name.format(type='output', dataset=dataset_name, model=model), outputs)"
   ],
   "id": "54336d77347614db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_dpr-claim_verification: _________________________\n",
      "Evaluating german_dpr-claim_verification with gpt-3.5-turbo...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9403    0.7500    0.8344        84\n",
      "           1     0.7921    0.9524    0.8649        84\n",
      "\n",
      "    accuracy                         0.8512       168\n",
      "   macro avg     0.8662    0.8512    0.8497       168\n",
      "weighted avg     0.8662    0.8512    0.8497       168\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9259    0.7246    0.8130        69\n",
      "           1     0.7765    0.9429    0.8516        70\n",
      "\n",
      "    accuracy                         0.8345       139\n",
      "   macro avg     0.8512    0.8337    0.8323       139\n",
      "weighted avg     0.8507    0.8345    0.8324       139\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o-mini...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8706    0.8810    0.8757        84\n",
      "           1     0.8795    0.8690    0.8743        84\n",
      "\n",
      "    accuracy                         0.8750       168\n",
      "   macro avg     0.8751    0.8750    0.8750       168\n",
      "weighted avg     0.8751    0.8750    0.8750       168\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8806    0.8551    0.8676        69\n",
      "           1     0.8611    0.8857    0.8732        70\n",
      "\n",
      "    accuracy                         0.8705       139\n",
      "   macro avg     0.8709    0.8704    0.8704       139\n",
      "weighted avg     0.8708    0.8705    0.8705       139\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.9524    0.8889        84\n",
      "           1     0.9444    0.8095    0.8718        84\n",
      "\n",
      "    accuracy                         0.8810       168\n",
      "   macro avg     0.8889    0.8810    0.8803       168\n",
      "weighted avg     0.8889    0.8810    0.8803       168\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.9420    0.8844        69\n",
      "           1     0.9344    0.8143    0.8702        70\n",
      "\n",
      "    accuracy                         0.8777       139\n",
      "   macro avg     0.8839    0.8782    0.8773       139\n",
      "weighted avg     0.8842    0.8777    0.8772       139\n",
      "\n",
      "german_wiktionary-claim_verification-mini: _________________________\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-3.5-turbo...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8421    0.4800    0.6115       100\n",
      "           1     0.6364    0.9100    0.7490       100\n",
      "\n",
      "    accuracy                         0.6950       200\n",
      "   macro avg     0.7392    0.6950    0.6802       200\n",
      "weighted avg     0.7392    0.6950    0.6802       200\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8864    0.4815    0.6240        81\n",
      "           1     0.6379    0.9367    0.7590        79\n",
      "\n",
      "    accuracy                         0.7063       160\n",
      "   macro avg     0.7621    0.7091    0.6915       160\n",
      "weighted avg     0.7637    0.7063    0.6906       160\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o-mini...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7653    0.7500    0.7576       100\n",
      "           1     0.7549    0.7700    0.7624       100\n",
      "\n",
      "    accuracy                         0.7600       200\n",
      "   macro avg     0.7601    0.7600    0.7600       200\n",
      "weighted avg     0.7601    0.7600    0.7600       200\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.7407    0.7453        81\n",
      "           1     0.7375    0.7468    0.7421        79\n",
      "\n",
      "    accuracy                         0.7438       160\n",
      "   macro avg     0.7438    0.7438    0.7437       160\n",
      "weighted avg     0.7438    0.7438    0.7438       160\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6884    0.9500    0.7983       100\n",
      "           1     0.9194    0.5700    0.7037       100\n",
      "\n",
      "    accuracy                         0.7600       200\n",
      "   macro avg     0.8039    0.7600    0.7510       200\n",
      "weighted avg     0.8039    0.7600    0.7510       200\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.9506    0.8063        81\n",
      "           1     0.9200    0.5823    0.7132        79\n",
      "\n",
      "    accuracy                         0.7688       160\n",
      "   macro avg     0.8100    0.7664    0.7597       160\n",
      "weighted avg     0.8086    0.7688    0.7603       160\n",
      "\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5 Zero Shot Single Facts",
   "id": "e665b558ede09f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T09:32:58.456310Z",
     "start_time": "2024-08-19T09:32:58.453473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "claim_split_types = [\n",
    "    'DisSim_facts',\n",
    "    'T5SplitRephrase_facts',\n",
    "    'Factscore_facts'\n",
    "]"
   ],
   "id": "36f7a32c6652cd96",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:08:41.901310Z",
     "start_time": "2024-08-19T11:08:41.897917Z"
    }
   },
   "cell_type": "code",
   "source": "file_base_name = str(EVALUATION_DIR / '{dataset}/zero_shot_{split_type}/{type}/{type}_zero_shot_{split_type}-{dataset}-{model}.jsonl')",
   "id": "243f1e8f685e7a25",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T09:33:51.739767Z",
     "start_time": "2024-08-19T09:33:50.962798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        for split_type in claim_split_types:\n",
    "            tasks = []\n",
    "            for idx, entry in enumerate(dataset):\n",
    "                word = entry['word']\n",
    "                atomic_facts = entry[split_type].split('--;--')\n",
    "    \n",
    "                for aidx, atomic_fact in enumerate(atomic_facts):\n",
    "                    tasks.append(build_task(f'{idx}-{aidx}', model, f'Input: {word}: {atomic_fact} True or False?\\nOutput:'))\n",
    "            fh.write(file_base_name.format(type='input', dataset=dataset_name, model=model, split_type=split_type), tasks)"
   ],
   "id": "494c10852271c29d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.91it/s]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once input files are created, head to 2, to manually fetch your outputs",
   "id": "2f903b7824971f9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:13:28.209317Z",
     "start_time": "2024-08-19T11:13:28.199825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(dataset_name, dataset, model, split_type):\n",
    "    print(f\"Evaluating {dataset_name} with {model} - {split_type}...\")\n",
    "    results = fh.read(file_base_name.format(type='raw_output', dataset=dataset_name, model=model, split_type=split_type))\n",
    "    \n",
    "    data_dict = init_data_dict(dataset)\n",
    "    process_results(results, dataset, data_dict, split_type)\n",
    "    gt_labels, pr_labels = generate_labels(data_dict)\n",
    "    \n",
    "    fh.write(file_base_name.format(type='output', dataset=dataset_name, model=model, split_type=split_type), data_dict.values())\n",
    "    print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "\n",
    "def init_data_dict(dataset):\n",
    "    data_dict = {}\n",
    "    for entry in dataset:\n",
    "        data_dict[entry['id']] = {\n",
    "            'id': entry['id'],\n",
    "            'word': entry['word'],\n",
    "            'claim': entry['claim'],\n",
    "            'label': entry['label'],\n",
    "            'predicted': -1,\n",
    "            'atoms': [],\n",
    "            'in_wiki': entry['in_wiki']\n",
    "        }\n",
    "    return data_dict\n",
    "\n",
    "def process_results(results, dataset, data_dict, split_type):\n",
    "    for res in results:\n",
    "        task_id = res['custom_id']\n",
    "        index = int(task_id.split('-')[1])\n",
    "        atom_index = int(task_id.split('-')[2])\n",
    "        \n",
    "        entry = dataset[index]\n",
    "        atom = entry[split_type].split('--;--')[atom_index]\n",
    "        \n",
    "        predicted = get_openai_prediction(res['response']['body'])\n",
    "        if predicted == 'UNKOWN':\n",
    "            txt_answer = res['response']['body']['choices'][0]['message']['content']\n",
    "            predicted = parse_model_answer(txt_answer)\n",
    "        \n",
    "        data_dict[entry['id']]['atoms'].append({\"atom\": atom, \"predicted\": predicted})\n",
    "\n",
    "def generate_labels(data_dict):\n",
    "    gt_labels = []\n",
    "    pr_labels = []\n",
    "    \n",
    "    for entry_id, entry in data_dict.items():\n",
    "        all_predictions = [decision['predicted'] == 'SUPPORTED' for decision in entry['atoms']]\n",
    "        average_is_supported = np.mean(all_predictions)\n",
    "        data_dict[entry_id]['predicted'] = 'SUPPORTED' if average_is_supported == 1 else 'NOT_SUPPORTED'\n",
    "        \n",
    "        gt_labels.append(1 if entry['label'] == 'SUPPORTED' else 0)\n",
    "        pr_labels.append(1 if entry['predicted'] == 'SUPPORTED' else 0)\n",
    "    \n",
    "    return gt_labels, pr_labels"
   ],
   "id": "16ed886b227f27fc",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:13:34.374046Z",
     "start_time": "2024-08-19T11:13:29.755178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    print(f\"{dataset_name}: _________________________\")\n",
    "    for model in models:\n",
    "        for split_type in claim_split_types:\n",
    "            evaluate_model(dataset_name, dataset, model, split_type)"
   ],
   "id": "e412f2206d5ec6b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_dpr-claim_verification: _________________________\n",
      "Evaluating german_dpr-claim_verification with gpt-3.5-turbo - DisSim_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8395    0.8095    0.8242        84\n",
      "           1     0.8161    0.8452    0.8304        84\n",
      "\n",
      "    accuracy                         0.8274       168\n",
      "   macro avg     0.8278    0.8274    0.8273       168\n",
      "weighted avg     0.8278    0.8274    0.8273       168\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-3.5-turbo - T5SplitRephrase_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8987    0.8452    0.8712        84\n",
      "           1     0.8539    0.9048    0.8786        84\n",
      "\n",
      "    accuracy                         0.8750       168\n",
      "   macro avg     0.8763    0.8750    0.8749       168\n",
      "weighted avg     0.8763    0.8750    0.8749       168\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-3.5-turbo - Factscore_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8539    0.9048    0.8786        84\n",
      "           1     0.8987    0.8452    0.8712        84\n",
      "\n",
      "    accuracy                         0.8750       168\n",
      "   macro avg     0.8763    0.8750    0.8749       168\n",
      "weighted avg     0.8763    0.8750    0.8749       168\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o-mini - DisSim_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7670    0.9405    0.8449        84\n",
      "           1     0.9231    0.7143    0.8054        84\n",
      "\n",
      "    accuracy                         0.8274       168\n",
      "   macro avg     0.8450    0.8274    0.8251       168\n",
      "weighted avg     0.8450    0.8274    0.8251       168\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o-mini - T5SplitRephrase_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8298    0.9286    0.8764        84\n",
      "           1     0.9189    0.8095    0.8608        84\n",
      "\n",
      "    accuracy                         0.8690       168\n",
      "   macro avg     0.8744    0.8690    0.8686       168\n",
      "weighted avg     0.8744    0.8690    0.8686       168\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o-mini - Factscore_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7570    0.9643    0.8482        84\n",
      "           1     0.9508    0.6905    0.8000        84\n",
      "\n",
      "    accuracy                         0.8274       168\n",
      "   macro avg     0.8539    0.8274    0.8241       168\n",
      "weighted avg     0.8539    0.8274    0.8241       168\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o - DisSim_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7315    0.9405    0.8229        84\n",
      "           1     0.9167    0.6548    0.7639        84\n",
      "\n",
      "    accuracy                         0.7976       168\n",
      "   macro avg     0.8241    0.7976    0.7934       168\n",
      "weighted avg     0.8241    0.7976    0.7934       168\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o - T5SplitRephrase_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8316    0.9405    0.8827        84\n",
      "           1     0.9315    0.8095    0.8662        84\n",
      "\n",
      "    accuracy                         0.8750       168\n",
      "   macro avg     0.8815    0.8750    0.8745       168\n",
      "weighted avg     0.8815    0.8750    0.8745       168\n",
      "\n",
      "Evaluating german_dpr-claim_verification with gpt-4o - Factscore_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7664    0.9762    0.8586        84\n",
      "           1     0.9672    0.7024    0.8138        84\n",
      "\n",
      "    accuracy                         0.8393       168\n",
      "   macro avg     0.8668    0.8393    0.8362       168\n",
      "weighted avg     0.8668    0.8393    0.8362       168\n",
      "\n",
      "german_wiktionary-claim_verification-mini: _________________________\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-3.5-turbo - DisSim_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8161    0.7100    0.7594       100\n",
      "           1     0.7434    0.8400    0.7887       100\n",
      "\n",
      "    accuracy                         0.7750       200\n",
      "   macro avg     0.7797    0.7750    0.7740       200\n",
      "weighted avg     0.7797    0.7750    0.7740       200\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-3.5-turbo - T5SplitRephrase_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8144    0.7900    0.8020       100\n",
      "           1     0.7961    0.8200    0.8079       100\n",
      "\n",
      "    accuracy                         0.8050       200\n",
      "   macro avg     0.8053    0.8050    0.8050       200\n",
      "weighted avg     0.8053    0.8050    0.8050       200\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-3.5-turbo - Factscore_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7339    0.8000    0.7656       100\n",
      "           1     0.7802    0.7100    0.7435       100\n",
      "\n",
      "    accuracy                         0.7550       200\n",
      "   macro avg     0.7571    0.7550    0.7545       200\n",
      "weighted avg     0.7571    0.7550    0.7545       200\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o-mini - DisSim_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7037    0.9500    0.8085       100\n",
      "           1     0.9231    0.6000    0.7273       100\n",
      "\n",
      "    accuracy                         0.7750       200\n",
      "   macro avg     0.8134    0.7750    0.7679       200\n",
      "weighted avg     0.8134    0.7750    0.7679       200\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o-mini - T5SplitRephrase_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.9000    0.7965       100\n",
      "           1     0.8649    0.6400    0.7356       100\n",
      "\n",
      "    accuracy                         0.7700       200\n",
      "   macro avg     0.7896    0.7700    0.7660       200\n",
      "weighted avg     0.7896    0.7700    0.7660       200\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o-mini - Factscore_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6528    0.9400    0.7705       100\n",
      "           1     0.8929    0.5000    0.6410       100\n",
      "\n",
      "    accuracy                         0.7200       200\n",
      "   macro avg     0.7728    0.7200    0.7058       200\n",
      "weighted avg     0.7728    0.7200    0.7058       200\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o - DisSim_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6139    0.9700    0.7519       100\n",
      "           1     0.9286    0.3900    0.5493       100\n",
      "\n",
      "    accuracy                         0.6800       200\n",
      "   macro avg     0.7712    0.6800    0.6506       200\n",
      "weighted avg     0.7712    0.6800    0.6506       200\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o - T5SplitRephrase_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7132    0.9700    0.8220       100\n",
      "           1     0.9531    0.6100    0.7439       100\n",
      "\n",
      "    accuracy                         0.7900       200\n",
      "   macro avg     0.8332    0.7900    0.7830       200\n",
      "weighted avg     0.8332    0.7900    0.7830       200\n",
      "\n",
      "Evaluating german_wiktionary-claim_verification-mini with gpt-4o - Factscore_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6139    0.9700    0.7519       100\n",
      "           1     0.9286    0.3900    0.5493       100\n",
      "\n",
      "    accuracy                         0.6800       200\n",
      "   macro avg     0.7712    0.6800    0.6506       200\n",
      "weighted avg     0.7712    0.6800    0.6506       200\n",
      "\n",
      "squad-claim_verification: _________________________\n",
      "Evaluating squad-claim_verification with gpt-3.5-turbo - DisSim_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8732    0.7848    0.8267        79\n",
      "           1     0.8046    0.8861    0.8434        79\n",
      "\n",
      "    accuracy                         0.8354       158\n",
      "   macro avg     0.8389    0.8354    0.8350       158\n",
      "weighted avg     0.8389    0.8354    0.8350       158\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-3.5-turbo - T5SplitRephrase_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9028    0.8228    0.8609        79\n",
      "           1     0.8372    0.9114    0.8727        79\n",
      "\n",
      "    accuracy                         0.8671       158\n",
      "   macro avg     0.8700    0.8671    0.8668       158\n",
      "weighted avg     0.8700    0.8671    0.8668       158\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-3.5-turbo - Factscore_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.8861    0.8589        79\n",
      "           1     0.8784    0.8228    0.8497        79\n",
      "\n",
      "    accuracy                         0.8544       158\n",
      "   macro avg     0.8559    0.8544    0.8543       158\n",
      "weighted avg     0.8559    0.8544    0.8543       158\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-4o-mini - DisSim_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8068    0.8987    0.8503        79\n",
      "           1     0.8857    0.7848    0.8322        79\n",
      "\n",
      "    accuracy                         0.8418       158\n",
      "   macro avg     0.8463    0.8418    0.8413       158\n",
      "weighted avg     0.8463    0.8418    0.8413       158\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-4o-mini - T5SplitRephrase_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8161    0.8987    0.8554        79\n",
      "           1     0.8873    0.7975    0.8400        79\n",
      "\n",
      "    accuracy                         0.8481       158\n",
      "   macro avg     0.8517    0.8481    0.8477       158\n",
      "weighted avg     0.8517    0.8481    0.8477       158\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-4o-mini - Factscore_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7653    0.9494    0.8475        79\n",
      "           1     0.9333    0.7089    0.8058        79\n",
      "\n",
      "    accuracy                         0.8291       158\n",
      "   macro avg     0.8493    0.8291    0.8266       158\n",
      "weighted avg     0.8493    0.8291    0.8266       158\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-4o - DisSim_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8105    0.9747    0.8851        79\n",
      "           1     0.9683    0.7722    0.8592        79\n",
      "\n",
      "    accuracy                         0.8734       158\n",
      "   macro avg     0.8894    0.8734    0.8721       158\n",
      "weighted avg     0.8894    0.8734    0.8721       158\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-4o - T5SplitRephrase_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8261    0.9620    0.8889        79\n",
      "           1     0.9545    0.7975    0.8690        79\n",
      "\n",
      "    accuracy                         0.8797       158\n",
      "   macro avg     0.8903    0.8797    0.8789       158\n",
      "weighted avg     0.8903    0.8797    0.8789       158\n",
      "\n",
      "Evaluating squad-claim_verification with gpt-4o - Factscore_facts...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7379    0.9620    0.8352        79\n",
      "           1     0.9455    0.6582    0.7761        79\n",
      "\n",
      "    accuracy                         0.8101       158\n",
      "   macro avg     0.8417    0.8101    0.8056       158\n",
      "weighted avg     0.8417    0.8101    0.8056       158\n",
      "\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d23815a0046be6b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
