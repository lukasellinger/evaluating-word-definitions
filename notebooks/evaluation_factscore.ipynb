{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0 Preparations\n",
    "Before starting, ensure that you have cloned the repository to your Google Drive.\n",
    "We will connect to this:"
   ],
   "id": "294d7129d1edcf7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "repository = 'evaluating_factuality_word_definitions'\n",
    "\n",
    "%cd /content/drive/My Drive/{repository}"
   ],
   "id": "7fc5cb56b6eb286d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we install the packages and import the modules needed in this notebook:",
   "id": "a415871ea8be1ebe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install datasets~=2.18.0\n",
    "!pip install openai~=1.35.10"
   ],
   "id": "58670057eb89e41a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T14:25:00.462580Z",
     "start_time": "2024-09-27T14:24:43.268186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "from datasets import load_dataset\n",
    "from general_utils.reader import JSONLineReader\n",
    "from config import PROJECT_DIR\n",
    "from fetchers.wikipedia import Wikipedia\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from general_utils.utils import parse_model_answer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from fetchers.openai import OpenAiFetcher\n",
    "from general_utils.atomic_facts import FactScoreFactGenerator\n",
    "from tqdm import tqdm\n",
    "from config import FACT_EVULATION_OPENAI_TOKEN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import DatasetDict\n",
    "from config import HF_WRITE_TOKEN\n",
    "from dataset.def_dataset import Fact\n",
    "from general_utils.utils import print_classification_report\n",
    "from sklearn.metrics import classification_report"
   ],
   "id": "b3b485ea1b41d602",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 Setup: Define Datasets\n",
    "Now we define our models and datasets we want to evaluate:"
   ],
   "id": "4bebf4345d08a25e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T03:09:02.770245Z",
     "start_time": "2024-09-30T03:08:42.689050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Datasets with language information\n",
    "datasets = {\n",
    "    'german_dpr-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german_dpr-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'german_wiktionary-claim_verification-mini': {\n",
    "        'dataset': load_dataset('lukasellinger/german_wiktionary-claim_verification-mini', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'squad-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/squad-claim_verification', split='test'),\n",
    "        'lang': 'en'\n",
    "    },\n",
    "    'shroom-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/shroom-claim_verification', split='test'),\n",
    "        'lang': 'en'\n",
    "    }\n",
    "    # optional (contains 10k entries)\n",
    "    #'german_wiktionary-claim_verification-large': {\n",
    "    #    'dataset': load_dataset('lukasellinger/german_wiktionary-claim_verification-large', split='test'),\n",
    "    #    'lang': 'de'\n",
    "    #},\n",
    "    # outdated\n",
    "    #'german-claim_verification': {\n",
    "    #    'dataset': load_dataset('lukasellinger/german-claim_verification', split='test'),\n",
    "    #    'lang': 'de'\n",
    "    #},\n",
    "}"
   ],
   "id": "e47c89b9882c79ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/810 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f6b948859a1448fbb41207eb4d1a516"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 232k/232k [00:00<00:00, 242kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/563 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc6bb920793b4b79895df6b9a70645f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T14:26:09.671726Z",
     "start_time": "2024-09-27T14:25:29.177973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "openai_fetcher = OpenAiFetcher(api_key=FACT_EVULATION_OPENAI_TOKEN)\n",
    "fh = JSONLineReader()\n",
    "offline_wiki = 'lukasellinger/wiki_dump_2024-09-27'\n",
    "wiki = Wikipedia(use_dataset=offline_wiki)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fb9060c0c5448789c9b57ed0e9f2bd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 166M/166M [00:28<00:00, 5.75MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/35566 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d482e205885e417aad3a7756bae82da5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasellinger/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T14:26:09.681089Z",
     "start_time": "2024-09-27T14:26:09.675172Z"
    }
   },
   "cell_type": "code",
   "source": "EVALUATION_DIR = PROJECT_DIR / 'data/evaluation'",
   "id": "69f8399e95180fc0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2 Create Factscore Facts\n",
    "Batched Request are not supported with gpt-3.5-turbo-instruct"
   ],
   "id": "84a675291ce471c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T14:26:10.188226Z",
     "start_time": "2024-09-27T14:26:09.683888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "af_prompt_generator = FactScoreFactGenerator(PROJECT_DIR.joinpath('factscore/demos'), is_bio=False)\n",
    "model = \"gpt-3.5-turbo-instruct\"\n",
    "temperature = 0.7\n",
    "max_tokens = 512"
   ],
   "id": "9b544153bb9496dc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T14:33:27.034414Z",
     "start_time": "2024-09-27T14:26:29.070848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "\n",
    "    fact_column = []\n",
    "    for entry in tqdm(dataset):\n",
    "        claim = entry['connected_claim']\n",
    "        prompt = af_prompt_generator.get_prompt_for_sentence(claim)\n",
    "        response = openai_fetcher.client.completions.create(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            prompt=prompt,\n",
    "            seed=42\n",
    "        )\n",
    "        generated_answer = response.choices[0].text\n",
    "        facts = af_prompt_generator.get_facts_from_response(claim, generated_answer)\n",
    "        fact_column.append('--;--'.join(facts))\n",
    "        \n",
    "    dataset = dataset.add_column('Factscore_facts', fact_column)\n",
    "    data_dict = DatasetDict()\n",
    "    data_dict['test'] = dataset\n",
    "    data_dict.push_to_hub(dataset_name, token=HF_WRITE_TOKEN)"
   ],
   "id": "a760b69f1bcfbc87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 563/563 [06:53<00:00,  1.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6d0349eecce4b3aae3053c67ee43cd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "433e0fe8baa44de98dbad9a477942903"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 Calc OpenAi Prediction",
   "id": "99e82cebe90e9a88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T14:35:00.350780Z",
     "start_time": "2024-09-27T14:35:00.347311Z"
    }
   },
   "cell_type": "code",
   "source": "file_base_name = str(EVALUATION_DIR / '{dataset}/factscore/{type}/{type}_factscore-{dataset}.jsonl')",
   "id": "167c69b96a936bee",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1 Helper functions",
   "id": "8004579435c5fb82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T14:35:04.105641Z",
     "start_time": "2024-09-27T14:35:04.087627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_bm25_passages_indices(query, passages, k):\n",
    "    assert len(passages) > 0, f'passages are empty for {query}'\n",
    "    bm25 = BM25Okapi([psg.replace(\"<s>\", \"\").replace(\"</s>\", \"\").split() for psg in passages])\n",
    "    scores = bm25.get_scores(query.split())\n",
    "    return np.argsort(-scores)[:k]\n",
    "\n",
    "\n",
    "def get_gtr_passages_indices(retrieval_query, passages, k, encoder, batch_size=8):\n",
    "    inputs = [psg[\"title\"] + \" \" + psg[\"text\"].replace(\"<s>\", \"\").replace(\"</s>\", \"\") for\n",
    "                  psg in passages]\n",
    "    passage_vectors = encoder.encode(inputs, batch_size=batch_size,\n",
    "                                    device=encoder.device)\n",
    "    query_vectors = encoder.encode([retrieval_query],\n",
    "                                    batch_size=batch_size,\n",
    "                                    device=encoder.device)[0]\n",
    "    scores = np.inner(query_vectors, passage_vectors)\n",
    "    return np.argsort(-scores)[:k]\n",
    "\n",
    "\n",
    "def get_passages(topic, fallback_topic, question, search_word, k=5, only_intro=True, word_lang='de', retrieval='bm25', encoder=None):\n",
    "    assert retrieval in ('bm25', 'gtr'), 'retrieval method not supported'\n",
    "    texts, wiki_word = wiki.get_pages(topic, fallback_topic, word_lang, only_intro=only_intro, split_level='passage', search_word=search_word)\n",
    "    if texts:\n",
    "        passages = {'word': wiki_word, 'passages': [{'title': str(text[0]).split('(wik')[0], 'text': text[1]} for text in texts]}\n",
    "        if retrieval == 'bm25':\n",
    "            ranked_indices = get_bm25_passages_indices(question, [passage.get('text') for passage in passages.get('passages')], k)     \n",
    "        else:\n",
    "            ranked_indices = get_gtr_passages_indices(question, passages.get('passages'), k, encoder)\n",
    "        return {'word': wiki_word, 'passages': [passages.get('passages')[i] for i in ranked_indices]}\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def build_prompts(topic, fallback_topic, search_word, atomic_facts, retrieval='gtr', encoder=None):\n",
    "    prompts = []\n",
    "    not_in_context = 0\n",
    "    for atom in atomic_facts:\n",
    "        atom = atom.strip()\n",
    "        retrieved = get_passages(topic, fallback_topic, atom, search_word, k=5, retrieval=retrieval, encoder=encoder)\n",
    "        word = retrieved.get('word')\n",
    "        passages = retrieved.get('passages')\n",
    "        \n",
    "        if not (word and passages):\n",
    "            not_in_context += 1\n",
    "            continue\n",
    " \n",
    "        definition = \"Answer the question about {} based on the given context.\\n\\n\".format(word)\n",
    "        context = \"\"\n",
    "        for psg_idx, psg in enumerate(reversed(passages)):\n",
    "            context += \"Title: {}\\nText: {}\\n\\n\".format(psg[\"title\"],psg[\"text\"].replace(\"<s>\", \"\").replace(\"</s>\", \"\"))\n",
    "        definition += context.strip()\n",
    "        if not definition[-1] in string.punctuation:\n",
    "            definition += \".\"\n",
    "        prompt = \"{}\\n\\nInput: {} True or False?\\nOutput:\".format(definition.strip(),atom.strip())\n",
    "        prompts.append(prompt)\n",
    "    return prompts, not_in_context\n",
    "\n",
    "\n",
    "def create_task(custom_id, prompt, model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=2048):\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def process_dataset(dataset_name, config, encoder):\n",
    "    dataset = config['dataset']\n",
    "    tasks = []\n",
    "    total_not_in_context = 0\n",
    "    \n",
    "    for idx, entry in tqdm(enumerate(dataset)):\n",
    "        topic = entry['word']\n",
    "        atomic_facts = entry['Factscore_facts'].split('--;--')\n",
    "        prompts, not_in_context = build_prompts(topic, entry.get('english_word', topic), entry.get('document_search_word'), atomic_facts, encoder=encoder)\n",
    "        \n",
    "        total_not_in_context += 1 if not_in_context > 0 else 0\n",
    "        \n",
    "        for pidx, prompt in enumerate(prompts):\n",
    "            custom_id = f\"task-{idx}-{pidx}\"\n",
    "            task = create_task(custom_id, prompt)\n",
    "            tasks.append(task)\n",
    "    print(f'Not in context for {dataset_name}: {total_not_in_context}')\n",
    "    return tasks"
   ],
   "id": "49cca4ba8f22e060",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 Get Openai Outputs",
   "id": "bc5d4738073ce53d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T14:35:22.860420Z",
     "start_time": "2024-09-27T14:35:15.321397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = SentenceTransformer(\"sentence-transformers/gtr-t5-large\")\n",
    "encoder = encoder.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = encoder.eval()"
   ],
   "id": "8fef814421678018",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasellinger/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T15:37:14.075222Z",
     "start_time": "2024-09-27T14:37:19.841794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    tasks = process_dataset(dataset_name, config, encoder)\n",
    "    fh.write(file_base_name.format(dataset=dataset_name, type='input'), tasks)"
   ],
   "id": "9c57a8cb7eb2fbd6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [14:01, 20.81s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "563it [59:54,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in context for shroom-claim_verification: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now manually initializes the batch processing for each created file",
   "id": "577c54225107bf92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:57:42.059061Z",
     "start_time": "2024-09-29T17:57:42.044913Z"
    }
   },
   "cell_type": "code",
   "source": "batch_jobs = {}",
   "id": "f184c89ec2f1d3f4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T17:58:46.640177Z",
     "start_time": "2024-09-29T17:58:42.942792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_file_name = str(EVALUATION_DIR / 'shroom-claim_verification/factscore/input/input_factscore-shroom-claim_verification.jsonl')\n",
    "batch_job = openai_fetcher.create_batch_job(input_file_name)\n",
    "batch_jobs[input_file_name] = batch_job"
   ],
   "id": "863a0216ba92222f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T18:10:16.423291Z",
     "start_time": "2024-09-29T18:10:15.823681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file_name, batch_job in batch_jobs.items():\n",
    "    batch_job = openai_fetcher.get_batch_update(batch_job)\n",
    "    batch_jobs[file_name] = batch_job\n",
    "    print(file_name)\n",
    "    print(batch_job)\n",
    "    print(\"_______________\")"
   ],
   "id": "944054b4a043a4cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lukasellinger/PycharmProjects/evaluating_factuality_word_definitions/data/evaluation/shroom-claim_verification/factscore/input/input_factscore-shroom-claim_verification.jsonl\n",
      "Batch(id='batch_66f99a26eb4081908f7bcbd2c09fe5e0', completion_window='24h', created_at=1727633959, endpoint='/v1/chat/completions', input_file_id='file-VxPPbGWg5QJb5o7SdKVQ3T7o', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1727634564, error_file_id=None, errors=None, expired_at=None, expires_at=1727720359, failed_at=None, finalizing_at=1727634433, in_progress_at=1727634021, metadata=None, output_file_id='file-TQTq8n26pgaPT97Gftm6lGgs', request_counts=BatchRequestCounts(completed=1478, failed=0, total=1478))\n",
      "_______________\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T18:10:44.723300Z",
     "start_time": "2024-09-29T18:10:43.516341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file_name, batch_job in batch_jobs.items():\n",
    "    output_file_name = file_name.replace('input', 'raw_output')\n",
    "    openai_fetcher.get_batch_result(output_file_name, batch_job)"
   ],
   "id": "3874adabbc106bcf",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 Calc Factscore",
   "id": "ff12f6ceaef9adb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T18:11:12.322952Z",
     "start_time": "2024-09-29T18:11:12.315330Z"
    }
   },
   "cell_type": "code",
   "source": "file_base_name = str(EVALUATION_DIR / '{dataset}/factscore/{type}/{type}_factscore-{dataset}.jsonl')",
   "id": "c2d4294bd2c05b96",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1 Helper functions",
   "id": "69e145f5a5b5cf9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T18:11:16.794964Z",
     "start_time": "2024-09-29T18:11:16.754731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(x):\n",
    "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())\n",
    "\n",
    "class NPM:\n",
    "    def __init__(self, model_name):\n",
    "        assert model_name.startswith(\"npm\")\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/\" + self.model_name)\n",
    "        self.mask_id = self.tokenizer.mask_token_id\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        with open(PROJECT_DIR.joinpath(\"factscore/roberta_stopwords.txt\"), \"r\") as f:\n",
    "            self.stopwords = set()\n",
    "            for line in f:\n",
    "                self.stopwords.add(int(line.strip()))\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(\"facebook/\" + self.model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def tokenize(self, texts, skip_special_tokens=False, padding=True):\n",
    "        assert type(texts)==list\n",
    "        all_input_ids = self.tokenizer(texts)[\"input_ids\"]\n",
    "        if skip_special_tokens:\n",
    "            for i, input_ids in enumerate(all_input_ids):\n",
    "                assert input_ids[0]==0 and input_ids[-1]==2\n",
    "                all_input_ids[i] = input_ids[1:-1]\n",
    "        if not padding:\n",
    "            return all_input_ids\n",
    "        max_length = np.max([len(_ids) for _ids in all_input_ids])    \n",
    "        _all_input_ids = []\n",
    "        _all_attention_mask = []   \n",
    "        for i, input_ids in enumerate(all_input_ids):\n",
    "            n_valid = len(input_ids)\n",
    "            n_masks = max_length - n_valid\n",
    "            _all_input_ids.append(input_ids + [0 for _ in range(n_masks)])\n",
    "            _all_attention_mask.append([1 for _ in range(n_valid)] + [0 for _ in range(n_masks)])\n",
    "        return torch.LongTensor(_all_input_ids), torch.LongTensor(_all_attention_mask)\n",
    "\n",
    "    def decode(self, input_ids):\n",
    "        return self.tokenizer.decode(input_ids)\n",
    "\n",
    "    def encode(self, texts, skip_special_tokens=False, gt_input_ids=None):\n",
    "        assert type(texts)==list\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        if gt_input_ids is not None:\n",
    "            assert len(texts)==len(gt_input_ids)\n",
    "        all_input_ids, all_attention_mask = self.tokenize(texts, skip_special_tokens=skip_special_tokens)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(all_input_ids.to(self.device),\n",
    "                                 all_attention_mask.to(self.device),\n",
    "                                 output_hidden_states=True,\n",
    "                                 return_dict=True)\n",
    "            all_logits = outputs[\"logits\"].detach().cpu().numpy()\n",
    "            all_hidden_states = outputs[\"hidden_states\"][-1].detach().cpu().numpy()\n",
    "\n",
    "        results = []\n",
    "        for i, (text, input_ids, logits, hidden_states) in enumerate(zip(texts, all_input_ids, all_logits, all_hidden_states)):\n",
    "            input_ids = input_ids.numpy().tolist()\n",
    "            if self.mask_id in input_ids:\n",
    "                idx = input_ids.index(self.mask_id)\n",
    "                assert gt_input_ids is not None\n",
    "                prob = softmax(logits[idx])[gt_input_ids[i]]\n",
    "                results.append((prob, hidden_states[idx]))\n",
    "            else:\n",
    "                _input_ids = [_id for _id in input_ids if _id not in [0, 2]]\n",
    "                _hidden_states = [h for _id, h in zip(input_ids, hidden_states) if _id not in [0, 2]]\n",
    "                results.append((_input_ids, _hidden_states))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_probabilty(self, topic, fallback_topic, question, search_word):\n",
    "        retrieved = get_passages(topic, fallback_topic, question, search_word, k=3, only_intro=True, word_lang='de')        \n",
    "        passages = [p[\"text\"].strip() for p in retrieved.get('passages')]\n",
    "        \n",
    "        encoded = self.encode(passages, skip_special_tokens=True)\n",
    "        stacked_passage_tokens, stacked_passage_vectors = [], []\n",
    "        for input_ids, vectors in encoded:\n",
    "            stacked_passage_tokens += input_ids\n",
    "            if len(vectors)>0:\n",
    "                stacked_passage_vectors.append(vectors)\n",
    "        stacked_passage_vectors = np.concatenate(stacked_passage_vectors, 0)\n",
    "            \n",
    "        question_input_ids = self.tokenize([\"Fact: \" + question], skip_special_tokens=False, padding=False)[0]\n",
    "        if 2 in question_input_ids:\n",
    "            question_input_ids = question_input_ids[:question_input_ids.index(2)]\n",
    "        question_input_ids = question_input_ids[1:]\n",
    "\n",
    "        triples = []\n",
    "        batch = []\n",
    "        gt_input_ids = []\n",
    "        prefix = True\n",
    "        for i, input_id in enumerate(question_input_ids):\n",
    "            if prefix:\n",
    "                if input_id==35: # the end of prefix\n",
    "                    prefix = False\n",
    "                continue\n",
    "            if input_id in [0, 2] or input_id in self.stopwords:\n",
    "                continue\n",
    "            batch.append(self.decode(question_input_ids[:i] + [self.mask_id] + question_input_ids[i+1:]))\n",
    "            gt_input_ids.append(input_id)\n",
    "        for (prob, vector), gt_input_id in zip(self.encode(batch, gt_input_ids=gt_input_ids), gt_input_ids):\n",
    "            triples.append((prob, vector, gt_input_id))\n",
    "\n",
    "        stacked_question_vectors = np.stack([v for _, v, _ in triples], 0)\n",
    "        all_scores = np.exp(np.inner(stacked_question_vectors, stacked_passage_vectors) / np.sqrt(stacked_passage_vectors.shape[-1]))\n",
    "\n",
    "        probs = []\n",
    "        for (softmax_prob, vector, input_id), scores in zip(triples, all_scores):\n",
    "            assert len(stacked_passage_tokens)==len(scores)\n",
    "            if input_id not in stacked_passage_tokens:\n",
    "                probs.append(0)\n",
    "            else:\n",
    "                aggregated_scores = defaultdict(list)\n",
    "                for token, score in zip(stacked_passage_tokens, scores):\n",
    "                    aggregated_scores[token].append(score)\n",
    "                tot = np.sum([np.sum(v) for v in aggregated_scores.values()])\n",
    "                prob = np.sum(aggregated_scores[input_id]) / tot\n",
    "                probs.append(prob)\n",
    "        return np.mean(probs)"
   ],
   "id": "d812c5771dfe8d1e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T18:11:21.491821Z",
     "start_time": "2024-09-29T18:11:19.781416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "npm = NPM('npm-single')\n",
    "\n",
    "def calc_factscore(topic, fallback_topic, search_word, generated_answer, atom, use_npm=True):\n",
    "    is_supported = parse_model_answer(generated_answer)\n",
    "    if is_supported == 'SUPPORTED' and use_npm:\n",
    "        npprob = npm.get_probabilty(topic, fallback_topic, atom, search_word)\n",
    "        is_supported = is_supported if npprob > 0.3 else 'NOT_SUPPORTED'\n",
    "    return is_supported"
   ],
   "id": "b2181e492198b3ac",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 Get Factscore Output",
   "id": "87c87305a68b64ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T19:04:22.396191Z",
     "start_time": "2024-09-29T18:11:56.631867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    results = fh.read(file_base_name.format(dataset=dataset_name, type='raw_output'))\n",
    "    data_dict = {}\n",
    "\n",
    "    # Filter out entries not in the wiki and prepare the data_dict\n",
    "    for entry in dataset:\n",
    "        if entry['in_wiki'] == 'No':\n",
    "            continue\n",
    "\n",
    "        data_dict[entry['id']] = {\n",
    "            'id': entry['id'],\n",
    "            'word': entry['word'],\n",
    "            'claim': entry['claim'],\n",
    "            'label': entry['label'],\n",
    "            'predicted': -1,\n",
    "            'atoms': [],\n",
    "            'selected_evidences': []\n",
    "        }\n",
    "        \n",
    "    # Process each result and update data_dict\n",
    "    for res in tqdm(results):\n",
    "        task_id = res['custom_id']\n",
    "        index, atom_index = map(int, task_id.split('-')[1:3])\n",
    "        \n",
    "        entry = dataset[index]\n",
    "        generated_answer = res['response']['body']['choices'][0]['message']['content'].lower()\n",
    "        atom = entry['Factscore_facts'].split('--;--')[atom_index]\n",
    "        \n",
    "        predicted = calc_factscore(entry['word'], entry.get('english_word', entry['word']), entry['document_search_word'], generated_answer, atom, use_npm=True)\n",
    "        data_dict[entry['id']]['atoms'].append({\"atom\": atom, \"predicted\": predicted})\n",
    "\n",
    "    # Calculate the final prediction for each entry\n",
    "    for entry_id, entry in data_dict.items():\n",
    "        all_predictions = [decision['predicted'] == 'SUPPORTED' for decision in entry['atoms']]\n",
    "        average_is_supported = np.mean(all_predictions)\n",
    "        \n",
    "        entry['predicted'] = Fact.SUPPORTED.name if average_is_supported == 1 else Fact.NOT_SUPPORTED.name\n",
    "\n",
    "    # Write the output to a file\n",
    "    fh.write(file_base_name.format(dataset=dataset_name, type='output'), data_dict.values())"
   ],
   "id": "9955b53835988347",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1478/1478 [52:25<00:00,  2.13s/it]   \n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T08:56:24.374869Z",
     "start_time": "2024-09-30T08:56:24.320550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name in datasets.keys():\n",
    "    print(f'Evaluating {dataset_name} with Factscore...')\n",
    "    file_name = file_base_name.format(dataset=dataset_name, type='output')\n",
    "    gt_labels, pr_labels = [], []\n",
    "    for entry in fh.read(file_name):\n",
    "        if entry['predicted'] != -1:\n",
    "            gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "            pr_labels.append(Fact[entry['predicted']].to_factuality())\n",
    "    report = classification_report(gt_labels, pr_labels, zero_division=0, digits=4)\n",
    "    print_classification_report(report)"
   ],
   "id": "19d610448c4dc37c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating german_dpr-claim_verification with Factscore...\n",
      "################################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5354    0.9855    0.6939        69\n",
      "           1     0.9167    0.1571    0.2683        70\n",
      "\n",
      "    accuracy                         0.5683       139\n",
      "   macro avg     0.7260    0.5713    0.4811       139\n",
      "weighted avg     0.7274    0.5683    0.4796       139\n",
      "\n",
      "################################\n",
      "Evaluating german_wiktionary-claim_verification-mini with Factscore...\n",
      "################################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5226    1.0000    0.6864        81\n",
      "           1     1.0000    0.0633    0.1190        79\n",
      "\n",
      "    accuracy                         0.5375       160\n",
      "   macro avg     0.7613    0.5316    0.4027       160\n",
      "weighted avg     0.7583    0.5375    0.4063       160\n",
      "\n",
      "################################\n",
      "Evaluating squad-claim_verification with Factscore...\n",
      "################################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5849    0.9841    0.7337        63\n",
      "           1     0.9500    0.3016    0.4578        63\n",
      "\n",
      "    accuracy                         0.6429       126\n",
      "   macro avg     0.7675    0.6429    0.5958       126\n",
      "weighted avg     0.7675    0.6429    0.5958       126\n",
      "\n",
      "################################\n",
      "Evaluating shroom-claim_verification with Factscore...\n",
      "################################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5582    0.9317    0.6981       278\n",
      "           1     0.7564    0.2235    0.3450       264\n",
      "\n",
      "    accuracy                         0.5867       542\n",
      "   macro avg     0.6573    0.5776    0.5216       542\n",
      "weighted avg     0.6547    0.5867    0.5261       542\n",
      "\n",
      "################################\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fe7feb1d438bc6f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
