{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T22:40:30.265810Z",
     "start_time": "2024-10-17T22:40:16.850221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from pipeline_module.evidence_fetcher import WikipediaEvidenceFetcher"
   ],
   "id": "c8cc0a773a4c4645",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T10:21:07.675032Z",
     "start_time": "2024-09-10T10:21:03.709975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"lukasellinger/fever_evidence_selection-v1\")\n",
    "combined_dataset = concatenate_datasets([dataset['train'], dataset['dev'], dataset['test']])"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T10:21:39.261627Z",
     "start_time": "2024-09-10T10:21:39.252823Z"
    }
   },
   "cell_type": "code",
   "source": "combined_dataset[3]['evidence_lines']",
   "id": "4095a1b358e19b6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0;1;6;7;14;16;15'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T10:29:40.632252Z",
     "start_time": "2024-09-10T10:29:37.820186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count entries with more than 3 evidence lines\n",
    "count = 0\n",
    "for entry in combined_dataset:\n",
    "    evidences = entry['evidence_lines'].split(';')\n",
    "    min_evidence = 4\n",
    "    for evidence in evidences:\n",
    "        evidence_len = len(evidence.split(','))\n",
    "        if min_evidence > evidence_len:\n",
    "            min_evidence = evidence_len\n",
    "    if min_evidence > 3:\n",
    "        count += 1\n",
    "        \n",
    "print(f\"Number of entries with evidence lines > 3: {count}\")"
   ],
   "id": "78c24ac780f9679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with evidence lines > 3: 9\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T10:28:37.909719Z",
     "start_time": "2024-09-10T10:28:37.903597Z"
    }
   },
   "cell_type": "code",
   "source": "len(combined_dataset)",
   "id": "d0ad1128c93bba5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32900"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T22:40:52.979555Z",
     "start_time": "2024-10-17T22:40:30.268944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Datasets with language information\n",
    "datasets = {\n",
    "    'german_dpr-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german_dpr-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'german_wiktionary-claim_verification-mini': {\n",
    "        'dataset': load_dataset('lukasellinger/german_wiktionary-claim_verification-mini', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'squad-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/squad-claim_verification', split='test'),\n",
    "        'lang': 'en'\n",
    "    },\n",
    "    'shroom-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/shroom-claim_verification', split='test'),\n",
    "        'lang': 'en'\n",
    "    }\n",
    "    # optional (contains 10k entries)\n",
    "    #'german_wiktionary-claim_verification-large': {\n",
    "    #    'dataset': load_dataset('lukasellinger/german_wiktionary-claim_verification-large', split='test'),\n",
    "    #    'lang': 'de'\n",
    "    #},\n",
    "    # outdated\n",
    "    #'german-claim_verification': {\n",
    "    #    'dataset': load_dataset('lukasellinger/german-claim_verification', split='test'),\n",
    "    #    'lang': 'de'\n",
    "    #},\n",
    "}\n",
    "\n",
    "fever = load_dataset('lukasellinger/filtered_fever-claim_verification')"
   ],
   "id": "4d170da2b9716f53",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T14:00:55.240257Z",
     "start_time": "2024-09-30T14:00:55.083384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    data_dict = {}\n",
    "    \n",
    "    not_in_wiki = 0\n",
    "    avg_claim_count_wiki = {'DisSim_facts': 0,\n",
    "                            'Factscore_facts': 0,\n",
    "                            'T5SplitRephrase_facts': 0}\n",
    "    # Filter out entries not in the wiki and prepare the data_dict\n",
    "    for entry in dataset:\n",
    "        if entry['in_wiki'] == 'No':\n",
    "            not_in_wiki += 1\n",
    "        else:\n",
    "            for key in avg_claim_count_wiki.keys():\n",
    "                avg_claim_count_wiki[key] += len(entry[key].split('--;--'))        \n",
    "    for key, value in avg_claim_count_wiki.items():\n",
    "        avg_claim_count_wiki[key] = round(value / (len(dataset) - not_in_wiki), 2)\n",
    "    print(f'{dataset_name}: {1 - round(not_in_wiki / len(dataset), 4)}, {not_in_wiki}')\n",
    "    print(avg_claim_count_wiki)\n",
    "    print('-----------------')"
   ],
   "id": "77ecf18649b99ea8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_dpr-claim_verification: 0.8274, 29\n",
      "{'DisSim_facts': 1.87, 'Factscore_facts': 3.56, 'T5SplitRephrase_facts': 1.47}\n",
      "-----------------\n",
      "german_wiktionary-claim_verification-mini: 0.8, 40\n",
      "{'DisSim_facts': 1.7, 'Factscore_facts': 3.76, 'T5SplitRephrase_facts': 1.62}\n",
      "-----------------\n",
      "squad-claim_verification: 0.7975, 32\n",
      "{'DisSim_facts': 1.12, 'Factscore_facts': 2.39, 'T5SplitRephrase_facts': 1.06}\n",
      "-----------------\n",
      "shroom-claim_verification: 0.9627, 21\n",
      "{'DisSim_facts': 1.27, 'Factscore_facts': 2.73, 'T5SplitRephrase_facts': 1.22}\n",
      "-----------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T08:13:04.905838Z",
     "start_time": "2024-10-21T08:13:04.895084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dpr = 3.56\n",
    "wiki = 3.76\n",
    "squad = 2.39\n",
    "shroom = 2.73\n",
    "\n",
    "result = (136 * dpr + 160 * wiki + 126 * squad + 542 * shroom) / 967\n",
    "print(result)"
   ],
   "id": "256df6e062b656ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9643846949327823\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:46:16.481589Z",
     "start_time": "2024-10-04T10:46:16.457701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    \n",
    "    def claim_word_length(example):\n",
    "        # Split the claim into words and count the number of words\n",
    "        return {\"word_length\": len(example['claim'].split())}\n",
    "    \n",
    "    # Apply the function to the entire dataset using map\n",
    "    dataset_with_lengths = dataset.map(claim_word_length)\n",
    "    \n",
    "    # Now compute the average word length\n",
    "    avg_word_length = sum(dataset_with_lengths['word_length']) / len(dataset_with_lengths)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"{dataset_name}: Avg Claim Length: {avg_word_length:.2f}\")\n"
   ],
   "id": "92d5259ab8ae0d6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_dpr-claim_verification: Avg Claim Length: 11.93\n",
      "german_wiktionary-claim_verification-mini: Avg Claim Length: 11.19\n",
      "squad-claim_verification: Avg Claim Length: 3.32\n",
      "shroom-claim_verification: Avg Claim Length: 6.48\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:48:46.847503Z",
     "start_time": "2024-10-04T10:48:46.802766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, dataset in fever.items():    \n",
    "    def claim_word_length(example):\n",
    "        # Split the claim into words and count the number of words\n",
    "        return {\"word_length\": len(example['claim'].split())}\n",
    "    \n",
    "    # Apply the function to the entire dataset using map\n",
    "    dataset_with_lengths = dataset.map(claim_word_length)\n",
    "    \n",
    "    # Now compute the average word length\n",
    "    avg_word_length = sum(dataset_with_lengths['word_length']) / len(dataset_with_lengths)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"{dataset_name}: Avg Claim Length: {avg_word_length:.2f}\")"
   ],
   "id": "75b83c5c7f5009f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Avg Claim Length: 7.23\n",
      "dev: Avg Claim Length: 7.25\n",
      "test: Avg Claim Length: 7.35\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T22:46:46.061909Z",
     "start_time": "2024-10-17T22:44:50.812576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "offline_evid_fetcher = WikipediaEvidenceFetcher(offline=True)\n",
    "\n",
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']    \n",
    "    evidence_presence = {'in_both': 0,\n",
    "                         'only_wikipedia': 0,\n",
    "                         'only_wiktionary': 0,\n",
    "                         'in_none': 0}\n",
    "    for entry in tqdm(dataset):\n",
    "        if entry['in_wiki'] == 'No':\n",
    "            evidence_presence['in_none'] += 1\n",
    "            continue\n",
    "            \n",
    "        _, evidences = offline_evid_fetcher.fetch_evidences(search_word=entry['document_search_word'])\n",
    "        \n",
    "        in_wikipedia = False\n",
    "        in_wiktionary = False\n",
    "        for evidence in evidences:\n",
    "            if in_wikipedia and in_wiktionary:\n",
    "                break\n",
    "            \n",
    "            if evidence.get('title').endswith('(wikipedia)'):\n",
    "                in_wikipedia = True\n",
    "            else:\n",
    "                in_wiktionary = True\n",
    "        \n",
    "        assert in_wikipedia or in_wiktionary, f\"Evidence must be in wikipedia or wiktionary. But found else {entry}\"\n",
    "        if in_wikipedia and in_wiktionary:\n",
    "            evidence_presence['in_both'] += 1\n",
    "        elif in_wikipedia:\n",
    "            evidence_presence['only_wikipedia'] += 1\n",
    "        elif in_wiktionary:\n",
    "            evidence_presence['only_wiktionary'] += 1\n",
    "        \n",
    "    print(f'{dataset_name}:')\n",
    "    print(evidence_presence)\n",
    "    print('-----------------')"
   ],
   "id": "af81dcac58c64a0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasellinger/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 168/168 [00:30<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_dpr-claim_verification:\n",
      "{'in_both': 89, 'only_wikipedia': 47, 'only_wiktionary': 3, 'in_none': 29}\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 23/200 [00:03<00:36,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imam/German/noun: DEBUG: unrecognized sense qualifier: Twelver Shiism at ['Imam']\n",
      "imam/English/noun: DEBUG: unrecognized sense qualifier: Twelver Shi'ism at ['imam']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 59/200 [00:06<00:08, 17.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unterbinden/German/verb: DEBUG: unrecognized sense qualifier: transitive or dative reflexive, dated or regional at ['unterbinden']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 64/200 [00:07<00:13,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tüte/German/proper noun: DEBUG: unrecognized sense qualifier: Bielefeld, colloquial at ['Tüte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 68/200 [00:08<00:12, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request/English/verb: DEBUG: unrecognized sense qualifier: transitive or with a subjunctive clause at ['request']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 128/200 [00:15<00:11,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbesserung/German/noun: DEBUG: unrecognized sense qualifier: homework at ['Verbesserung']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 137/200 [00:21<00:45,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unterbinden/German/verb: DEBUG: unrecognized sense qualifier: transitive or dative reflexive, dated or regional at ['unterbinden']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 178/200 [00:26<00:03,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imam/German/noun: DEBUG: unrecognized sense qualifier: Twelver Shiism at ['Imam']\n",
      "imam/English/noun: DEBUG: unrecognized sense qualifier: Twelver Shi'ism at ['imam']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 195/200 [00:28<00:00, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specific/English/adjective: DEBUG: unrecognized sense qualifier: bioscience, taxonomy at ['specific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:28<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_wiktionary-claim_verification-mini:\n",
      "{'in_both': 120, 'only_wikipedia': 18, 'only_wiktionary': 22, 'in_none': 40}\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [00:15<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad-claim_verification:\n",
      "{'in_both': 82, 'only_wikipedia': 40, 'only_wiktionary': 4, 'in_none': 32}\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 29/563 [00:00<00:12, 43.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piss/English/noun: DEBUG: gloss may contain unhandled list items: 1999, Tin House #2 (→ISBN, Win McCormack, Rob Spillman, Elissa Schappell), page 170: at ['piss']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 330/563 [00:19<00:15, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demosophy/English/noun: DEBUG: unrecognized sense qualifier: folkloristics, sociology, rare at ['demosophy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 563/563 [00:33<00:00, 16.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shroom-claim_verification:\n",
      "{'in_both': 147, 'only_wikipedia': 0, 'only_wiktionary': 395, 'in_none': 21}\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "132ca4466b40d4eb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
