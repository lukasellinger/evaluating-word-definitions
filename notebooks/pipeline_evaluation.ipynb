{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T18:33:50.347535Z",
     "start_time": "2024-07-29T18:33:45.888408Z"
    }
   },
   "source": [
    "from general_utils.reader import JSONLineReader\n",
    "from sklearn.metrics import classification_report\n",
    "from dataset.def_dataset import Fact\n",
    "from config import PROJECT_DIR"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:35:27.409916Z",
     "start_time": "2024-07-17T22:35:21.778100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"lukasellinger/german_dpr_claim_verification_dissim-v1\"\n",
    "dataset = load_dataset(dataset_name).get('train')\n",
    "outputs = JSONLineReader().read(PROJECT_DIR.joinpath(\n",
    "    'dataset/openai/output/german_dpr/output_german_dpr_factscore-gpt3_5-turbo-gtr.jsonl'))\n",
    "outputs = {d['id']: d for d in outputs}"
   ],
   "id": "a31f0196f3196fa8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:35:29.521760Z",
     "start_time": "2024-07-17T22:35:29.489894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exclude_not_in_wiki = True\n",
    "\n",
    "gt_labels = []\n",
    "pr_labels = []\n",
    "for entry in dataset:\n",
    "    if exclude_not_in_wiki and entry['in_wiki'] == 'No':\n",
    "        continue\n",
    "    output = outputs[entry['id']]\n",
    "    pr_labels.append(Fact[output['predicted']].to_factuality())\n",
    "    gt_labels.append(Fact[output['label']].to_factuality())"
   ],
   "id": "fc892c04b3536c1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:35:31.451238Z",
     "start_time": "2024-07-17T22:35:31.412939Z"
    }
   },
   "cell_type": "code",
   "source": "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))",
   "id": "57ffb9943127e47a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5312    0.9855    0.6904        69\n",
      "           1     0.9091    0.1429    0.2469        70\n",
      "\n",
      "    accuracy                         0.5612       139\n",
      "   macro avg     0.7202    0.5642    0.4686       139\n",
      "weighted avg     0.7215    0.5612    0.4670       139\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0 Preparations\n",
    "Before starting, ensure that you have cloned the repository to your Google Drive.\n",
    "We will connect to this:"
   ],
   "id": "fb26ea97c9d115a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "repository = 'evaluating_factuality_word_definitions'\n",
    "\n",
    "%cd /content/drive/My Drive/{repository}"
   ],
   "id": "82121a98ee7c420c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we install the packages and import the modules needed in this notebook:",
   "id": "724dfbc3c9b0f439"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:04:30.506552Z",
     "start_time": "2024-08-01T22:04:19.278181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import PROJECT_DIR\n",
    "from general_utils.utils import print_classification_report, calc_bin_stats, rank_docs, print_fever_classification_report\n",
    "\n",
    "from general_utils.reader import JSONLineReader, JSONReader\n",
    "from pipeline_module.statement_verifier import ModelStatementVerifier\n",
    "from pipeline_module.evidence_selector import ModelEvidenceSelector\n",
    "from pipeline_module.translator import OpusMTTranslator\n",
    "from pipeline_module.sentence_connector import PhiSentenceConnector, ColonSentenceConnector\n",
    "from pipeline_module.evidence_fetcher import WikipediaEvidenceFetcher\n",
    "from pipeline_module.pipeline import Pipeline, FeverPipeline\n",
    "from pipeline_module.claim_splitter import DisSimSplitter, T5SplitRephraseSplitter, FactscoreSplitter"
   ],
   "id": "b7c7b2cf02eae43c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 Setup: Define Models and Datasets\n",
    "Now we define our models and datasets we want to evaluate:"
   ],
   "id": "e9de36e42353fec7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:04:30.513677Z",
     "start_time": "2024-08-01T22:04:30.509642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Base Models\n",
    "base_selection_model = 'Snowflake/snowflake-arctic-embed-m-long'\n",
    "base_verification_model = 'MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7'\n",
    "\n",
    "# Finetuned Models\n",
    "finetuned_selection_model = 'lukasellinger/evidence_selection_model-v3'\n",
    "finetuned_verification_model = 'lukasellinger/claim_verification_model-v3'"
   ],
   "id": "40d038ee87be8aed",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:04:42.977640Z",
     "start_time": "2024-08-01T22:04:30.515275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Datasets with language information\n",
    "datasets = {\n",
    "    'german_dpr-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german_dpr-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'german-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'squad-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/squad-claim_verification', split='test'),\n",
    "        'lang': 'en'\n",
    "    }\n",
    "}"
   ],
   "id": "26dff9eae25d4da9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.1 Evaluation Util Functions\n",
    "Next we define some helper functions"
   ],
   "id": "41f68e4fd9f3e8aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:04:42.996028Z",
     "start_time": "2024-08-01T22:04:42.983182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_pipeline(pipeline: Pipeline, dataset, batch_size=4, output_file_name='',\n",
    "                      only_intro=True):\n",
    "    outputs, report, not_in_wiki = pipeline.verify_test_dataset(dataset, batch_size,\n",
    "                                                                output_file_name, only_intro)\n",
    "\n",
    "    total_claim_count = sum(len(entry['atoms']) for entry in outputs if entry.get('atoms'))\n",
    "    total_entries_with_atoms = sum(1 for entry in outputs if entry.get('atoms'))\n",
    "\n",
    "    avg_claim_count = total_claim_count / total_entries_with_atoms if total_entries_with_atoms > 0 else 0\n",
    "\n",
    "    print_classification_report(report, not_in_wiki, avg_claim_count)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def calc_claim_lengths_stats(outputs: List[Dict]):\n",
    "    pr_labels, gt_labels, claim_lengths = [], [], []\n",
    "    for output in outputs:\n",
    "        pr_labels.append(1 if output['predicted'] == 'SUPPORTED' else 0)\n",
    "        gt_labels.append(1 if output['label'] == 'SUPPORTED' else 0)\n",
    "        claim_lengths.append(len(output['connected_claim'].split()))\n",
    "    return calc_bin_stats(pr_labels, gt_labels, claim_lengths)\n",
    "\n",
    "\n",
    "def calc_additional_stats(outputs: List[Dict], output_file_name=''):\n",
    "    evid_line_number_dist = defaultdict(int)\n",
    "    total_wikipedia = 0\n",
    "    total_wiktionary = 0\n",
    "    in_intro = 0\n",
    "\n",
    "    for output in outputs:\n",
    "        evidences = output.get('evidence', [])\n",
    "        for evidence in evidences:\n",
    "            if evidence.get('title').endswith('(wikipedia)'):\n",
    "                if evidence.get('in_intro'):\n",
    "                    in_intro += 1\n",
    "                total_wikipedia += 1\n",
    "            else:\n",
    "                total_wiktionary += 1\n",
    "\n",
    "            evid_line_number_dist[evidence.get('line_idx')] += 1\n",
    "    total_evidences = total_wikipedia + total_wiktionary\n",
    "\n",
    "    stats =  {\n",
    "        'evid_line_number_dist': dict(sorted(evid_line_number_dist.items())),\n",
    "        'claim_length_stats': calc_claim_lengths_stats(outputs),\n",
    "        'avg_sent_0_selected': evid_line_number_dist.get(0) / sum(evid_line_number_dist.values()),\n",
    "        'avg_in_intro': in_intro / total_wikipedia if total_wikipedia > 0 else 0,\n",
    "        'in_intro': in_intro,\n",
    "        'avg_wikipedia': total_wikipedia / total_evidences if total_evidences > 0 else 0,\n",
    "        'total_wikipedia': total_wikipedia,\n",
    "        'avg_wiktionary': total_wiktionary / total_evidences if total_evidences > 0 else 0,\n",
    "        'total_wiktionary': total_wiktionary\n",
    "    }\n",
    "    if output_file_name:\n",
    "        JSONReader().write(f'{output_file_name}.json', stats)\n",
    "        \n",
    "    return stats"
   ],
   "id": "7ff307516b01c8e9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.2 Initialization of the pipeline modules\n",
    "Here, we initialize the pipeline modules that will be used later. \n",
    "These modules will be loaded onto your device when the first inference step is performed. \n",
    "In this notebook, the translator and sentence connector are not directly utilized, but their results are already included in the datasets. We use their names to ensure the correct output is retrieved."
   ],
   "id": "d192c83fd39a9084"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:05:02.601967Z",
     "start_time": "2024-08-01T22:04:42.998828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "# Translator\n",
    "translator = OpusMTTranslator()\n",
    "\n",
    "# Sentence Connectors\n",
    "colon_sentence_connector = ColonSentenceConnector()\n",
    "phi_sentence_connector = PhiSentenceConnector()\n",
    "\n",
    "# Evidence Fetcher\n",
    "offline_evid_fetcher = WikipediaEvidenceFetcher()\n",
    "online_evid_fetcher = WikipediaEvidenceFetcher(offline=False)\n",
    "\n",
    "pipeline_models = {\n",
    "    'base': {\n",
    "        'evid_selector': ModelEvidenceSelector(model_name=base_selection_model),\n",
    "        'stm_verifier': ModelStatementVerifier(model_name=base_verification_model)\n",
    "    },\n",
    "    'finetuned': {\n",
    "        'evid_selector': ModelEvidenceSelector(model_name=finetuned_selection_model),\n",
    "        'stm_verifier': ModelStatementVerifier(model_name=finetuned_verification_model)\n",
    "    }\n",
    "}"
   ],
   "id": "a90688f2c34bad97",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 88 column 3",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 19\u001B[0m\n\u001B[1;32m      9\u001B[0m offline_evid_fetcher \u001B[38;5;241m=\u001B[39m WikipediaEvidenceFetcher()\n\u001B[1;32m     10\u001B[0m online_evid_fetcher \u001B[38;5;241m=\u001B[39m WikipediaEvidenceFetcher(offline\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     12\u001B[0m pipeline_models \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbase\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevid_selector\u001B[39m\u001B[38;5;124m'\u001B[39m: ModelEvidenceSelector(model_name\u001B[38;5;241m=\u001B[39mbase_selection_model),\n\u001B[1;32m     15\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstm_verifier\u001B[39m\u001B[38;5;124m'\u001B[39m: ModelStatementVerifier(model_name\u001B[38;5;241m=\u001B[39mbase_verification_model)\n\u001B[1;32m     16\u001B[0m     },\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinetuned\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevid_selector\u001B[39m\u001B[38;5;124m'\u001B[39m: ModelEvidenceSelector(model_name\u001B[38;5;241m=\u001B[39mfinetuned_selection_model),\n\u001B[0;32m---> 19\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstm_verifier\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mModelStatementVerifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfinetuned_verification_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     }\n\u001B[1;32m     21\u001B[0m }\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/pipeline_module/statement_verifier.py:64\u001B[0m, in \u001B[0;36mModelStatementVerifier.__init__\u001B[0;34m(self, model_name)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name \u001B[38;5;241m=\u001B[39m model_name \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMODEL_NAME\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 64\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:837\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    834\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    835\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTokenizer class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtokenizer_class_candidate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not exist or is not currently imported.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    836\u001B[0m         )\n\u001B[0;32m--> 837\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    839\u001B[0m \u001B[38;5;66;03m# Otherwise we have to be creative.\u001B[39;00m\n\u001B[1;32m    840\u001B[0m \u001B[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001B[39;00m\n\u001B[1;32m    841\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, EncoderDecoderConfig):\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2086\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2083\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2084\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2086\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2087\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2088\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2089\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2090\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2091\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2092\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2093\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2094\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2095\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_is_local\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2096\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2097\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2098\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2325\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2323\u001B[0m \u001B[38;5;66;03m# Instantiate the tokenizer.\u001B[39;00m\n\u001B[1;32m   2324\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2325\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m   2327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m   2328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load vocabulary from file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2329\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2330\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py:133\u001B[0m, in \u001B[0;36mDebertaV2TokenizerFast.__init__\u001B[0;34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    120\u001B[0m     vocab_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    132\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_lower_case\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_lower_case\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbos_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m        \u001B[49m\u001B[43munk_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munk_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[43m        \u001B[49m\u001B[43msep_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcls_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcls_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_by_punct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_by_punct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_lower_case \u001B[38;5;241m=\u001B[39m do_lower_case\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_by_punct \u001B[38;5;241m=\u001B[39m split_by_punct\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:111\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    108\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(tokenizer_object)\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m fast_tokenizer_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m from_slow:\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001B[39;00m\n\u001B[0;32m--> 111\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mTokenizerFast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfast_tokenizer_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m slow_tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001B[39;00m\n\u001B[1;32m    114\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001B[0;31mException\u001B[0m: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 88 column 3"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2 Evaluating Finetuned vs. Base Pipeline (Optimal Setup)\n",
    "\n",
    "In this section, we assess the performance of the base and finetuned pipeline using the identified optimal setup.\n",
    "\n",
    "#### Datasets and Models\n",
    "\n",
    "- **Datasets:**\n",
    "  - `lukasellinger/german_dpr-claim_verification`\n",
    "  - `lukasellinger/german-claim_verification`\n",
    "  - `lukasellinger/squad-claim_verification`\n",
    "\n",
    "- **Base Models:**\n",
    "  - Selection: `Snowflake/snowflake-arctic-embed-m-long`\n",
    "  - Verification: `MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7`\n",
    "\n",
    "- **Finetuned Models:**\n",
    "  - Selection: `lukasellinger/evidence_selection_model-v2`\n",
    "  - Verification: `lukasellinger/claim_verification_model-v1`\n",
    "\n",
    "#### Evaluation Strategy\n",
    "\n",
    "- **Configuration:**\n",
    "  - Using `OpusMTTranslator`\n",
    "  - Using `PhiSentenceConnector`\n",
    "  - No claim splitting\n",
    "  - Offline Evidence Fetcher for reproducibility (state of 08.07.2024)"
   ],
   "id": "b515e0bcde2b6a25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T21:51:54.718110Z",
     "start_time": "2024-08-01T21:51:54.683819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file_base_name = str(PROJECT_DIR / \"data/evaluation/{dataset}_{model}\")\n",
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "    for model_name, models in pipeline_models.items():\n",
    "        print(f\"Evaluating {dataset_name} with pipeline {model_name}...\")\n",
    "        pipeline = Pipeline(translator=translator,\n",
    "                            sent_connector=phi_sentence_connector,\n",
    "                            claim_splitter=None,\n",
    "                            evid_fetcher=offline_evid_fetcher,\n",
    "                            evid_selector=models.get('evid_selector'),\n",
    "                            stm_verifier=models.get('stm_verifier'),\n",
    "                            lang=lang)\n",
    "        outputs = evaluate_pipeline(pipeline, dataset,\n",
    "                                    output_file_name=output_file_base_name.format(\n",
    "                                        dataset=dataset_name, model=model_name))\n",
    "        additional_stats = calc_additional_stats(outputs, f'{output_file_base_name.format(dataset=dataset_name, model=model_name)}_additional_stats')"
   ],
   "id": "cfc5253e611023ba",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m dataset \u001B[38;5;241m=\u001B[39m config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      4\u001B[0m lang \u001B[38;5;241m=\u001B[39m config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlang\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_name, models \u001B[38;5;129;01min\u001B[39;00m \u001B[43mpipeline_models\u001B[49m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with pipeline \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m     pipeline \u001B[38;5;241m=\u001B[39m Pipeline(translator\u001B[38;5;241m=\u001B[39mtranslator,\n\u001B[1;32m      8\u001B[0m                         sent_connector\u001B[38;5;241m=\u001B[39mphi_sentence_connector,\n\u001B[1;32m      9\u001B[0m                         claim_splitter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m                         stm_verifier\u001B[38;5;241m=\u001B[39mmodels\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstm_verifier\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     13\u001B[0m                         lang\u001B[38;5;241m=\u001B[39mlang)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pipeline_models' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3 Evaluating Finetuned Pipeline Online (Best Setup)\n",
    "In the previous section, we evaluated our finetuned pipeline using the offline evidence fetcher. We can also connect our pipeline to the Wikipedia API to retrieve the most current knowledge available.\n",
    "\n",
    "Let's check on that:"
   ],
   "id": "35c70a72c490dc6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_base_name = str(PROJECT_DIR / \"data/evaluation/{dataset}_finetuned_online\")\n",
    "\n",
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "    print(f\"Evaluating {dataset_name}...\")\n",
    "    pipeline = Pipeline(translator=translator,\n",
    "                        sent_connector=phi_sentence_connector,\n",
    "                        claim_splitter=None,\n",
    "                        evid_fetcher=online_evid_fetcher,\n",
    "                        evid_selector=pipeline_models['finetuned']['evid_selector'],\n",
    "                        stm_verifier=pipeline_models['finetuned']['stm_verifier'],\n",
    "                        lang=lang)\n",
    "    evaluate_pipeline(pipeline, dataset,\n",
    "                      output_file_name=output_file_base_name.format(dataset=dataset_name))"
   ],
   "id": "48756de4e1d0b58c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4 Evaluation of Different Claim Splitters\n",
    "\n",
    "A sentence can be split into multiple facts, where the combination of these facts represents the entire sentence.\n",
    "\n",
    "For this evaluation, we test four different splitters:\n",
    "\n",
    "- **`DisSimSplitter`**: Based on [DiscourseSimplification](https://github.com/Lambda-3/DiscourseSimplification)\n",
    "- **`T5SplitRephraseSplitter`**: Based on [T5 Split and Rephrase](https://huggingface.co/unikei/t5-base-split-and-rephrase)\n",
    "- **`FactscoreSplitter`**: Based on [FActScore](https://github.com/shmsw25/FActScore)\n",
    "- **`None`**: No splitting"
   ],
   "id": "e876b9d09804e464"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T16:35:02.679325Z",
     "start_time": "2024-08-01T16:35:01.613409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "claim_splitters = {\n",
    "    'DisSimSplitter': DisSimSplitter(),\n",
    "    'T5SplitRephraseSplitter': T5SplitRephraseSplitter(),\n",
    "    'FactscoreSplitter': FactscoreSplitter(),\n",
    "    'None': None\n",
    "}"
   ],
   "id": "73ac75db95a6d25b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this setting, the splits are not calculated. Instead, they are reused as they have already been precomputed and are present in the datasets.",
   "id": "e10a3b3786578985"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T16:36:14.522712Z",
     "start_time": "2024-08-01T16:35:15.028632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file_base_name = str(PROJECT_DIR / \"data/evaluation/{dataset}_finetuned_{splitter}\")\n",
    "\n",
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "    for name, splitter in claim_splitters.items():\n",
    "        print(f\"Evaluating {dataset_name} with claim splitter {name}...\")\n",
    "        pipeline = Pipeline(translator=translator,\n",
    "                            sent_connector=phi_sentence_connector,\n",
    "                            claim_splitter=splitter,\n",
    "                            evid_fetcher=offline_evid_fetcher,\n",
    "                            evid_selector=pipeline_models['finetuned']['evid_selector'],\n",
    "                            stm_verifier=pipeline_models['finetuned']['stm_verifier'],\n",
    "                            lang=lang)\n",
    "        evaluate_pipeline(pipeline, dataset,\n",
    "                          output_file_name=output_file_base_name.format(dataset=dataset_name,\n",
    "                                                                        splitter=name))"
   ],
   "id": "54be8e1ab3f695c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating german_dpr-claim_verification with claim splitter DisSimSplitter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]<All keys matched successfully>\n",
      " 10%|â–‰         | 4/42 [00:57<09:10, 14.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 15\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with claim splitter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m Pipeline(translator\u001B[38;5;241m=\u001B[39mtranslator,\n\u001B[1;32m      9\u001B[0m                     sent_connector\u001B[38;5;241m=\u001B[39mphi_sentence_connector,\n\u001B[1;32m     10\u001B[0m                     claim_splitter\u001B[38;5;241m=\u001B[39msplitter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m                     stm_verifier\u001B[38;5;241m=\u001B[39mpipeline_models[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinetuned\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstm_verifier\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     14\u001B[0m                     lang\u001B[38;5;241m=\u001B[39mlang)\n\u001B[0;32m---> 15\u001B[0m \u001B[43mevaluate_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpipeline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m                  \u001B[49m\u001B[43moutput_file_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_file_base_name\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m                                                                \u001B[49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m, in \u001B[0;36mevaluate_pipeline\u001B[0;34m(pipeline, dataset, batch_size, output_file_name, only_intro)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate_pipeline\u001B[39m(pipeline: Pipeline, dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, output_file_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      2\u001B[0m                       only_intro\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m----> 3\u001B[0m     outputs, report, not_in_wiki \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverify_test_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                                                \u001B[49m\u001B[43moutput_file_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     total_claim_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mlen\u001B[39m(entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124matoms\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m outputs \u001B[38;5;28;01mif\u001B[39;00m entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124matoms\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      7\u001B[0m     total_entries_with_atoms \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m outputs \u001B[38;5;28;01mif\u001B[39;00m entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124matoms\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/pipeline_module/pipeline.py:186\u001B[0m, in \u001B[0;36mPipeline.verify_test_dataset\u001B[0;34m(self, dataset, batch_size, output_file, only_intro)\u001B[0m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(dataset), batch_size)):\n\u001B[1;32m    185\u001B[0m     batch \u001B[38;5;241m=\u001B[39m dataset[i:i \u001B[38;5;241m+\u001B[39m batch_size]\n\u001B[0;32m--> 186\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverify_test_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_intro\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m output:\n\u001B[1;32m    189\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredicted\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/pipeline_module/pipeline.py:134\u001B[0m, in \u001B[0;36mPipeline.verify_test_batch\u001B[0;34m(self, batch, only_intro)\u001B[0m\n\u001B[1;32m    127\u001B[0m         filtered_batch\u001B[38;5;241m.\u001B[39mappend(entry)\n\u001B[1;32m    129\u001B[0m evid_fetcher_input \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m: entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    130\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtranslated_word\u001B[39m\u001B[38;5;124m'\u001B[39m: entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish_word\u001B[39m\u001B[38;5;124m'\u001B[39m, entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n\u001B[1;32m    131\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msearch_word\u001B[39m\u001B[38;5;124m'\u001B[39m: entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument_search_word\u001B[39m\u001B[38;5;124m'\u001B[39m]} \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m\n\u001B[1;32m    132\u001B[0m                       filtered_batch]\n\u001B[0;32m--> 134\u001B[0m _, evids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevid_fetcher\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevid_fetcher_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_lang\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_intro\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m filtered_batch:\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/pipeline_module/evidence_fetcher.py:21\u001B[0m, in \u001B[0;36mEvidenceFetcher.__call__\u001B[0;34m(self, batch, only_intro, word_lang)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: List[Dict], only_intro: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, word_lang: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mde\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     13\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124;03m    Fetch evidences for a batch of words.\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m    :return: Tuple of lists: evidence words and evidence details.\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch_evidences_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_lang\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/pipeline_module/evidence_fetcher.py:114\u001B[0m, in \u001B[0;36mWikipediaEvidenceFetcher.fetch_evidences_batch\u001B[0;34m(self, batch, only_intro, word_lang, offline)\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m entry \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m batch), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m missing in batch entries\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# Fetch evidences for each entry in the batch\u001B[39;00m\n\u001B[0;32m--> 114\u001B[0m evidence_batch \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    115\u001B[0m     {\n\u001B[1;32m    116\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m: wiki_word,\n\u001B[1;32m    117\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevidences\u001B[39m\u001B[38;5;124m'\u001B[39m: [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m: page, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mline_indices\u001B[39m\u001B[38;5;124m'\u001B[39m: [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(lines))], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlines\u001B[39m\u001B[38;5;124m'\u001B[39m: lines} \u001B[38;5;28;01mfor\u001B[39;00m page, lines \u001B[38;5;129;01min\u001B[39;00m texts],\n\u001B[1;32m    118\u001B[0m     }\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m batch\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m texts, wiki_word \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwiki\u001B[38;5;241m.\u001B[39mget_pages(\n\u001B[1;32m    121\u001B[0m         word\u001B[38;5;241m=\u001B[39mentry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m    122\u001B[0m         fallback_word\u001B[38;5;241m=\u001B[39mentry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtranslated_word\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m    123\u001B[0m         word_lang\u001B[38;5;241m=\u001B[39mword_lang,\n\u001B[1;32m    124\u001B[0m         only_intro\u001B[38;5;241m=\u001B[39monly_intro,\n\u001B[1;32m    125\u001B[0m         search_word\u001B[38;5;241m=\u001B[39mentry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msearch_word\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffline \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    126\u001B[0m     )]\n\u001B[1;32m    127\u001B[0m ]\n\u001B[1;32m    129\u001B[0m \u001B[38;5;66;03m# Unpack evidences and words\u001B[39;00m\n\u001B[1;32m    130\u001B[0m evid_words \u001B[38;5;241m=\u001B[39m [entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m evidence_batch]\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/pipeline_module/evidence_fetcher.py:120\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m entry \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m batch), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m missing in batch entries\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# Fetch evidences for each entry in the batch\u001B[39;00m\n\u001B[1;32m    114\u001B[0m evidence_batch \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    115\u001B[0m     {\n\u001B[1;32m    116\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m: wiki_word,\n\u001B[1;32m    117\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevidences\u001B[39m\u001B[38;5;124m'\u001B[39m: [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m: page, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mline_indices\u001B[39m\u001B[38;5;124m'\u001B[39m: [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(lines))], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlines\u001B[39m\u001B[38;5;124m'\u001B[39m: lines} \u001B[38;5;28;01mfor\u001B[39;00m page, lines \u001B[38;5;129;01min\u001B[39;00m texts],\n\u001B[1;32m    118\u001B[0m     }\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m batch\n\u001B[0;32m--> 120\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m texts, wiki_word \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwiki\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_pages\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mentry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mword\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfallback_word\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mentry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtranslated_word\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m        \u001B[49m\u001B[43mword_lang\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mword_lang\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[43m        \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_intro\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m        \u001B[49m\u001B[43msearch_word\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mentry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msearch_word\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moffline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m    127\u001B[0m ]\n\u001B[1;32m    129\u001B[0m \u001B[38;5;66;03m# Unpack evidences and words\u001B[39;00m\n\u001B[1;32m    130\u001B[0m evid_words \u001B[38;5;241m=\u001B[39m [entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m evidence_batch]\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/fetchers/wikipedia.py:216\u001B[0m, in \u001B[0;36mWikipedia.get_pages\u001B[0;34m(self, word, fallback_word, word_lang, only_intro, split_level, return_raw, search_word)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_pages\u001B[39m(\u001B[38;5;28mself\u001B[39m, word: \u001B[38;5;28mstr\u001B[39m, fallback_word: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, word_lang: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    213\u001B[0m               only_intro\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    214\u001B[0m               split_level\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m, return_raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, search_word\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffline_backend \u001B[38;5;129;01mand\u001B[39;00m search_word:\n\u001B[0;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_pages_offline\u001B[49m\u001B[43m(\u001B[49m\u001B[43msearch_word\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_raw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_pages_online(word, fallback_word, word_lang, only_intro, split_level,\n\u001B[1;32m    219\u001B[0m                                      return_raw)\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/fetchers/wikipedia.py:233\u001B[0m, in \u001B[0;36mWikipedia.get_pages_offline\u001B[0;34m(self, search_word, only_intro, return_raw, split_level)\u001B[0m\n\u001B[1;32m    231\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m         text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clean_text(text)\n\u001B[0;32m--> 233\u001B[0m         texts\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_split_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtitle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_level\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(texts\u001B[38;5;241m.\u001B[39mitems()), search_word\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/fetchers/wikipedia.py:170\u001B[0m, in \u001B[0;36mWikipedia._split_text\u001B[0;34m(self, title, site, text, split_level, sentence_limit)\u001B[0m\n\u001B[1;32m    168\u001B[0m     texts \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m: passage \u001B[38;5;28;01mfor\u001B[39;00m i, passage \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(passages)}\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m split_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 170\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m \u001B[43msplit_into_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m     texts[key_base] \u001B[38;5;241m=\u001B[39m sentences[:sentence_limit]\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# split_level == 'none'\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/general_utils/spacy_utils.py:100\u001B[0m, in \u001B[0;36msplit_into_sentences\u001B[0;34m(txt, lang)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msplit_into_sentences\u001B[39m(txt: \u001B[38;5;28mstr\u001B[39m, lang: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m     99\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Split a text into sentences.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 100\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mget_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtxt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [sent\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m doc\u001B[38;5;241m.\u001B[39msents]\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/general_utils/spacy_utils.py:37\u001B[0m, in \u001B[0;36mget_doc\u001B[0;34m(txt, lang)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_doc\u001B[39m(txt: \u001B[38;5;28mstr\u001B[39m, lang: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m lang \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 37\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtxt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m lang \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mde\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m german_nlp(txt)\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/spacy/language.py:1049\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[0;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[1;32m   1047\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[1;32m   1048\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1049\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1051\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[1;32m   1052\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001B[0m, in \u001B[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:264\u001B[0m, in \u001B[0;36mspacy.pipeline.transition_parser.Parser.predict\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:285\u001B[0m, in \u001B[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/thinc/model.py:334\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m OutT:\n\u001B[1;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/spacy/ml/tb_framework.py:34\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(model, X, is_train):\n\u001B[0;32m---> 34\u001B[0m     step_model \u001B[38;5;241m=\u001B[39m \u001B[43mParserStepModel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m        \u001B[49m\u001B[43munseen_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43munseen_classes\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_upper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhas_upper\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m step_model, step_model\u001B[38;5;241m.\u001B[39mfinish_steps\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/spacy/ml/parser_model.pyx:247\u001B[0m, in \u001B[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/thinc/model.py:115\u001B[0m, in \u001B[0;36mModel.__init__\u001B[0;34m(self, name, forward, init, dims, params, layers, shims, attrs, refs, ops)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shims \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(shims)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# Take care to increment the base class here! It needs to be unique\u001B[39;00m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;66;03m# across all models.\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Model\u001B[38;5;241m.\u001B[39mglobal_id_lock:\n\u001B[1;32m    116\u001B[0m     Model\u001B[38;5;241m.\u001B[39mglobal_id \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mid \u001B[38;5;241m=\u001B[39m Model\u001B[38;5;241m.\u001B[39mglobal_id\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5 Whole Wiki Page Run (Best Setup)\n",
    "\n",
    "In our previous setting, we limited the text fetched to the intro sections of Wikipedia pages. However, we could extend this to use the entire Wikipedia page, which would provide more comprehensive information but would significantly increase the processing time.\n",
    "\n",
    "Let us try it out:\n"
   ],
   "id": "ad649a80875a1c04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_base_name = str(PROJECT_DIR / \"data/evaluation/{dataset}_finetuned_whole_page\")\n",
    "\n",
    "for dataset_name, config in datasets.items():\n",
    "    print(f\"Evaluating {dataset_name}...\")\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "\n",
    "    pipeline = Pipeline(translator=translator,\n",
    "                        sent_connector=phi_sentence_connector,\n",
    "                        claim_splitter=None,\n",
    "                        evid_fetcher=offline_evid_fetcher,\n",
    "                        evid_selector=pipeline_models['finetuned']['evid_selector'],\n",
    "                        stm_verifier=pipeline_models['finetuned']['stm_verifier'],\n",
    "                        lang=lang)\n",
    "    outputs = evaluate_pipeline(pipeline, dataset,\n",
    "                                output_file_name=output_file_base_name.format(dataset=dataset_name),\n",
    "                                only_intro=False)\n",
    "    additional_stats = calc_additional_stats(outputs, f'{output_file_base_name.format(dataset=dataset_name)}_additional_stats')"
   ],
   "id": "68275aae2ea98bfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6 Evaluate FEVER Score\n",
    "\n",
    "Our models were trained on the FEVER dataset. To assess their performance, we use the FEVER Score, a metric specifically designed to evaluate fact-checking systems. The FEVER Score measures both the accuracy of claim verification and the relevance of the retrieved evidence.\n",
    "\n",
    "The FEVER Score for a sample is determined as follows:\n",
    "- **Score of 1**: If at least one group of evidence is correctly identified among the selected evidence sentences and the predicted label (Supported, Refuted) matches the true label.\n",
    "- **Score of 0**: If the above conditions are not met.\n",
    "\n",
    "In this section, we will evaluate and compare the FEVER Scores of our base and finetuned pipelines."
   ],
   "id": "5b07f26f32fc5455"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T17:11:52.098712Z",
     "start_time": "2024-07-31T17:11:52.093105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_fever_pipeline(pipeline: FeverPipeline, dataset, batch_size=4, output_file_name=''):\n",
    "    outputs, report, fever_report = pipeline.verify_test_dataset(dataset, batch_size, output_file_name)\n",
    "    print_fever_classification_report(report, fever_report)\n",
    "    return outputs"
   ],
   "id": "2d2597150da251ff",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T17:12:02.227842Z",
     "start_time": "2024-07-31T17:11:53.128676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file_base_name = str(PROJECT_DIR / \"data/evaluation/fever_{name}\")\n",
    "dataset = load_dataset(\"lukasellinger/fever_evidence_selection-v1\", split='test')\n",
    "dataset = dataset.map(lambda entry: {'label': 'SUPPORTED' if entry['label'] == 'SUPPORTS' else 'NOT_SUPPORTED'})"
   ],
   "id": "4cd020610ab38ba0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T18:51:32.174693Z",
     "start_time": "2024-07-31T17:12:08.451918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, models in pipeline_models.items():\n",
    "    print(f\"Evaluating with {name}...\")\n",
    "    pipeline = FeverPipeline(claim_splitter=None,\n",
    "                             evid_selector=models.get('evid_selector'),\n",
    "                             stm_verifier=models.get('stm_verifier'))\n",
    "    evaluate_fever_pipeline(pipeline, dataset, output_file_name=output_file_base_name.format(name=name))"
   ],
   "id": "c8ebb8939b563e43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/422 [00:00<?, ?it/s]A new version of the following files was downloaded from https://huggingface.co/Snowflake/snowflake-arctic-embed-m-long:\n",
      "- configuration_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Snowflake/snowflake-arctic-embed-m-long:\n",
      "- modeling_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "<All keys matched successfully>\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 422/422 [1:20:08<00:00, 11.39s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "FeverScore: 0.8943620178041543\n",
      "Gold FeverScore: 0.9448071216617211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9324    0.9462    0.9392       874\n",
      "           1     0.9411    0.9260    0.9335       811\n",
      "\n",
      "    accuracy                         0.9365      1685\n",
      "   macro avg     0.9367    0.9361    0.9364      1685\n",
      "weighted avg     0.9366    0.9365    0.9365      1685\n",
      "\n",
      "################################\n",
      "Evaluating with finetuned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/422 [00:00<?, ?it/s]<All keys matched successfully>\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 422/422 [19:14<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "FeverScore: 0.913353115727003\n",
      "Gold FeverScore: 0.9614243323442137\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9601    0.9371    0.9485       874\n",
      "           1     0.9339    0.9581    0.9458       811\n",
      "\n",
      "    accuracy                         0.9472      1685\n",
      "   macro avg     0.9470    0.9476    0.9471      1685\n",
      "weighted avg     0.9475    0.9472    0.9472      1685\n",
      "\n",
      "################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 7 Check on incorrect predicted\n",
    "\n",
    "Similar to the approach used in FactScore, we sample 30 incorrect predictions from our test sets, excluding those cases where no evidence is available on Wikipedia. These sampled examples are saved to a separate file for detailed manual evaluation. The analysis focuses on identifying the following types of issues:\n",
    "- No direct evidence from retrieved passages\n",
    "- Atomic fact is contex-dependent\n",
    "- Distracted by other passages\n",
    "- Wrong prediction even with right passage\n",
    "- Error in pipeline (e.g translation error)\n",
    "- Annotation Error"
   ],
   "id": "c40445ac2058f9a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T11:23:52.296432Z",
     "start_time": "2024-07-30T11:23:52.290927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sample_incorrect_predictions(dataset_names: List[str], file_pattern: str, output_file: str,\n",
    "                                 sample_size: int = 30, seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Samples incorrect predictions from test sets and saves them to a file for manual assessment.\n",
    "\n",
    "    Args:\n",
    "        dataset_names (List[str]): List of dataset names to process.\n",
    "        file_pattern (str): Pattern to match file names for loading predictions.\n",
    "        output_file (str): Path to the output file for saving the sampled incorrect predictions.\n",
    "        sample_size (int): Number of incorrect predictions to sample.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    all_outputs = []\n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "        file_path = PROJECT_DIR / file_pattern.format(dataset_name=dataset_name)\n",
    "        outputs = JSONLineReader().read(file_path)\n",
    "        all_outputs.extend(outputs)\n",
    "    false_predicted_outputs = [output for output in all_outputs if\n",
    "                               output['label'] != output['predicted'] and output['predicted'] != -1]\n",
    "\n",
    "    sample_size = min(sample_size, len(false_predicted_outputs))\n",
    "    sampled_outputs = random.sample(false_predicted_outputs, sample_size)\n",
    "\n",
    "    # Print sampled outputs for review\n",
    "    print(f\"Sampled {len(sampled_outputs)} incorrect predictions:\")\n",
    "    for sample in sampled_outputs:\n",
    "        print(json.dumps(sample, indent=4))\n",
    "        print('############################')\n",
    "    JSONLineReader().write(output_file, sampled_outputs)"
   ],
   "id": "dcf4e52812a1cc42",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T11:23:53.105453Z",
     "start_time": "2024-07-30T11:23:53.078100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample incorrect predictions from the finetuned pipeline without Claim Splitting\n",
    "sample_incorrect_predictions(\n",
    "    dataset_names=list(datasets.keys()),\n",
    "    file_pattern=\"data/evaluation/{dataset_name}_finetuned.jsonl\",\n",
    "    output_file=str(PROJECT_DIR / \"data/evaluation/incorrect_pred_samples.jsonl\")\n",
    ")\n",
    "\n",
    "# Sample incorrect predictions from models with DisSimSplitter\n",
    "sample_incorrect_predictions(\n",
    "    dataset_names=list(datasets.keys()),\n",
    "    file_pattern=\"data/evaluation/{dataset_name}_finetuned_DisSimSplitter.jsonl\",\n",
    "    output_file=str(\n",
    "        PROJECT_DIR / \"data/evaluation/incorrect_pred_samples_DisSimSplitter.jsonl\")\n",
    ")"
   ],
   "id": "652294f80392faf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 30 incorrect predictions:\n",
      "{\n",
      "    \"id\": 15,\n",
      "    \"word\": \"Starbright Foundation\",\n",
      "    \"claim\": \"schwer kranken Kindern hilft\",\n",
      "    \"connected_claim\": \"The Starbright Foundation symbolizes the support and care provided to severely ill children.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"The Starbright Foundation symbolizes the support and care provided to severely ill children.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Starlight Children's Foundation (wikipedia)\",\n",
      "            \"line_idx\": 1,\n",
      "            \"text\": \"Starlight's programs include providing hospital wear, games, and deliveries to hospitalized children.\",\n",
      "            \"sim\": 0.6311880946159363,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Starlight Children's Foundation (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Starlight Children's Foundation is a nonprofit organization founded in 1982.\",\n",
      "            \"sim\": 0.6104554533958435,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Starlight Children's Foundation (wikipedia)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"The programs are provided directly to children through Starlight's network of more than 700 children's hospitals and other community health partners throughout the world.\",\n",
      "            \"sim\": 0.586753249168396,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 42,\n",
      "    \"word\": \"proto-neolithisch\",\n",
      "    \"claim\": \"Diese Vorstufe zur produzierenden Landwirtschaft\",\n",
      "    \"connected_claim\": \"Proto-neolithic denotes this precursor to producing agriculture.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Proto-neolithic denotes this precursor to producing agriculture.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Pre-Pottery Neolithic A (wikipedia)\",\n",
      "            \"line_idx\": 3,\n",
      "            \"text\": \"The Pre-Pottery Neolithic A and the following Pre-Pottery Neolithic B (PPNB) were originally defined by Kathleen Kenyon in the type site of Jericho, State of Palestine.\",\n",
      "            \"sim\": 0.6079156994819641,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Pre-Pottery Neolithic A (wikipedia)\",\n",
      "            \"line_idx\": 5,\n",
      "            \"text\": \"They precede the ceramic Neolithic Yarmukian culture.\",\n",
      "            \"sim\": 0.6027340888977051,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Pre-Pottery Neolithic A (wikipedia)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"The time period is characterized by tiny circular mud-brick dwellings, the cultivation of crops, the hunting of wild game, and unique burial customs in which bodies were buried below the floors of dwellings.\",\n",
      "            \"sim\": 0.5867552161216736,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 13,\n",
      "    \"word\": \"Vakuum\",\n",
      "    \"claim\": \"in der technischen Praxis ein Raum mit weitgehender Abwesenheit von Materie \\u2013 im Vakuum gibt es keine festen Objekte oder Fl\\u00fcssigkeit, extrem wenig Gas und damit auch einen extrem niedrigen Gasdruck\",\n",
      "    \"connected_claim\": \"vacuum: in technical practice a room with extensive absence of matter \\u2013 in vacuum there are no solid objects or liquid, extremely little gas and thus also an extremely low gas pressure\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"vacuum: in technical practice a room with extensive absence of matter \\u2013 in vacuum there are no solid objects or liquid, extremely little gas and thus also an extremely low gas pressure\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Vacuum (wikipedia)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure.\",\n",
      "            \"sim\": 0.7078853249549866,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Vacuum (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"A vacuum (pl.: vacuums or vacua) is  space devoid of matter.\",\n",
      "            \"sim\": 0.6949751973152161,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Vacuum (disambiguation) (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Vacuum is the absence of matter.\",\n",
      "            \"sim\": 0.6893324851989746,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 50,\n",
      "    \"word\": \"Broadway\",\n",
      "    \"claim\": \"das Theaterviertel am Times Square zwischen der 41st Street und 53rd Street und zwischen der Sixth und Ninth Avenue\",\n",
      "    \"connected_claim\": \"Broadway represents the theatre district on Times Square between 41st Street and 53rd Street and between Sixth and Ninth Avenue.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Broadway represents the theatre district on Times Square between 41st Street and 53rd Street and between Sixth and Ninth Avenue.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Broadway (Brooklyn) (wikipedia)\",\n",
      "            \"line_idx\": 4,\n",
      "            \"text\": \"Broadway forms the boundary between the neighborhoods of Bushwick, which lies above Broadway to the northeast, and Bedford\\u2013Stuyvesant, which is to the southwest.\",\n",
      "            \"sim\": 0.6014193892478943,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Broadway (Brooklyn) (wikipedia)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"The East New York terminus is a complicated intersection with East New York Avenue, Fulton Street, Jamaica Avenue, and Alabama Avenue.\",\n",
      "            \"sim\": 0.595174252986908,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Broadway (Brooklyn) (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Broadway is an avenue in the New York City borough of Brooklyn that extends from the East River in the neighborhood of Williamsburg in a southeasterly direction to East New York for a length of 4.32 miles (6.95 km).\",\n",
      "            \"sim\": 0.5890106558799744,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 150,\n",
      "    \"word\": \"Urum\",\n",
      "    \"claim\": \"\\u00fcberwiegend aus Teig und einer w\\u00fcrzigen F\\u00fcllung bestehende Gericht\",\n",
      "    \"connected_claim\": \"Urums mainly represents a dish made of dough and a spicy filling.\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Urums mainly represents a dish made of dough and a spicy filling.\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Urum (wiktionary)\",\n",
      "            \"line_idx\": 3,\n",
      "            \"text\": \"Urum.\",\n",
      "            \"sim\": 0.5646771192550659,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Urum (wiktionary)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"IPA(key): /u\\u02d0\\u02c8\\u0279u\\u02d0m/.\",\n",
      "            \"sim\": 0.5489026308059692,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Urum (wiktionary)\",\n",
      "            \"line_idx\": 7,\n",
      "            \"text\": \"639-3 code uum (SIL).Ethnologue entry for Urum, uum \\u2060.\\u0410\\u043b\\u0435\\u043a\\u0441\\u0435\\u0439 \\u041a\\u0430\\u0437\\u0430\\u043a\\u043e\\u0432\",\n",
      "            \"sim\": 0.5482784509658813,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 120,\n",
      "    \"word\": \"Grand Central Terminal\",\n",
      "    \"claim\": \"umgangssprachlich die Nutzung von verschiedenen Energien in f\\u00fcr Menschen gut verwendbaren Formen\",\n",
      "    \"connected_claim\": \"Grand Central Terminal symbolizes the colloquial use of different energies in forms that are suitable for humans.\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Grand Central Terminal symbolizes the colloquial use of different energies in forms that are suitable for humans.\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Grand Central Terminal (GCT; also referred to as Grand Central Station or simply as Grand Central) is a commuter rail terminal located at 42nd Street and Park Avenue in Midtown Manhattan, New York City.\",\n",
      "            \"sim\": 0.5338200926780701,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 8,\n",
      "            \"text\": \"The terminal's Main Concourse is often used as a meeting place, and is especially featured in films and television.\",\n",
      "            \"sim\": 0.5320994257926941,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 5,\n",
      "            \"text\": \"The distinctive architecture and interior design of Grand Central Terminal's station house have earned it several landmark designations, including as a National Historic Landmark.\",\n",
      "            \"sim\": 0.516218900680542,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 84,\n",
      "    \"word\": \"Prim\\u00e4rtuberkulose\",\n",
      "    \"claim\": \"Krankheitszeichen, die sich direkt nach der Infektion manifestieren\",\n",
      "    \"connected_claim\": \"Primary tuberculosis signifies signs of disease that manifest immediately after infection.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Primary tuberculosis signifies signs of disease that manifest immediately after infection.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Tuberculosis (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Tuberculosis (TB), also known colloquially as the \\\"white death\\\", or historically as consumption, is an infectious disease usually caused by Mycobacterium tuberculosis (MTB) bacteria.\",\n",
      "            \"sim\": 0.606168270111084,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Tuberculosis (wikipedia)\",\n",
      "            \"line_idx\": 10,\n",
      "            \"text\": \"Diagnosis of latent TB relies on the tuberculin skin test (TST) or blood tests.\",\n",
      "            \"sim\": 0.5917250514030457,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Tuberculosis (wikipedia)\",\n",
      "            \"line_idx\": 4,\n",
      "            \"text\": \"Typical symptoms of active TB are chronic cough with blood-containing mucus, fever, night sweats, and weight loss.\",\n",
      "            \"sim\": 0.5826032161712646,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 46,\n",
      "    \"word\": \"Splintholz\",\n",
      "    \"claim\": \"Bereich des Stammes, der aktiv am ''Wasser''- und ''N\\u00e4hrstofftransport'' und der Speicherung teilnimmt\",\n",
      "    \"connected_claim\": \"Sapwood denotes the area of the trunk that actively participates in ''water'' and ''nutrient transport'' and storage.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Sapwood denotes the area of the trunk that actively participates in ''water'' and ''nutrient transport'' and storage.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"sapwood (wiktionary)\",\n",
      "            \"line_idx\": 3,\n",
      "            \"text\": \"The wood just under the bark of a stem or branch; it differs from the heartwood in color and in physiologic activity (flow of sap).\",\n",
      "            \"sim\": 0.6544414162635803,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"sapwood (wiktionary)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"(UK) IPA(key): /\\u02c8sapw\\u028ad/. sapwood (countable and uncountable, plural sapwoods).\",\n",
      "            \"sim\": 0.6465111374855042,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"sapwood (wiktionary)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"sap-wood.\",\n",
      "            \"sim\": 0.6266425848007202,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 143,\n",
      "    \"word\": \"Seidenstra\\u00dfe\",\n",
      "    \"claim\": \"baumartig verzweigende Strukturen\",\n",
      "    \"connected_claim\": \"The silk road signifies tree-like branching structures.\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"The silk road signifies tree-like branching structures.\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Silk Road (wikipedia)\",\n",
      "            \"line_idx\": 19,\n",
      "            \"text\": \"In June 2014, UNESCO designated the Chang'an-Tianshan corridor of the Silk Road as a World Heritage Site, while the Indian portion remains on the tentative site list.\",\n",
      "            \"sim\": 0.6038500070571899,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Silk Road (disambiguation) (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"The Silk Road is a number of trade routes across the Eurasian landmass.\",\n",
      "            \"sim\": 0.5964723229408264,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Silk Road (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"The Silk Road (Chinese: \\u4e1d\\u7ef8\\u4e4b\\u8def) was a network of Eurasian trade routes active from the second century BCE until the mid-15th century.\",\n",
      "            \"sim\": 0.586681067943573,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 40,\n",
      "    \"word\": \"Post-Punk\",\n",
      "    \"claim\": \"die Bezeichnung f\\u00fcr eine Musikbewegung, die vor allem und zuerst in Gro\\u00dfbritannien Ende der 1970er/Anfang der 1980er Jahre aus dem Punk hervorging\",\n",
      "    \"connected_claim\": \"Post-punk represents the name for a music movement that emerged from the punk mainly and first in Great Britain at the end of the 1970s/early 1980s.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Post-punk represents the name for a music movement that emerged from the punk mainly and first in Great Britain at the end of the 1970s/early 1980s.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Post-punk (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Post-punk (originally called new musick) is a broad genre of music that emerged in 1977 in the wake of punk rock.\",\n",
      "            \"sim\": 0.7906240820884705,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Post-punk (wikipedia)\",\n",
      "            \"line_idx\": 6,\n",
      "            \"text\": \"By the mid-1980s, post-punk had dissipated, but it provided a foundation for the new pop movement and the later alternative and independent genres.\",\n",
      "            \"sim\": 0.7582657337188721,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Post-punk (wikipedia)\",\n",
      "            \"line_idx\": 5,\n",
      "            \"text\": \"The movement was closely related to the development of ancillary genres such as gothic rock, neo-psychedelia, no wave, and industrial music.\",\n",
      "            \"sim\": 0.7295201420783997,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 35,\n",
      "    \"word\": \"Herzinfarkt\",\n",
      "    \"claim\": \"eine anhaltende Durchblutungsst\\u00f6rung (''Isch\\u00e4mie'') von Teilen des Herzmuskels\",\n",
      "    \"connected_claim\": \"Myocardial infarction signifies a persistent circulation disorder (''Ischemia'') of parts of the heart muscle.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Myocardial infarction signifies a persistent circulation disorder (''Ischemia'') of parts of the heart muscle.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Myocardial infarction (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"A myocardial infarction (MI), commonly known as a heart attack, occurs when blood flow decreases or stops in one of the coronary arteries of the heart, causing infarction (tissue death) to the heart muscle.\",\n",
      "            \"sim\": 0.7188685536384583,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"myocardial infarction (wiktionary)\",\n",
      "            \"line_idx\": 3,\n",
      "            \"text\": \"(cardiology, pathology) Necrosis of heart muscle caused by an interruption to the supply of blood to the heart, often as a result of coronary thrombosis.\",\n",
      "            \"sim\": 0.7184348106384277,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"myocardial infarction (wiktionary)\",\n",
      "            \"line_idx\": 5,\n",
      "            \"text\": \"myocardial infarction on  Wikipedia.\",\n",
      "            \"sim\": 0.7127590179443359,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 86,\n",
      "    \"word\": \"Vakuum\",\n",
      "    \"claim\": \"im engeren Wortsinn das Eingreifen staatlicher oder sonstiger machtgest\\u00fctzter Institutionen, durch die im Sinne dieser Institutionen unerw\\u00fcnschte Inhalte von der Medien\\u00f6ffentlichkeit ferngehalten werden sollen\",\n",
      "    \"connected_claim\": \"vacuum: in the narrow sense of the word, the intervention of state or other power-based institutions, through which unwanted content is to be kept away from the media public in the sense of these institutions\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"vacuum: in the narrow sense of the word, the intervention of state or other power-based institutions, through which unwanted content is to be kept away from the media public in the sense of these institutions\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Vacuum (disambiguation) (wikipedia)\",\n",
      "            \"line_idx\": 4,\n",
      "            \"text\": \"Vacuum (outer space), the very high, but imperfect, vacuum of the solar system and interstellar space.\",\n",
      "            \"sim\": 0.6338034272193909,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Vacuum (disambiguation) (wikipedia)\",\n",
      "            \"line_idx\": 6,\n",
      "            \"text\": \"Vacuum state, the quantum state with the lowest possible energy.\",\n",
      "            \"sim\": 0.6259678602218628,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Vacuum (disambiguation) (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Vacuum is the absence of matter.\",\n",
      "            \"sim\": 0.625314474105835,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 120,\n",
      "    \"word\": \"Grand Central Terminal\",\n",
      "    \"claim\": \"umgangssprachlich die Nutzung von verschiedenen Energien in f\\u00fcr Menschen gut verwendbaren Formen\",\n",
      "    \"connected_claim\": \"Grand Central Terminal symbolizes the colloquial use of different energies in forms that are suitable for humans.\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Grand Central Terminal symbolizes the colloquial use of different energies in forms that are suitable for humans.\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Grand Central Terminal (GCT; also referred to as Grand Central Station or simply as Grand Central) is a commuter rail terminal located at 42nd Street and Park Avenue in Midtown Manhattan, New York City.\",\n",
      "            \"sim\": 0.5338200926780701,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 8,\n",
      "            \"text\": \"The terminal's Main Concourse is often used as a meeting place, and is especially featured in films and television.\",\n",
      "            \"sim\": 0.5320994257926941,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 5,\n",
      "            \"text\": \"The distinctive architecture and interior design of Grand Central Terminal's station house have earned it several landmark designations, including as a National Historic Landmark.\",\n",
      "            \"sim\": 0.516218900680542,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 36,\n",
      "    \"word\": \"Energieverbrauch\",\n",
      "    \"claim\": \"umgangssprachlich die Nutzung von verschiedenen Energien in f\\u00fcr Menschen gut verwendbaren Formen\",\n",
      "    \"connected_claim\": \"Energy consumption represents colloquially the use of different energies in forms that are suitable for humans.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Energy consumption represents colloquially the use of different energies in forms that are suitable for humans.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Energieverbrauch (wiktionary)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"energy consumption / expenditure.\",\n",
      "            \"sim\": 0.6395981907844543,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Energy consumption (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Energy consumption is the amount of energy used.\",\n",
      "            \"sim\": 0.6286062002182007,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Energieverbrauch (wiktionary)\",\n",
      "            \"line_idx\": 1,\n",
      "            \"text\": \"Energieverbrauch m (strong, genitive Energieverbrauchs, no plural).\",\n",
      "            \"sim\": 0.5710309743881226,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 162,\n",
      "    \"word\": \"Grand Central Terminal\",\n",
      "    \"claim\": \"ein nicht real existierender Vogel, der eine Mischung aus zwei Raubv\\u00f6geln, n\\u00e4mlich einem Falken oder Habicht (englisch \\u201ehawk\\u201c) und dem H\\u00e4her (englisch \\u201ejay\\u201c) darstellt\",\n",
      "    \"connected_claim\": \"The grand central terminal symbolizes a non-real bird representing a mixture of two birds of prey, a hawk or hawk, and the haher.\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"The grand central terminal symbolizes a non-real bird representing a mixture of two birds of prey, a hawk or hawk, and the haher.\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Grand Central Terminal (GCT; also referred to as Grand Central Station or simply as Grand Central) is a commuter rail terminal located at 42nd Street and Park Avenue in Midtown Manhattan, New York City.\",\n",
      "            \"sim\": 0.5347939729690552,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 1,\n",
      "            \"text\": \"Grand Central is the southern terminus of the Metro-North Railroad's Harlem, Hudson and New Haven Lines, serving the northern parts of the New York metropolitan area.\",\n",
      "            \"sim\": 0.5127512812614441,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 5,\n",
      "            \"text\": \"The distinctive architecture and interior design of Grand Central Terminal's station house have earned it several landmark designations, including as a National Historic Landmark.\",\n",
      "            \"sim\": 0.511115550994873,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 45,\n",
      "    \"word\": \"Glas\\u00fcbergang\",\n",
      "    \"claim\": \"eine sprunghafte \\u00c4nderung des W\\u00e4rmeausdehnungskoeffizienten sowie eine Abnahme der spezifischen W\\u00e4rme\",\n",
      "    \"connected_claim\": \"The glass transition signifies a sudden change in the coefficient of thermal expansion and a decrease in the specific heat.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"The glass transition signifies a sudden change in the coefficient of thermal expansion and a decrease in the specific heat.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Glass transition (wikipedia)\",\n",
      "            \"line_idx\": 10,\n",
      "            \"text\": \"Upon cooling or heating through this glass-transition range, the material also exhibits a smooth step in the thermal-expansion coefficient and in the specific heat, with the location of these effects again being dependent on the history of the material.\",\n",
      "            \"sim\": 0.671835720539093,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Glass transition (wikipedia)\",\n",
      "            \"line_idx\": 8,\n",
      "            \"text\": \"Despite the change in the physical properties of a material through its glass transition, the transition is not considered a phase transition; rather it is a phenomenon extending over a range of temperature and defined by one of several conventions.\",\n",
      "            \"sim\": 0.6502382159233093,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Glass transition (wikipedia)\",\n",
      "            \"line_idx\": 3,\n",
      "            \"text\": \"The glass-transition temperature Tg of a material characterizes the range of temperatures over which this glass transition occurs (as an experimental definition, typically marked as 100 s of relaxation time).\",\n",
      "            \"sim\": 0.6476070284843445,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 14,\n",
      "    \"word\": \"TU\",\n",
      "    \"claim\": \"eine promotionsberechtigte Hochschule mit einem breiten Angebot an Ingenieurf\\u00e4chern und naturwissenschaftlichen F\\u00e4chern, zumeist erg\\u00e4nzt um weitere F\\u00e4cher\",\n",
      "    \"connected_claim\": \"Tu represents a doctoral university with a wide range of engineering and scientific subjects, mostly supplemented by other subjects.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Tu represents a doctoral university with a wide range of engineering and scientific subjects, mostly supplemented by other subjects.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"tu (wiktionary)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"tu.\",\n",
      "            \"sim\": 0.5237563848495483,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"tu (wiktionary)\",\n",
      "            \"line_idx\": 64,\n",
      "            \"text\": \"Hard mutation of du. Mixed mutation of du.\",\n",
      "            \"sim\": 0.5122022032737732,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Tu (cuneiform) (wikipedia)\",\n",
      "            \"line_idx\": 5,\n",
      "            \"text\": \"The Hittite language version of tu, (and ideogram TU) is identical in common form to the Sumerian.\",\n",
      "            \"sim\": 0.499368816614151,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 150,\n",
      "    \"word\": \"Urum\",\n",
      "    \"claim\": \"\\u00fcberwiegend aus Teig und einer w\\u00fcrzigen F\\u00fcllung bestehende Gericht\",\n",
      "    \"connected_claim\": \"Urums mainly represents a dish made of dough and a spicy filling.\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Urums mainly represents a dish made of dough and a spicy filling.\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Urum (wiktionary)\",\n",
      "            \"line_idx\": 3,\n",
      "            \"text\": \"Urum.\",\n",
      "            \"sim\": 0.5646771192550659,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Urum (wiktionary)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"IPA(key): /u\\u02d0\\u02c8\\u0279u\\u02d0m/.\",\n",
      "            \"sim\": 0.5489026308059692,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Urum (wiktionary)\",\n",
      "            \"line_idx\": 7,\n",
      "            \"text\": \"639-3 code uum (SIL).Ethnologue entry for Urum, uum \\u2060.\\u0410\\u043b\\u0435\\u043a\\u0441\\u0435\\u0439 \\u041a\\u0430\\u0437\\u0430\\u043a\\u043e\\u0432\",\n",
      "            \"sim\": 0.5482784509658813,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 70,\n",
      "    \"word\": \"Internet-Konnektivit\\u00e4t\",\n",
      "    \"claim\": \" Transfer von IP-Paketen in und aus dem Internet\",\n",
      "    \"connected_claim\": \"Internet connectivity signifies the transfer of IP packages to and from the Internet.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Internet connectivity signifies the transfer of IP packages to and from the Internet.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Internet access (wikipedia)\",\n",
      "            \"line_idx\": 1,\n",
      "            \"text\": \"Internet access is offered for sale by an international hierarchy of Internet service providers (ISPs) using various networking technologies.\",\n",
      "            \"sim\": 0.618850827217102,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Internet access (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Internet access is a facility or service that provides connectivity for a computer, a computer network, or other network device to the Internet, and for individuals or organizations to access or use applications such as email and the World Wide Web.\",\n",
      "            \"sim\": 0.5986930131912231,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Internet access (wikipedia)\",\n",
      "            \"line_idx\": 7,\n",
      "            \"text\": \"Types of connections range from fixed cable home (such as DSL and fiber optic) to mobile (via cellular) and satellite.\",\n",
      "            \"sim\": 0.5529840588569641,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 77,\n",
      "    \"word\": \"Montgolfiere\",\n",
      "    \"claim\": \"ein Hei\\u00dfluftballon aus Leinwand, der mit einer d\\u00fcnnen Papierschicht luftdicht verkleidet war\",\n",
      "    \"connected_claim\": \"A montgolfiere represents a hot air balloon made of canvas covered with a thin layer of paper.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"A montgolfiere represents a hot air balloon made of canvas covered with a thin layer of paper.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Montgolfier brothers (wikipedia)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"They invented the Montgolfi\\u00e8re-style hot air balloon, globe a\\u00e9rostatique, which launched the first confirmed piloted ascent by humans in 1783, carrying Jacques-\\u00c9tienne.\",\n",
      "            \"sim\": 0.5598392486572266,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Montgolfier brothers (wikipedia)\",\n",
      "            \"line_idx\": 1,\n",
      "            \"text\": \"[\\u0292ak etj\\u025bn m\\u0254\\u0303\\u0261\\u0254lfje]; 6 January 1745 \\u2013 2 August 1799) \\u2013 were aviation pioneers, balloonists and paper manufacturers from the commune Annonay in Ard\\u00e8che, France.\",\n",
      "            \"sim\": 0.5331383943557739,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Montgolfier brothers (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"The Montgolfier brothers \\u2013 Joseph-Michel Montgolfier (French pronunciation: [\\u0292oz\\u025bf mi\\u0283\\u025bl m\\u0254\\u0303\\u0261\\u0254lfje]; 26 August 1740 \\u2013 26 June 1810) and Jacques-\\u00c9tienne Montgolfier (French pronunciation:\",\n",
      "            \"sim\": 0.5220468640327454,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 86,\n",
      "    \"word\": \"Vakuum\",\n",
      "    \"claim\": \"im engeren Wortsinn das Eingreifen staatlicher oder sonstiger machtgest\\u00fctzter Institutionen, durch die im Sinne dieser Institutionen unerw\\u00fcnschte Inhalte von der Medien\\u00f6ffentlichkeit ferngehalten werden sollen\",\n",
      "    \"connected_claim\": \"vacuum: in the narrow sense of the word, the intervention of state or other power-based institutions, through which unwanted content is to be kept away from the media public in the sense of these institutions\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"vacuum: in the narrow sense of the word, the intervention of state or other power-based institutions, through which unwanted content is to be kept away from the media public in the sense of these institutions\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Vacuum (disambiguation) (wikipedia)\",\n",
      "            \"line_idx\": 4,\n",
      "            \"text\": \"Vacuum (outer space), the very high, but imperfect, vacuum of the solar system and interstellar space.\",\n",
      "            \"sim\": 0.6338034272193909,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Vacuum (disambiguation) (wikipedia)\",\n",
      "            \"line_idx\": 6,\n",
      "            \"text\": \"Vacuum state, the quantum state with the lowest possible energy.\",\n",
      "            \"sim\": 0.6259678602218628,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Vacuum (disambiguation) (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Vacuum is the absence of matter.\",\n",
      "            \"sim\": 0.625314474105835,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 76,\n",
      "    \"word\": \"VidCon\",\n",
      "    \"claim\": \"die gr\\u00f6\\u00dfte Video-Convention der Welt\",\n",
      "    \"connected_claim\": \"Vidcon represents the largest video convention in the world.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Vidcon represents the largest video convention in the world.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"VidCon (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"VidCon is an annual convention for influencers, fans, executives, and online brands.\",\n",
      "            \"sim\": 0.6679027676582336,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"VidCon (wikipedia)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"It was founded by veteran YouTube creators John and Hank Green (Vlogbrothers), and was later acquired by Viacom (now Paramount) in 2018.\",\n",
      "            \"sim\": 0.6391170024871826,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"VidCon (wikipedia)\",\n",
      "            \"line_idx\": 1,\n",
      "            \"text\": \"The event primarily features prominent video stars from across the internet.\",\n",
      "            \"sim\": 0.6354268193244934,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 8,\n",
      "    \"word\": \"Russistik\",\n",
      "    \"claim\": \"Die Wissenschaft, die sich mit der russischen Sprache und der umfangreichen russischen Literatur besch\\u00e4ftigt\",\n",
      "    \"connected_claim\": \"Russian studies represents the science that deals with the Russian language and extensive Russian literature.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Russian studies represents the science that deals with the Russian language and extensive Russian literature.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Russian studies (wikipedia)\",\n",
      "            \"line_idx\": 1,\n",
      "            \"text\": \"Russian studies should not be confused with the study of the Russian literature or linguistics, which is often a distinct department within universities.\",\n",
      "            \"sim\": 0.7578105330467224,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Russian studies (wikipedia)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"In university, a Russian studies major includes many cultural classes teaching Russian politics, history, geography, linguistics, Russian language, literature, and arts.\",\n",
      "            \"sim\": 0.7526252865791321,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Russian studies (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Russian studies is an interdisciplinary field crossing politics, history, culture, economics, and languages of Russia and its neighborhood, often grouped under Soviet and Communist studies.\",\n",
      "            \"sim\": 0.749969482421875,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 57,\n",
      "    \"word\": \"Flechte\",\n",
      "    \"claim\": \"eine symbiotische Lebensgemeinschaft zwischen einem Pilz und Gr\\u00fcnalgen oder Cyanobakterien\",\n",
      "    \"connected_claim\": \"Lichen represents a symbiotic community between a fungus and green algae or cyanobacteria.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Lichen represents a symbiotic community between a fungus and green algae or cyanobacteria.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Lichen (disambiguation) (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Lichen is a type of symbiotic organism.\",\n",
      "            \"sim\": 0.6837023496627808,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Lichen (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"A lichen ( LY-k\\u0259n, UK also  LITCH-\\u0259n) is a symbiosis of algae or cyanobacteria living among filaments of multiple fungi species, along with a yeast embedded in the cortex or \\\"skin\\\", in a mutualistic relationship.\",\n",
      "            \"sim\": 0.6818994283676147,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Lichen (wikipedia)\",\n",
      "            \"line_idx\": 18,\n",
      "            \"text\": \"They can be seen as being relatively self-contained miniature ecosystems, where the fungi, algae, or cyanobacteria have the potential to engage with other microorganisms in a functioning system that may evolve as an even more complex composite organism.\",\n",
      "            \"sim\": 0.6353198885917664,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 125,\n",
      "    \"word\": \"YouTube-Kanal\",\n",
      "    \"claim\": \"Bewegungsleistung in elektrische Leistung umwandelt\",\n",
      "    \"connected_claim\": \"A YouTube channel signifies motion power converted into electrical power.\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"A YouTube channel signifies motion power converted into electrical power.\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Vlog (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"A vlog (), also known as a video blog or video log, is a form of blog for which the medium is video.\",\n",
      "            \"sim\": 0.4910823106765747,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Vlog (wikipedia)\",\n",
      "            \"line_idx\": 7,\n",
      "            \"text\": \"The vlog category is popular on the video-sharing platform YouTube.\",\n",
      "            \"sim\": 0.49019020795822144,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"YouTube (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"YouTube is an American online video sharing platform owned by Google.\",\n",
      "            \"sim\": 0.4847424030303955,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 72,\n",
      "    \"word\": \"Macintosh\",\n",
      "    \"claim\": \"der erste Mikrocomputer mit grafischer Benutzeroberfl\\u00e4che, der in gr\\u00f6\\u00dferen St\\u00fcckzahlen produziert wurde\",\n",
      "    \"connected_claim\": \"The Mac represents the first microcomputer with a graphical user interface produced in larger quantities.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"The Mac represents the first microcomputer with a graphical user interface produced in larger quantities.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Mac (computer) (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Mac, short for Macintosh (its official name until 1999), is a family of personal computers designed and marketed by Apple.\",\n",
      "            \"sim\": 0.6573758721351624,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Mac (computer) (wikipedia)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"The product lineup includes the MacBook Air and MacBook Pro laptops, and the iMac, Mac Mini, Mac Studio, and Mac Pro desktops.\",\n",
      "            \"sim\": 0.6565395593643188,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Mac (computer) (wikipedia)\",\n",
      "            \"line_idx\": 9,\n",
      "            \"text\": \"In 1987, the Macintosh II brought color graphics, but priced as a professional workstation and not a personal computer.\",\n",
      "            \"sim\": 0.6526975631713867,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 26,\n",
      "    \"word\": \"Biodiversit\\u00e4ts-Hotspot\",\n",
      "    \"claim\": \"F\\u00fcr ein geografisches Gebiet, in dem die Biodiversit\\u00e4t besonders gro\\u00df ist \\u2013 und das gleichzeitig besonders bedroht ist\",\n",
      "    \"connected_claim\": \"A biodiversity hotspot signifies a geographical area where biodiversity is particularly large \\u2013 and which is at the same time particularly threatened.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"A biodiversity hotspot signifies a geographical area where biodiversity is particularly large \\u2013 and which is at the same time particularly threatened.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"biodiversity hotspot (wiktionary)\",\n",
      "            \"line_idx\": 7,\n",
      "            \"text\": \"A place with a significant level of biodiversity, particularly if the flora and fauna are threatened with loss of their habitat.\",\n",
      "            \"sim\": 0.7846282124519348,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"biodiversity hotspot (wiktionary)\",\n",
      "            \"line_idx\": 9,\n",
      "            \"text\": \"In the writings of Norman Myers and his collaborators, a biodiversity hotspot is defined as a region that contains at least 1,500 endemic species of vascular plants (more than 0.5% of the world\\u2019s total), and which has lost at least 70% of its primary vegetation.\",\n",
      "            \"sim\": 0.749369740486145,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"biodiversity hotspot (wiktionary)\",\n",
      "            \"line_idx\": 5,\n",
      "            \"text\": \"biodiversity hotspot (plural biodiversity hotspots).\",\n",
      "            \"sim\": 0.717515230178833,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 72,\n",
      "    \"word\": \"Macintosh\",\n",
      "    \"claim\": \"der erste Mikrocomputer mit grafischer Benutzeroberfl\\u00e4che, der in gr\\u00f6\\u00dferen St\\u00fcckzahlen produziert wurde\",\n",
      "    \"connected_claim\": \"The Mac represents the first microcomputer with a graphical user interface produced in larger quantities.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"The Mac represents the first microcomputer with a graphical user interface produced in larger quantities.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Mac (computer) (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Mac, short for Macintosh (its official name until 1999), is a family of personal computers designed and marketed by Apple.\",\n",
      "            \"sim\": 0.6573758721351624,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Mac (computer) (wikipedia)\",\n",
      "            \"line_idx\": 2,\n",
      "            \"text\": \"The product lineup includes the MacBook Air and MacBook Pro laptops, and the iMac, Mac Mini, Mac Studio, and Mac Pro desktops.\",\n",
      "            \"sim\": 0.6565395593643188,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Mac (computer) (wikipedia)\",\n",
      "            \"line_idx\": 9,\n",
      "            \"text\": \"In 1987, the Macintosh II brought color graphics, but priced as a professional workstation and not a personal computer.\",\n",
      "            \"sim\": 0.6526975631713867,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 44,\n",
      "    \"word\": \"Sepsis\",\n",
      "    \"claim\": \"Eine schwere generalisierte Infektion, also die Ausbreitung von einem lokalen Infektionsort \\u00fcber die Blutbahn im gesamten K\\u00f6rper\",\n",
      "    \"connected_claim\": \"Sepsis signifies a severe generalized infection, i.e. the spread of a local infection site through the bloodstream throughout the body.\",\n",
      "    \"label\": \"SUPPORTED\",\n",
      "    \"predicted\": \"NOT_SUPPORTED\",\n",
      "    \"factuality\": 0.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Sepsis signifies a severe generalized infection, i.e. the spread of a local infection site through the bloodstream throughout the body.\",\n",
      "            \"predicted\": \"NOT_SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Sepsis (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Sepsis is a potentially life-threatening condition that arises when the body's response to infection causes injury to its own tissues and organs.\",\n",
      "            \"sim\": 0.681774914264679,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Sepsis (wikipedia)\",\n",
      "            \"line_idx\": 1,\n",
      "            \"text\": \"This initial stage of sepsis is followed by suppression of the immune system.\",\n",
      "            \"sim\": 0.6739721298217773,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Sepsis (wikipedia)\",\n",
      "            \"line_idx\": 3,\n",
      "            \"text\": \"There may also be symptoms related to a specific infection, such as a cough with pneumonia, or painful urination with a kidney infection.\",\n",
      "            \"sim\": 0.6713650226593018,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n",
      "{\n",
      "    \"id\": 120,\n",
      "    \"word\": \"Grand Central Terminal\",\n",
      "    \"claim\": \"umgangssprachlich die Nutzung von verschiedenen Energien in f\\u00fcr Menschen gut verwendbaren Formen\",\n",
      "    \"connected_claim\": \"Grand Central Terminal symbolizes the colloquial use of different energies in forms that are suitable for humans.\",\n",
      "    \"label\": \"NOT_SUPPORTED\",\n",
      "    \"predicted\": \"SUPPORTED\",\n",
      "    \"factuality\": 1.0,\n",
      "    \"atoms\": [\n",
      "        {\n",
      "            \"atom\": \"Grand Central Terminal symbolizes the colloquial use of different energies in forms that are suitable for humans.\",\n",
      "            \"predicted\": \"SUPPORTED\"\n",
      "        }\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 0,\n",
      "            \"text\": \"Grand Central Terminal (GCT; also referred to as Grand Central Station or simply as Grand Central) is a commuter rail terminal located at 42nd Street and Park Avenue in Midtown Manhattan, New York City.\",\n",
      "            \"sim\": 0.5338200926780701,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 8,\n",
      "            \"text\": \"The terminal's Main Concourse is often used as a meeting place, and is especially featured in films and television.\",\n",
      "            \"sim\": 0.5320994257926941,\n",
      "            \"in_intro\": true\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Grand Central Terminal (wikipedia)\",\n",
      "            \"line_idx\": 5,\n",
      "            \"text\": \"The distinctive architecture and interior design of Grand Central Terminal's station house have earned it several landmark designations, including as a National Historic Landmark.\",\n",
      "            \"sim\": 0.516218900680542,\n",
      "            \"in_intro\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "############################\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. BM25 Selection (Whole Page vs. Intro Page) (Best Setup)\n",
    "\n",
    "In our document selection process, we first retrieve all pages related to a specific word. Given the potentially large number of pages and the associated computation time, we use the BM25 algorithm to select the top 3 most relevant pages for the claim.\n",
    "\n",
    "In this section, we evaluate whether using only the intro sections of these pages is sufficient for effective BM25 selection, which also reduces computation time. We will compare the results of selecting from intro sections versus the full pages to determine if the selections are consistent and if the top-ranked pages are the same in both scenarios."
   ],
   "id": "124c91e34be865fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T13:50:23.288849Z",
     "start_time": "2024-07-30T13:33:41.688234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    print(f\"Evaluating {dataset_name}...\")\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "\n",
    "    equal_evids = 0\n",
    "    first_evid_match = 0\n",
    "    first_intro_match = 0\n",
    "    first_full_match = 0\n",
    "    for entry in tqdm(dataset):\n",
    "        if entry['in_wiki'] == 'No':\n",
    "            continue\n",
    "\n",
    "        _, intro_evids = offline_evid_fetcher.fetch_evidences(\n",
    "            search_word=entry['document_search_word'], only_intro=True)\n",
    "        _, full_evids = offline_evid_fetcher.fetch_evidences(\n",
    "            search_word=entry['document_search_word'], only_intro=False)\n",
    "\n",
    "        intro_evids_indices = rank_docs(entry['connected_claim'],\n",
    "                                        [\" \".join(evidence.get('lines')) for evidence in\n",
    "                                         intro_evids], k=3)\n",
    "        selected_intro_pages = [intro_evids[idx].get('title') for idx in intro_evids_indices]\n",
    "        full_evids_indices = rank_docs(entry['connected_claim'],\n",
    "                                       [\" \".join(evidence.get('lines')) for evidence in full_evids],\n",
    "                                       k=3)\n",
    "        selected_full_pages = [full_evids[idx].get('title') for idx in full_evids_indices]\n",
    "\n",
    "        if set(selected_intro_pages) == set(selected_full_pages):\n",
    "            equal_evids += 1\n",
    "        if selected_intro_pages[0] == selected_full_pages[0]:\n",
    "            first_evid_match += 1\n",
    "        if selected_intro_pages[0] in selected_full_pages:\n",
    "            first_intro_match += 1    \n",
    "        if selected_full_pages[0] in selected_intro_pages:\n",
    "            first_full_match += 1   \n",
    "\n",
    "    print(f\"All Evidence Match: {round(100 * equal_evids / len(dataset), 2)}%\")\n",
    "    print(f\"First Evidence Match: {round(100 * first_evid_match / len(dataset), 2)}%\")\n",
    "    print(f\"First Intro in Full Pages: {round(100 * first_intro_match / len(dataset), 2)}%\")\n",
    "    print(f\"First Full Page in Intro: {round(100 * first_full_match / len(dataset), 2)}%\")"
   ],
   "id": "11e7312453230996",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating german_dpr-claim_verification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [06:06<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Evidence Match: 59.52\n",
      "First Evidence Match: 60.71\n",
      "Evaluating german-claim_verification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 73/710 [10:33<1:32:04,  8.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m     12\u001B[0m _, intro_evids \u001B[38;5;241m=\u001B[39m offline_evid_fetcher\u001B[38;5;241m.\u001B[39mfetch_evidences(search_word\u001B[38;5;241m=\u001B[39mentry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument_search_word\u001B[39m\u001B[38;5;124m'\u001B[39m], only_intro\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 13\u001B[0m _, full_evids \u001B[38;5;241m=\u001B[39m \u001B[43moffline_evid_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch_evidences\u001B[49m\u001B[43m(\u001B[49m\u001B[43msearch_word\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mentry\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdocument_search_word\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m intro_evids_indices \u001B[38;5;241m=\u001B[39m rank_docs(entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconnected_claim\u001B[39m\u001B[38;5;124m'\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(evidence\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlines\u001B[39m\u001B[38;5;124m'\u001B[39m)) \u001B[38;5;28;01mfor\u001B[39;00m evidence \u001B[38;5;129;01min\u001B[39;00m intro_evids], k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m     16\u001B[0m selected_intro_pages \u001B[38;5;241m=\u001B[39m [intro_evids[idx]\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m intro_evids_indices]\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/pipeline_module/evidence_fetcher.py:91\u001B[0m, in \u001B[0;36mWikipediaEvidenceFetcher.fetch_evidences\u001B[0;34m(self, word, translated_word, search_word, only_intro, word_lang)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch_evidences\u001B[39m(\u001B[38;5;28mself\u001B[39m, word: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, translated_word: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, search_word: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, only_intro: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     79\u001B[0m                     word_lang: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mde\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28mstr\u001B[39m, List[Tuple[\u001B[38;5;28mstr\u001B[39m, List[\u001B[38;5;28mstr\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]]]:\n\u001B[1;32m     81\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;124;03m    Fetch evidences for a single word using the batch method.\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;124;03m    :return: Tuple containing the word and its evidence.\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 91\u001B[0m     evid_words, evids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch_evidences_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mword\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtranslated_word\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtranslated_word\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msearch_word\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msearch_word\u001B[49m\u001B[43m}\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_intro\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_lang\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mword_lang\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m evid_words[\u001B[38;5;241m0\u001B[39m], evids[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/pipeline_module/evidence_fetcher.py:114\u001B[0m, in \u001B[0;36mWikipediaEvidenceFetcher.fetch_evidences_batch\u001B[0;34m(self, batch, only_intro, word_lang, offline)\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m entry \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m batch), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m missing in batch entries\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# Fetch evidences for each entry in the batch\u001B[39;00m\n\u001B[0;32m--> 114\u001B[0m evidence_batch \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    115\u001B[0m     {\n\u001B[1;32m    116\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m: wiki_word,\n\u001B[1;32m    117\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevidences\u001B[39m\u001B[38;5;124m'\u001B[39m: [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m: page, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mline_indices\u001B[39m\u001B[38;5;124m'\u001B[39m: [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(lines))], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlines\u001B[39m\u001B[38;5;124m'\u001B[39m: lines} \u001B[38;5;28;01mfor\u001B[39;00m page, lines \u001B[38;5;129;01min\u001B[39;00m texts],\n\u001B[1;32m    118\u001B[0m     }\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m batch\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m texts, wiki_word \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwiki\u001B[38;5;241m.\u001B[39mget_pages(\n\u001B[1;32m    121\u001B[0m         word\u001B[38;5;241m=\u001B[39mentry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m    122\u001B[0m         fallback_word\u001B[38;5;241m=\u001B[39mentry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtranslated_word\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m    123\u001B[0m         word_lang\u001B[38;5;241m=\u001B[39mword_lang,\n\u001B[1;32m    124\u001B[0m         only_intro\u001B[38;5;241m=\u001B[39monly_intro,\n\u001B[1;32m    125\u001B[0m         search_word\u001B[38;5;241m=\u001B[39mentry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msearch_word\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffline \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    126\u001B[0m     )]\n\u001B[1;32m    127\u001B[0m ]\n\u001B[1;32m    129\u001B[0m \u001B[38;5;66;03m# Unpack evidences and words\u001B[39;00m\n\u001B[1;32m    130\u001B[0m evid_words \u001B[38;5;241m=\u001B[39m [entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m evidence_batch]\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/pipeline_module/evidence_fetcher.py:120\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m entry \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m batch), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m missing in batch entries\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# Fetch evidences for each entry in the batch\u001B[39;00m\n\u001B[1;32m    114\u001B[0m evidence_batch \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    115\u001B[0m     {\n\u001B[1;32m    116\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m: wiki_word,\n\u001B[1;32m    117\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevidences\u001B[39m\u001B[38;5;124m'\u001B[39m: [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m: page, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mline_indices\u001B[39m\u001B[38;5;124m'\u001B[39m: [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(lines))], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlines\u001B[39m\u001B[38;5;124m'\u001B[39m: lines} \u001B[38;5;28;01mfor\u001B[39;00m page, lines \u001B[38;5;129;01min\u001B[39;00m texts],\n\u001B[1;32m    118\u001B[0m     }\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m batch\n\u001B[0;32m--> 120\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m texts, wiki_word \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwiki\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_pages\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mentry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mword\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfallback_word\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mentry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtranslated_word\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m        \u001B[49m\u001B[43mword_lang\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mword_lang\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[43m        \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_intro\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m        \u001B[49m\u001B[43msearch_word\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mentry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msearch_word\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moffline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m    127\u001B[0m ]\n\u001B[1;32m    129\u001B[0m \u001B[38;5;66;03m# Unpack evidences and words\u001B[39;00m\n\u001B[1;32m    130\u001B[0m evid_words \u001B[38;5;241m=\u001B[39m [entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m evidence_batch]\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/fetchers/wikipedia.py:216\u001B[0m, in \u001B[0;36mWikipedia.get_pages\u001B[0;34m(self, word, fallback_word, word_lang, only_intro, split_level, return_raw, search_word)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_pages\u001B[39m(\u001B[38;5;28mself\u001B[39m, word: \u001B[38;5;28mstr\u001B[39m, fallback_word: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, word_lang: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    213\u001B[0m               only_intro\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    214\u001B[0m               split_level\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m, return_raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, search_word\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffline_backend \u001B[38;5;129;01mand\u001B[39;00m search_word:\n\u001B[0;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_pages_offline\u001B[49m\u001B[43m(\u001B[49m\u001B[43msearch_word\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monly_intro\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_raw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_pages_online(word, fallback_word, word_lang, only_intro, split_level,\n\u001B[1;32m    219\u001B[0m                                      return_raw)\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/fetchers/wikipedia.py:233\u001B[0m, in \u001B[0;36mWikipedia.get_pages_offline\u001B[0;34m(self, search_word, only_intro, return_raw, split_level)\u001B[0m\n\u001B[1;32m    231\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m         text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clean_text(text)\n\u001B[0;32m--> 233\u001B[0m         texts\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_split_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtitle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_level\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(texts\u001B[38;5;241m.\u001B[39mitems()), search_word\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/fetchers/wikipedia.py:170\u001B[0m, in \u001B[0;36mWikipedia._split_text\u001B[0;34m(self, title, site, text, split_level, sentence_limit)\u001B[0m\n\u001B[1;32m    168\u001B[0m     texts \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m: passage \u001B[38;5;28;01mfor\u001B[39;00m i, passage \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(passages)}\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m split_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 170\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m \u001B[43msplit_into_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m     texts[key_base] \u001B[38;5;241m=\u001B[39m sentences[:sentence_limit]\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# split_level == 'none'\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/general_utils/spacy_utils.py:100\u001B[0m, in \u001B[0;36msplit_into_sentences\u001B[0;34m(txt, lang)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msplit_into_sentences\u001B[39m(txt: \u001B[38;5;28mstr\u001B[39m, lang: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m     99\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Split a text into sentences.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 100\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mget_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtxt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [sent\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m doc\u001B[38;5;241m.\u001B[39msents]\n",
      "File \u001B[0;32m~/PycharmProjects/evaluating_factuality_word_definitions/general_utils/spacy_utils.py:37\u001B[0m, in \u001B[0;36mget_doc\u001B[0;34m(txt, lang)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_doc\u001B[39m(txt: \u001B[38;5;28mstr\u001B[39m, lang: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m lang \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 37\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtxt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m lang \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mde\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m german_nlp(txt)\n",
      "File \u001B[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/spacy/language.py:1049\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[0;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[1;32m   1047\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[1;32m   1048\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1049\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1051\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[1;32m   1052\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
