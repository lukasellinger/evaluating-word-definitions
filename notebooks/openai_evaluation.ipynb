{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0 Preparations\n",
    "Before starting, ensure that you have cloned the repository to your Google Drive.\n",
    "We will connect to this:"
   ],
   "id": "89efdde2f316a2ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "repository = 'evaluating_factuality_word_definitions'\n",
    "\n",
    "%cd /content/drive/My Drive/{repository}"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we install the packages and import the modules needed in this notebook:",
   "id": "6231d35ad484e964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install datasets~=2.18.0\n",
    "!pip install openai~=1.35.10"
   ],
   "id": "96b44e8e7b8c3cb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:10:41.937465Z",
     "start_time": "2024-08-06T12:10:35.276839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from fetchers.openai import OpenAiFetcher\n",
    "from general_utils.reader import JSONLineReader\n",
    "from general_utils.utils import parse_model_answer, get_openai_prediction\n",
    "from config import FACT_EVULATION_OPENAI_TOKEN, PROJECT_DIR"
   ],
   "id": "fdbe8cbeb93713f4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 Setup: Define Datasets\n",
    "Now we define our models and datasets we want to evaluate:"
   ],
   "id": "f06ff4f131279207"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:10:47.125719Z",
     "start_time": "2024-08-06T12:10:41.941238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Datasets with language information\n",
    "datasets = {\n",
    "    'german_dpr-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german_dpr-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    #'german-claim_verification': {\n",
    "    #    'dataset': load_dataset('lukasellinger/german-claim_verification', split='test'),\n",
    "    #    'lang': 'de'\n",
    "    #},\n",
    "    #'squad-claim_verification': {\n",
    "    #    'dataset': load_dataset('lukasellinger/squad-claim_verification', split='test'),\n",
    "    #    'lang': 'en'\n",
    "    #}\n",
    "}"
   ],
   "id": "58d3ac7722cd6800",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:43:35.371883Z",
     "start_time": "2024-08-06T12:43:35.367140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [\n",
    "    #'gpt-3.5-turbo',\n",
    "    'gpt-4o-mini',\n",
    "    #'gpt-4o'\n",
    "]"
   ],
   "id": "ff15217b5457a60e",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:10:47.200795Z",
     "start_time": "2024-08-06T12:10:47.137173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "openai_fetcher = OpenAiFetcher(api_key=FACT_EVULATION_OPENAI_TOKEN)\n",
    "fh = JSONLineReader()"
   ],
   "id": "cc825c4d6b0397ab",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "EVALUATION_DIR = PROJECT_DIR / 'data/evaluation'",
   "id": "efb60ae2050d9965"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:31:25.357623Z",
     "start_time": "2024-08-06T12:31:25.340907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_task(idx, model, content):\n",
    "    return {\n",
    "            \"custom_id\": f\"task-{idx}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model,\n",
    "                \"temperature\": 0.1,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content\n",
    "                    }\n",
    "                ],\n",
    "                \"seed\": 42,\n",
    "                \"logprobs\": True,\n",
    "                \"top_logprobs\": 5\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def create_tasks(dataset, model, file_name, prompt_func):\n",
    "    tasks = [build_task(idx, model, prompt_func(entry)) for idx, entry in enumerate(dataset)]\n",
    "    fh.write(file_name, tasks)\n",
    "    \n",
    "    \n",
    "def create_long_prompt(entry):\n",
    "    return f'Please verify the following statement about {entry[\"word\"]}. Input: {entry[\"claim\"]} True or False?\\nOutput:'\n",
    "\n",
    "\n",
    "def create_short_prompt(entry):\n",
    "    return f'Input: {entry[\"word\"]}: {entry[\"claim\"]} True or False?\\nOutput:'\n",
    "    \n",
    "    \n",
    "def process_results(file_name, dataset, translations: bool):\n",
    "    results = fh.read(file_name)\n",
    "    outputs, all_gt_labels, all_pr_labels, wiki_gt_labels, wiki_pr_labels = [], [], [], [], []\n",
    "\n",
    "    for res in results:\n",
    "        task_id = res['custom_id']\n",
    "        index = int(task_id.split('-')[-1])\n",
    "        entry = dataset[index]\n",
    "        claim = entry['claim']\n",
    "        word = entry['word']\n",
    "        predicted = get_openai_prediction(res['response']['body'])\n",
    "        if predicted == 'UNKOWN':\n",
    "            txt_answer = res['response']['body']['choices'][0]['message']['content']\n",
    "            predicted = parse_model_answer(txt_answer)\n",
    "        \n",
    "        output = {\n",
    "            'id': entry['id'],\n",
    "            'word': word,\n",
    "            'claim': claim,\n",
    "            'label': entry['label'],\n",
    "            'predicted': predicted,\n",
    "            'in_wiki': entry['in_wiki']\n",
    "        }\n",
    "        \n",
    "        if translations:\n",
    "            output['translated_word'] = entry['english_word']\n",
    "            output['translated_claim'] = entry['english_claim']\n",
    "        \n",
    "        outputs.append(output)\n",
    "        gt_label = 1 if output['label'] == 'SUPPORTED' else 0\n",
    "        pr_label = 1 if output['predicted'] == 'SUPPORTED' else 0\n",
    "        all_gt_labels.append(gt_label)\n",
    "        all_pr_labels.append(pr_label)\n",
    "        if output['in_wiki'] == 'Yes':\n",
    "            wiki_gt_labels.append(gt_label)\n",
    "            wiki_pr_labels.append(pr_label)\n",
    "        \n",
    "    report = classification_report(all_gt_labels, all_pr_labels, zero_division=0, digits=4)\n",
    "    wiki_report = classification_report(wiki_gt_labels, wiki_pr_labels, zero_division=0, digits=4)\n",
    "    print('Report all entries:')\n",
    "    print(report)\n",
    "    print('Filtered Report for entries with evidence in wikipedia:')\n",
    "    print(wiki_report)\n",
    "    return outputs"
   ],
   "id": "64c17942c1b6ff67",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 Manual Batch Fetching",
   "id": "14a65477f8c45bf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:44:34.792188Z",
     "start_time": "2024-08-06T12:44:30.646804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_file_name = ''\n",
    "batch_job = openai_fetcher.create_batch_job(input_file_name)"
   ],
   "id": "751caf640f37d0df",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:54:43.665174Z",
     "start_time": "2024-08-06T13:54:42.952526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_job = openai_fetcher.get_batch_update(batch_job)\n",
    "print(batch_job)"
   ],
   "id": "8d1713d07602d940",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_UU1sMQdpHdo2n0zriICWf6zH', completion_window='24h', created_at=1722948274, endpoint='/v1/chat/completions', input_file_id='file-hT6LKEVDf1hJiChFgeE5yQna', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1722951636, error_file_id=None, errors=None, expired_at=None, expires_at=1723034674, failed_at=None, finalizing_at=1722951623, in_progress_at=1722948275, metadata=None, output_file_id='file-AZ4mVWMglAxBGakwWJvowL4h', request_counts=BatchRequestCounts(completed=168, failed=0, total=168))\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:54:51.676197Z",
     "start_time": "2024-08-06T13:54:49.700094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file_name = input_file_name.replace('input', 'raw_output')\n",
    "openai_fetcher.get_batch_result(output_file_name, batch_job)"
   ],
   "id": "c72b099bc661f41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raw_output_zero_shot-german_dpr-claim_verification-gpt-4o-mini_2.jsonl'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 Zero Shot",
   "id": "5a152c88dce51794"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:44:04.655629Z",
     "start_time": "2024-08-06T12:44:04.651815Z"
    }
   },
   "cell_type": "code",
   "source": "file_base_name = str(EVALUATION_DIR / '{dataset}/{type}_zero_shot-{dataset}-{model}.jsonl')",
   "id": "581980b95c784bd5",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:44:08.627611Z",
     "start_time": "2024-08-06T12:44:08.581861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        create_tasks(\n",
    "            dataset,\n",
    "            model,\n",
    "            file_base_name.format(type='input', dataset=dataset_name, model=model),\n",
    "            create_long_prompt\n",
    "        )"
   ],
   "id": "27e5f48b5824d684",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.22it/s]\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once input files are created, head to 2, to manually fetch your outputs",
   "id": "e127b7d91fc33df8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:55:06.828096Z",
     "start_time": "2024-08-06T13:55:06.469318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        print(f\"Evaluating {dataset_name} with {model}...\")\n",
    "        outputs = process_results(file_base_name.format(type='raw_output', dataset=dataset_name, model=model), dataset, translations=False)\n",
    "        fh.write(file_base_name.format(type='output', dataset=dataset_name, model=model), outputs)"
   ],
   "id": "a50d27f53a05d16a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating german_dpr-claim_verification with gpt-4o-mini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9508    0.6905    0.8000        84\n",
      "           1     0.7570    0.9643    0.8482        84\n",
      "\n",
      "    accuracy                         0.8274       168\n",
      "   macro avg     0.8539    0.8274    0.8241       168\n",
      "weighted avg     0.8539    0.8274    0.8241       168\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9375    0.6522    0.7692        69\n",
      "           1     0.7363    0.9571    0.8323        70\n",
      "\n",
      "    accuracy                         0.8058       139\n",
      "   macro avg     0.8369    0.8047    0.8008       139\n",
      "weighted avg     0.8362    0.8058    0.8010       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 Ablation Translated Claims",
   "id": "b0f8f3670917df8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:20:56.298033Z",
     "start_time": "2024-08-06T12:20:56.248600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_base_name = str(EVALUATION_DIR / '{dataset}/{type}_ablation_translated_claims-{dataset}-{model}.jsonl')\n",
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    if config['lang'] == 'en':\n",
    "        continue\n",
    "\n",
    "    dataset = config['dataset']    \n",
    "    for model in models:\n",
    "        create_tasks(\n",
    "            dataset,\n",
    "            model,\n",
    "            file_base_name.format(type='input', dataset=dataset_name, model=model),\n",
    "            create_long_prompt\n",
    "        )"
   ],
   "id": "c80afac9e66af1d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.63it/s]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once input files are created, head to 2, to manually fetch your outputs",
   "id": "8d2c3ea8259a6a50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T12:23:29.751004Z",
     "start_time": "2024-08-06T12:23:29.669302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        print(f\"Evaluating {dataset_name} with {model}...\")\n",
    "        outputs = process_results(file_base_name.format(type='raw_output', dataset=dataset_name, model=model), dataset, translations=False)\n",
    "        fh.write(file_base_name.format(type='output', dataset=dataset_name, model=model), outputs)"
   ],
   "id": "54336d77347614db",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating german_dpr-claim_verification with gpt-3.5-turbo...\n",
      "Report all entries:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9394    0.3690    0.5299        84\n",
      "           1     0.6074    0.9762    0.7489        84\n",
      "\n",
      "    accuracy                         0.6726       168\n",
      "   macro avg     0.7734    0.6726    0.6394       168\n",
      "weighted avg     0.7734    0.6726    0.6394       168\n",
      "\n",
      "Filtered Report for entries with evidence in wikipedia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9231    0.3478    0.5053        69\n",
      "           1     0.6018    0.9714    0.7432        70\n",
      "\n",
      "    accuracy                         0.6619       139\n",
      "   macro avg     0.7624    0.6596    0.6242       139\n",
      "weighted avg     0.7613    0.6619    0.6251       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5 Zero Shot Single Facts",
   "id": "e665b558ede09f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "claim_split_types = [\n",
    "    'DisSim_facts',\n",
    "    'T5SplitRephrase_facts',\n",
    "    'Factscore_facts'\n",
    "]"
   ],
   "id": "36f7a32c6652cd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_base_name = str(EVALUATION_DIR / '{model}/{type}_zero_shot_{split_type}-{dataset}-{model}.jsonl')\n",
    "\n",
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        for split_type in claim_split_types:\n",
    "            tasks = []\n",
    "            for idx, entry in tqdm(enumerate(dataset)):\n",
    "                word = entry['word']\n",
    "                atomic_facts = entry[split_type].split('--;--')\n",
    "    \n",
    "                for pidx, atomic_fact in enumerate(atomic_facts):\n",
    "                    tasks.append(build_task(f'{idx}-{pidx}', model, f'Input: {word}: {atomic_fact} True or False?\\nOutput:'))\n",
    "            fh.write(file_base_name.format(type='input', dataset=dataset_name, model=model, split_type=split_type), tasks)"
   ],
   "id": "494c10852271c29d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once input files are created, head to 2, to manually fetch your outputs",
   "id": "2f903b7824971f9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(dataset_name, dataset, model, split_type):\n",
    "    print(f\"Evaluating {dataset_name} with {model} - {split_type}...\")\n",
    "    results = fh.read(file_base_name.format(type='output', dataset=dataset_name, model=model, split_type=split_type))\n",
    "    \n",
    "    data_dict = initialize_data_dict(dataset)\n",
    "    process_results(results, dataset, data_dict, split_type)\n",
    "    gt_labels, pr_labels = generate_labels(data_dict)\n",
    "    \n",
    "    fh.write(file_base_name.format(type='output', dataset=dataset_name, model=model, split_type=split_type), data_dict.values())\n",
    "    print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "\n",
    "def initialize_data_dict(dataset):\n",
    "    data_dict = {}\n",
    "    for entry in dataset:\n",
    "        data_dict[entry['id']] = {\n",
    "            'id': entry['id'],\n",
    "            'word': entry['word'],\n",
    "            'claim': entry['claim'],\n",
    "            'label': entry['label'],\n",
    "            'predicted': -1,\n",
    "            'atoms': [],\n",
    "            'in_wiki': entry['in_wiki']\n",
    "        }\n",
    "    return data_dict\n",
    "\n",
    "def process_results(results, dataset, data_dict, split_type):\n",
    "    for res in results:\n",
    "        task_id = res['custom_id']\n",
    "        index = int(task_id.split('-')[1])\n",
    "        atom_index = int(task_id.split('-')[2])\n",
    "        \n",
    "        entry = dataset[index]\n",
    "        atom = entry[split_type].split('--;--')[atom_index]\n",
    "        \n",
    "        predicted = get_openai_prediction(res['body'])\n",
    "        if predicted == 'UNKOWN':\n",
    "            txt_answer = res['response']['body']['choices'][0]['message']['content']\n",
    "            predicted = parse_model_answer(txt_answer)\n",
    "        \n",
    "        data_dict[entry['id']]['atoms'].append({\"atom\": atom, \"predicted\": predicted})\n",
    "\n",
    "def generate_labels(data_dict):\n",
    "    gt_labels = []\n",
    "    pr_labels = []\n",
    "    \n",
    "    for entry_id, entry in data_dict.items():\n",
    "        all_predictions = [decision['predicted'] == 'SUPPORTED' for decision in entry['atoms']]\n",
    "        average_is_supported = np.mean(all_predictions)\n",
    "        data_dict[entry_id]['predicted'] = 'SUPPORTED' if average_is_supported == 1 else 'NOT_SUPPORTED'\n",
    "        \n",
    "        gt_labels.append(1 if entry['label'] == 'SUPPORTED' else 0)\n",
    "        pr_labels.append(1 if entry['predicted'] == 'SUPPORTED' else 0)\n",
    "    \n",
    "    return gt_labels, pr_labels"
   ],
   "id": "16ed886b227f27fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        for split_type in claim_split_types:\n",
    "            evaluate_model(dataset_name, dataset, model, split_type)"
   ],
   "id": "e412f2206d5ec6b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
