{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0 Preparations\n",
    "Before starting, ensure that you have cloned the repository to your Google Drive.\n",
    "We will connect to this:"
   ],
   "id": "89efdde2f316a2ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "repository = 'evaluating_factuality_word_definitions'\n",
    "\n",
    "%cd /content/drive/My Drive/{repository}"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we install the packages and import the modules needed in this notebook:",
   "id": "6231d35ad484e964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install datasets~=2.18.0\n",
    "!pip install openai~=1.35.10"
   ],
   "id": "96b44e8e7b8c3cb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from fetchers.openai import OpenAiFetcher\n",
    "from general_utils.reader import JSONLineReader\n",
    "from general_utils.utils import parse_model_answer, get_openai_prediction"
   ],
   "id": "fdbe8cbeb93713f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 Setup: Define Datasets\n",
    "Now we define our models and datasets we want to evaluate:"
   ],
   "id": "f06ff4f131279207"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Datasets with language information\n",
    "datasets = {\n",
    "    'german_dpr-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german_dpr-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'german-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'squad-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/squad-claim_verification', split='test'),\n",
    "        'lang': 'en'\n",
    "    }\n",
    "}"
   ],
   "id": "58d3ac7722cd6800"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "models = [\n",
    "    'gpt-3.5-turbo',\n",
    "    'gpt-4o-mini',\n",
    "    'gpt-4o'\n",
    "]"
   ],
   "id": "ff15217b5457a60e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "openai_fetcher = OpenAiFetcher()\n",
    "fh = JSONLineReader()"
   ],
   "id": "cc825c4d6b0397ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_task(idx, model, content):\n",
    "    return {\n",
    "            \"custom_id\": f\"task-{idx}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model,\n",
    "                \"temperature\": 0.1,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content\n",
    "                    }\n",
    "                ],\n",
    "                \"seed\": 42\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def create_tasks(dataset, model, file_name, statement_func):\n",
    "    tasks = [build_task(idx, model, f'Input: {statement_func(entry)} True or False?\\nOutput:') for idx, entry in enumerate(dataset)]\n",
    "    fh.write(file_name, tasks)\n",
    "    \n",
    "\n",
    "def process_results(file_name, dataset, translations: bool):\n",
    "    results = fh.read(file_name)\n",
    "    outputs, gt_labels, pr_labels = [], [], []\n",
    "\n",
    "    for res in results:\n",
    "        task_id = res['custom_id']\n",
    "        index = int(task_id.split('-')[-1])\n",
    "        entry = dataset[index]\n",
    "        claim = entry['claim']\n",
    "        word = entry['word']\n",
    "        predicted = get_openai_prediction(res['body'])\n",
    "        if predicted == 'UNKOWN':\n",
    "            txt_answer = res['response']['body']['choices'][0]['message']['content']\n",
    "            predicted = parse_model_answer(txt_answer)\n",
    "        \n",
    "        output = {\n",
    "            'id': entry['id'],\n",
    "            'word': word,\n",
    "            'claim': claim,\n",
    "            'label': entry['label'],\n",
    "            'predicted': predicted,\n",
    "            'in_wiki': entry['in_wiki']\n",
    "        }\n",
    "        \n",
    "        if translations:\n",
    "            output['translated_word'] = entry['english_word']\n",
    "            output['translated_claim'] = entry['english_claim']\n",
    "        \n",
    "        outputs.append(output)\n",
    "        gt_labels.append(1 if entry['label'] == 'SUPPORTED' else 0)\n",
    "        pr_labels.append(1 if entry['predicted'] == 'SUPPORTED' else 0)\n",
    "        \n",
    "    report = classification_report(gt_labels, pr_labels, zero_division=0, digits=4)\n",
    "    return outputs, report"
   ],
   "id": "64c17942c1b6ff67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 Manual Batch Fetching",
   "id": "14a65477f8c45bf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_file_name = ''\n",
    "batch_job = openai_fetcher.create_batch_job(input_file_name)"
   ],
   "id": "751caf640f37d0df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_job = openai_fetcher.get_batch_update(batch_job)\n",
    "print(batch_job)"
   ],
   "id": "8d1713d07602d940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_name = input_file_name.replace('input', 'raw_output')\n",
    "openai_fetcher.get_batch_result(output_file_name, batch_job)"
   ],
   "id": "c72b099bc661f41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 Zero Shot",
   "id": "5a152c88dce51794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "file_base_name = '{type}_zero_shot-{dataset}-{model}'",
   "id": "581980b95c784bd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        create_tasks(\n",
    "            dataset,\n",
    "            model,\n",
    "            file_base_name.format(type='input', dataset=dataset_name, model=model),\n",
    "            lambda entry: f\"{entry['word']}: {entry['claim']}\"\n",
    "        )"
   ],
   "id": "27e5f48b5824d684"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once input files are created, head to 2, to manually fetch your outputs",
   "id": "e127b7d91fc33df8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        print(f\"Evaluating {dataset_name} with {model}...\")\n",
    "        outputs, report = process_results(file_base_name.format(type='raw_output', dataset=dataset_name, model=model), dataset, translations=False)\n",
    "        fh.write(file_base_name.format(type='output', dataset=dataset_name, model=model), outputs)\n",
    "        print(report)"
   ],
   "id": "a50d27f53a05d16a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 Ablation Translated Claims",
   "id": "b0f8f3670917df8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_base_name = '{type}_ablation_translated_claims-{dataset}-{model}'\n",
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    if config['lang'] == 'en':\n",
    "        continue\n",
    "\n",
    "    dataset = config['dataset']    \n",
    "    for model in models:\n",
    "        create_tasks(\n",
    "            dataset,\n",
    "            model,\n",
    "            file_base_name.format(type='input', dataset=dataset_name, model=model),\n",
    "            lambda entry: f\"{entry['english_word']}: {entry['english_claim']}\"\n",
    "        )"
   ],
   "id": "c80afac9e66af1d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once input files are created, head to 2, to manually fetch your outputs",
   "id": "8d2c3ea8259a6a50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        print(f\"Evaluating {dataset_name} with {model}...\")\n",
    "        outputs, report = process_results(file_base_name.format(type='raw_output', dataset=dataset_name, model=model), dataset, translations=False)\n",
    "        fh.write(file_base_name.format(type='output', dataset=dataset_name, model=model), outputs)\n",
    "        print(report)"
   ],
   "id": "54336d77347614db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5 Zero Shot Single Facts",
   "id": "e665b558ede09f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "claim_split_types = [\n",
    "    'DisSim_facts',\n",
    "    'T5SplitRephrase_facts',\n",
    "    'Factscore_facts'\n",
    "]"
   ],
   "id": "36f7a32c6652cd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_base_name = '{type}_zero_shot_{split_type}-{dataset}-{model}'\n",
    "\n",
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        for split_type in claim_split_types:\n",
    "            tasks = []\n",
    "            for idx, entry in tqdm(enumerate(dataset)):\n",
    "                word = entry['word']\n",
    "                atomic_facts = entry[split_type].split('--;--')\n",
    "    \n",
    "                for pidx, atomic_fact in enumerate(atomic_facts):\n",
    "                    tasks.append(build_task(f'{idx}-{pidx}', model, f'Input: {word}: {atomic_fact} True or False?\\nOutput:'))\n",
    "            fh.write(file_base_name.format(type='input', dataset=dataset_name, model=model, split_type=split_type), tasks)"
   ],
   "id": "494c10852271c29d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once input files are created, head to 2, to manually fetch your outputs",
   "id": "2f903b7824971f9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(dataset_name, dataset, model, split_type):\n",
    "    print(f\"Evaluating {dataset_name} with {model} - {split_type}...\")\n",
    "    results = fh.read(file_base_name.format(type='output', dataset=dataset_name, model=model, split_type=split_type))\n",
    "    \n",
    "    data_dict = initialize_data_dict(dataset)\n",
    "    process_results(results, dataset, data_dict, split_type)\n",
    "    gt_labels, pr_labels = generate_labels(data_dict)\n",
    "    \n",
    "    fh.write(file_base_name.format(type='output', dataset=dataset_name, model=model, split_type=split_type), data_dict.values())\n",
    "    print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "\n",
    "def initialize_data_dict(dataset):\n",
    "    data_dict = {}\n",
    "    for entry in dataset:\n",
    "        data_dict[entry['id']] = {\n",
    "            'id': entry['id'],\n",
    "            'word': entry['word'],\n",
    "            'claim': entry['claim'],\n",
    "            'label': entry['label'],\n",
    "            'predicted': -1,\n",
    "            'atoms': [],\n",
    "            'in_wiki': entry['in_wiki']\n",
    "        }\n",
    "    return data_dict\n",
    "\n",
    "def process_results(results, dataset, data_dict, split_type):\n",
    "    for res in results:\n",
    "        task_id = res['custom_id']\n",
    "        index = int(task_id.split('-')[1])\n",
    "        atom_index = int(task_id.split('-')[2])\n",
    "        \n",
    "        entry = dataset[index]\n",
    "        atom = entry[split_type].split('--;--')[atom_index]\n",
    "        \n",
    "        predicted = get_openai_prediction(res['body'])\n",
    "        if predicted == 'UNKOWN':\n",
    "            txt_answer = res['response']['body']['choices'][0]['message']['content']\n",
    "            predicted = parse_model_answer(txt_answer)\n",
    "        \n",
    "        data_dict[entry['id']]['atoms'].append({\"atom\": atom, \"predicted\": predicted})\n",
    "\n",
    "def generate_labels(data_dict):\n",
    "    gt_labels = []\n",
    "    pr_labels = []\n",
    "    \n",
    "    for entry_id, entry in data_dict.items():\n",
    "        all_predictions = [decision['predicted'] == 'SUPPORTED' for decision in entry['atoms']]\n",
    "        average_is_supported = np.mean(all_predictions)\n",
    "        data_dict[entry_id]['predicted'] = 'SUPPORTED' if average_is_supported == 1 else 'NOT_SUPPORTED'\n",
    "        \n",
    "        gt_labels.append(1 if entry['label'] == 'SUPPORTED' else 0)\n",
    "        pr_labels.append(1 if entry['predicted'] == 'SUPPORTED' else 0)\n",
    "    \n",
    "    return gt_labels, pr_labels"
   ],
   "id": "16ed886b227f27fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for dataset_name, config in tqdm(datasets.items()):\n",
    "    dataset = config['dataset']\n",
    "    for model in models:\n",
    "        for split_type in claim_split_types:\n",
    "            evaluate_model(dataset_name, dataset, model, split_type)"
   ],
   "id": "e412f2206d5ec6b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
