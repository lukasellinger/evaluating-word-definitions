{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:10:54.359842Z",
     "start_time": "2024-07-18T14:10:54.337818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataset.def_dataset import DefinitionDataset, Fact\n",
    "from transformers import AutoTokenizer\n",
    "from models.evidence_selection_model import EvidenceSelectionModel\n",
    "from models.claim_verification_model import ClaimVerificationModel\n",
    "import torch\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification\n",
    "from general_utils.fever_scorer import fever_score\n",
    "from pipeline.pipeline import TestPipeline, WikiPipeline\n",
    "from general_utils.utils import build_fever_instance\n",
    "from general_utils.utils import convert_document_id_to_word\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from general_utils.reader import JSONLineReader\n",
    "from datasets import load_dataset"
   ],
   "id": "f64dbae6394734e0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:50:08.264944Z",
     "start_time": "2024-07-18T13:50:08.261024Z"
    }
   },
   "cell_type": "code",
   "source": "fh = JSONLineReader()",
   "id": "d8cc73ac2e8d8783",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:50:27.093014Z",
     "start_time": "2024-07-18T13:50:08.267002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "selection_model_name = 'Snowflake/snowflake-arctic-embed-m-long'\n",
    "selection_model_tokenizer_base = AutoTokenizer.from_pretrained(selection_model_name)\n",
    "selection_model_raw_base = AutoModel.from_pretrained(selection_model_name, trust_remote_code=True, add_pooling_layer=False, safe_serialization=True)\n",
    "selection_model_base = EvidenceSelectionModel(selection_model_raw_base).to(device)\n",
    "\n",
    "selection_model_name = 'lukasellinger/evidence_selection_model-v2'\n",
    "selection_model_tokenizer = AutoTokenizer.from_pretrained(selection_model_name)\n",
    "selection_model_raw = AutoModel.from_pretrained(selection_model_name, trust_remote_code=True, add_pooling_layer=False, safe_serialization=True)\n",
    "selection_model = EvidenceSelectionModel(selection_model_raw).to(device)\n",
    "\n",
    "verification_model_name = 'MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7'\n",
    "verification_model_tokenizer_base = AutoTokenizer.from_pretrained(verification_model_name)\n",
    "verification_model_raw_base = AutoModelForSequenceClassification.from_pretrained(verification_model_name)\n",
    "verification_model_base = ClaimVerificationModel(verification_model_raw_base).to(device)\n",
    "\n",
    "verification_model_name = 'lukasellinger/claim_verification_model-v1'\n",
    "verification_model_tokenizer = AutoTokenizer.from_pretrained(verification_model_name)\n",
    "verification_model_raw = AutoModelForSequenceClassification.from_pretrained(verification_model_name)\n",
    "verification_model = ClaimVerificationModel(verification_model_raw).to(device)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:21:23.720273Z",
     "start_time": "2024-07-03T15:20:40.615331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_dataset = load_dataset(\"lukasellinger/fever_evidence_selection-v1\", cache_dir=None).get('dev')\n",
    "# dataset = DefinitionDataset(raw_dataset, tokenizer=None, model='claim_verification')\n",
    "print(raw_dataset.features)\n",
    "\n",
    "test_pipeline = TestPipeline(selection_model=selection_model,selection_model_tokenizer=selection_model_tokenizer, \n",
    "                             verification_model=verification_model, verification_model_tokenizer=verification_model_tokenizer)\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "fever_instances = []\n",
    "for entry in tqdm(raw_dataset):\n",
    "    word = entry.get('document_id')\n",
    "    fallback_word = convert_document_id_to_word(word)\n",
    "\n",
    "    output = test_pipeline.verify(word, entry['short_claim'], fallback_word, split_facts=False)\n",
    "    if output.get('factuality') == 1:\n",
    "        factuality = Fact.SUPPORTED\n",
    "    else:\n",
    "        factuality = Fact.NOT_SUPPORTED\n",
    "    pr_labels.append(factuality.to_factuality())\n",
    "\n",
    "    if entry['label'] == 'SUPPORTS':\n",
    "        label = Fact.SUPPORTED\n",
    "    else:\n",
    "        label = Fact.NOT_SUPPORTED\n",
    "    gt_labels.append(label.to_factuality())\n",
    "\n",
    "    evidence = entry['evidence_lines'].split(';')\n",
    "    #predicted_label = output.get('factualities')[0]  # TODO add atomic fact support\n",
    "    #predicted_evidence = output.get('evidences')\n",
    "    predicted_evidence = [(x, y) for (x, y, z) in output.get('evidences')]\n",
    "    fever_instance = build_fever_instance(label.name, evidence, entry['document_id'], factuality, predicted_evidence)\n",
    "    fever_instances.append(fever_instance)\n",
    "\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0))\n",
    "strict_score, label_accuracy, precision, recall, f1 = fever_score(fever_instances)\n",
    "\n",
    "print(strict_score)\n",
    "print(label_accuracy)\n",
    "print(precision)  # TP / TP + FP not too important, rather at least one TP than none\n",
    "print(recall)     # more important\n",
    "print(f1)"
   ],
   "id": "ef957373e0ab88e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='int64', id=None), 'claim': Value(dtype='string', id=None), 'short_claim': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'document_id': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'lines': Value(dtype='string', id=None), 'evidence_lines': Value(dtype='string', id=None), 'atomic_facts': Value(dtype='string', id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1978 [00:37<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:51:59.415621Z",
     "start_time": "2024-07-18T13:50:53.169310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "offline_wiki = 'lukasellinger/wiki_dump_2024-07-08'\n",
    "pipeline_base = WikiPipeline(selection_model=selection_model_base, selection_model_tokenizer=selection_model_tokenizer_base, word_lang='de', use_offline_wiki=offline_wiki)\n",
    "pipeline = WikiPipeline(selection_model=selection_model, selection_model_tokenizer=selection_model_tokenizer, word_lang='de', use_offline_wiki=offline_wiki)"
   ],
   "id": "2fb0c78e3d2ad221",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GermanDPR",
   "id": "338a13b04efdf0f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:11:06.258367Z",
     "start_time": "2024-07-18T14:11:00.770846Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"lukasellinger/german_dpr_claim_verification_dissim-v1\").get('train')",
   "id": "f10bf94861830a9c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:11:14.349588Z",
     "start_time": "2024-07-18T14:11:14.338987Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset.features)",
   "id": "c4f9f7e49fc60fca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='int64', id=None), 'question': Value(dtype='string', id=None), 'claim': Value(dtype='string', id=None), 'english_claim': Value(dtype='string', id=None), 'fact': Value(dtype='string', id=None), 'word': Value(dtype='string', id=None), 'english_word': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'document_search_word': Value(dtype='string', id=None), 'connected_claim': Value(dtype='string', id=None), 'atomic_facts_old': Value(dtype='string', id=None), 'atomic_facts': Value(dtype='string', id=None), 'factscore_facts': Value(dtype='string', id=None), 'in_wiki': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:11:19.493106Z",
     "start_time": "2024-07-18T14:11:19.486677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_factuality(factualities, not_in_wiki, lines, pr_labels, gt_labels):\n",
    "    factualities.append(factuality)\n",
    "    \n",
    "    in_wiki = True\n",
    "    if factuality.get('factuality') == 1:\n",
    "        predicted = Fact.SUPPORTED\n",
    "    elif factuality.get('factuality') == -1:\n",
    "        not_in_wiki += 1\n",
    "        in_wiki = False\n",
    "    else:\n",
    "        predicted = Fact.NOT_SUPPORTED\n",
    "    if in_wiki:\n",
    "        pr_labels.append(predicted.to_factuality())\n",
    "        gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "    \n",
    "    lines.append({\n",
    "        'id': entry['id'],\n",
    "        'word': entry['word'],\n",
    "        'claim': claim,\n",
    "        'label': entry['label'],\n",
    "        'predicted': predicted.name,\n",
    "        'atoms': factuality.get('factualities'),\n",
    "        'selected_evidences': factuality.get('evidences')\n",
    "    })"
   ],
   "id": "75d03c25fb944097",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:14:08.039411Z",
     "start_time": "2024-07-18T14:14:08.031750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_prediction(factuality):\n",
    "    in_wiki = True\n",
    "    predicted = None\n",
    "    if factuality.get('factuality') == 1:\n",
    "        predicted = Fact.SUPPORTED\n",
    "    elif factuality.get('factuality') == -1:\n",
    "        in_wiki = False\n",
    "    else:\n",
    "        predicted = Fact.NOT_SUPPORTED\n",
    "        \n",
    "    return predicted, in_wiki\n",
    "\n",
    "def build_output(entry, claim, predicted, factuality):\n",
    "    return {\n",
    "        'id': entry['id'],\n",
    "        'word': entry['word'],\n",
    "        'claim': claim,\n",
    "        'label': entry['label'],\n",
    "        'predicted': predicted.name if predicted else None,\n",
    "        'atoms': factuality.get('factualities'),\n",
    "        'selected_evidences': factuality.get('evidences')\n",
    "    }"
   ],
   "id": "4feba98bb681f249",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## {word} : {claim}",
   "id": "c21fe5d11fa9c9ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:18:43.700739Z",
     "start_time": "2024-07-18T14:14:09.577271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import PROJECT_DIR\n",
    "\n",
    "identification = 'german_dpr_word_claim_finetuned'\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "factualities = []\n",
    "not_in_wiki = 0\n",
    "lines = []\n",
    "for entry in tqdm(dataset):\n",
    "    word = entry.get('word')\n",
    "    english_word = entry.get('english_word', word)\n",
    "    search_word = entry.get('document_search_word')\n",
    "    claim = entry.get('english_claim', entry['claim'])\n",
    "\n",
    "    factuality = pipeline.verify(word, claim, english_word, only_intro=True, split_facts=False, search_word=search_word)\n",
    "    factualities.append(factuality)\n",
    "    \n",
    "    predicted, in_wiki = get_prediction(factuality)\n",
    "    \n",
    "    if in_wiki:\n",
    "        pr_labels.append(predicted.to_factuality())\n",
    "        gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "    else:\n",
    "        not_in_wiki += 1\n",
    "    lines.append(build_output(entry, claim, predicted, factuality))\n",
    "\n",
    "print(f'Not in wiki {not_in_wiki}')\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "fh.write(PROJECT_DIR.joinpath(f'dataset/evaluation/{identification}_pipeline.jsonl'), lines)"
   ],
   "id": "aa4856295ab68dca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [04:34<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in wiki 29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.9130    0.7925        69\n",
      "           1     0.8776    0.6143    0.7227        70\n",
      "\n",
      "    accuracy                         0.7626       139\n",
      "   macro avg     0.7888    0.7637    0.7576       139\n",
      "weighted avg     0.7894    0.7626    0.7573       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:22:35.164140Z",
     "start_time": "2024-07-18T14:18:43.703928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import PROJECT_DIR\n",
    "\n",
    "identification = 'german_dpr_word_claim_base'\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "factualities = []\n",
    "not_in_wiki = 0\n",
    "lines = []\n",
    "for entry in tqdm(dataset):\n",
    "    word = entry.get('word')\n",
    "    english_word = entry.get('english_word', word)\n",
    "    search_word = entry.get('document_search_word')\n",
    "    claim = entry.get('english_claim', entry['claim'])\n",
    "\n",
    "    factuality = pipeline_base.verify(word, claim, english_word, only_intro=True, split_facts=False, search_word=search_word)\n",
    "    factualities.append(factuality)\n",
    "    \n",
    "    predicted, in_wiki = get_prediction(factuality)\n",
    "    \n",
    "    if in_wiki:\n",
    "        pr_labels.append(predicted.to_factuality())\n",
    "        gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "    else:\n",
    "        not_in_wiki += 1\n",
    "    lines.append(build_output(entry, claim, predicted, factuality))\n",
    "\n",
    "print(f'Not in wiki {not_in_wiki}')\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "fh.write(PROJECT_DIR.joinpath(f'dataset/evaluation/{identification}_pipeline.jsonl'), lines)"
   ],
   "id": "66dba0c9a6259997",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [03:51<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in wiki 29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6739    0.8986    0.7702        69\n",
      "           1     0.8511    0.5714    0.6838        70\n",
      "\n",
      "    accuracy                         0.7338       139\n",
      "   macro avg     0.7625    0.7350    0.7270       139\n",
      "weighted avg     0.7631    0.7338    0.7267       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## connected sentence",
   "id": "5ec2ab5bb3598379"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:26:34.012920Z",
     "start_time": "2024-07-18T14:22:35.166422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import PROJECT_DIR\n",
    "\n",
    "identification = 'german_dpr_connected_finetuned'\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "factualities = []\n",
    "not_in_wiki = 0\n",
    "lines = []\n",
    "for entry in tqdm(dataset):\n",
    "    word = entry.get('word')\n",
    "    english_word = entry.get('english_word', word)\n",
    "    search_word = entry.get('document_search_word')\n",
    "    claim = entry.get('connected_claim')\n",
    "\n",
    "    factuality = pipeline.verify3(word, claim, english_word, only_intro=True, split_facts=False, search_word=search_word)\n",
    "    factualities.append(factuality)\n",
    "    \n",
    "    predicted, in_wiki = get_prediction(factuality)\n",
    "    \n",
    "    if in_wiki:\n",
    "        pr_labels.append(predicted.to_factuality())\n",
    "        gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "    else:\n",
    "        not_in_wiki += 1\n",
    "    lines.append(build_output(entry, claim, predicted, factuality))\n",
    "\n",
    "print(f'Not in wiki {not_in_wiki}')\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "fh.write(PROJECT_DIR.joinpath(f'dataset/evaluation/{identification}_pipeline.jsonl'), lines)"
   ],
   "id": "aad50718219e084f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [03:58<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in wiki 29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7356    0.9275    0.8205        69\n",
      "           1     0.9038    0.6714    0.7705        70\n",
      "\n",
      "    accuracy                         0.7986       139\n",
      "   macro avg     0.8197    0.7995    0.7955       139\n",
      "weighted avg     0.8203    0.7986    0.7953       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T14:30:32.408823Z",
     "start_time": "2024-07-18T14:26:34.018675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import PROJECT_DIR\n",
    "\n",
    "identification = 'german_dpr_connected_base'\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "factualities = []\n",
    "not_in_wiki = 0\n",
    "lines = []\n",
    "for entry in tqdm(dataset):\n",
    "    word = entry.get('word')\n",
    "    english_word = entry.get('english_word', word)\n",
    "    search_word = entry.get('document_search_word')\n",
    "    claim = entry.get('connected_claim')\n",
    "\n",
    "    factuality = pipeline_base.verify3(word, claim, english_word, only_intro=True, split_facts=False, search_word=search_word)\n",
    "    factualities.append(factuality)\n",
    "    \n",
    "    predicted, in_wiki = get_prediction(factuality)\n",
    "    \n",
    "    if in_wiki:\n",
    "        pr_labels.append(predicted.to_factuality())\n",
    "        gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "    else:\n",
    "        not_in_wiki += 1\n",
    "    lines.append(build_output(entry, claim, predicted, factuality))\n",
    "\n",
    "print(f'Not in wiki {not_in_wiki}')\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "fh.write(PROJECT_DIR.joinpath(f'dataset/evaluation/{identification}_pipeline.jsonl'), lines)"
   ],
   "id": "6edaaadd78ee0da0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [03:58<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in wiki 29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6818    0.8696    0.7643        69\n",
      "           1     0.8235    0.6000    0.6942        70\n",
      "\n",
      "    accuracy                         0.7338       139\n",
      "   macro avg     0.7527    0.7348    0.7293       139\n",
      "weighted avg     0.7532    0.7338    0.7290       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## dissim facts",
   "id": "eab7afa761d1d1cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:13:45.201101Z",
     "start_time": "2024-07-18T15:08:58.460192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import PROJECT_DIR\n",
    "\n",
    "identification = 'german_dpr_dissim_finetuned'\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "factualities = []\n",
    "not_in_wiki = 0\n",
    "lines = []\n",
    "for entry in tqdm(dataset):\n",
    "    word = entry.get('word')\n",
    "    english_word = entry.get('english_word', word)\n",
    "    search_word = entry.get('document_search_word')\n",
    "    claim = f'{search_word}: {entry.get(\"english_claim\", entry[\"claim\"])}'\n",
    "    atomic_facts = entry['atomic_facts']\n",
    "    atomic_facts = atomic_facts.split('--;--') if atomic_facts else []\n",
    "\n",
    "    factuality = pipeline.verify3(word, claim, english_word, only_intro=True, split_facts=False, search_word=search_word, atomic_claims=atomic_facts)\n",
    "    factualities.append(factuality)\n",
    "    \n",
    "    predicted, in_wiki = get_prediction(factuality)\n",
    "    \n",
    "    if in_wiki:\n",
    "        pr_labels.append(predicted.to_factuality())\n",
    "        gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "    else:\n",
    "        not_in_wiki += 1\n",
    "    lines.append(build_output(entry, claim, predicted, factuality))\n",
    "\n",
    "print(f'Not in wiki {not_in_wiki}')\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "fh.write(PROJECT_DIR.joinpath(f'dataset/evaluation/{identification}_pipeline.jsonl'), lines)"
   ],
   "id": "7c2b50b0006e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [04:46<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in wiki 29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6381    0.9710    0.7701        69\n",
      "           1     0.9412    0.4571    0.6154        70\n",
      "\n",
      "    accuracy                         0.7122       139\n",
      "   macro avg     0.7896    0.7141    0.6927       139\n",
      "weighted avg     0.7907    0.7122    0.6922       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:18:20.399713Z",
     "start_time": "2024-07-18T15:13:45.203939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import PROJECT_DIR\n",
    "\n",
    "identification = 'german_dpr_dissim_base'\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "factualities = []\n",
    "not_in_wiki = 0\n",
    "lines = []\n",
    "for entry in tqdm(dataset):\n",
    "    word = entry.get('word')\n",
    "    english_word = entry.get('english_word', word)\n",
    "    search_word = entry.get('document_search_word')\n",
    "    claim = f'{search_word}: {entry.get(\"english_claim\", entry[\"claim\"])}'\n",
    "    atomic_facts = entry['atomic_facts']\n",
    "    atomic_facts = atomic_facts.split('--;--') if atomic_facts else []\n",
    "\n",
    "    factuality = pipeline_base.verify3(word, claim, english_word, only_intro=True, split_facts=False, search_word=search_word, atomic_claims=atomic_facts)\n",
    "    factualities.append(factuality)\n",
    "    \n",
    "    predicted, in_wiki = get_prediction(factuality)\n",
    "    \n",
    "    if in_wiki:\n",
    "        pr_labels.append(predicted.to_factuality())\n",
    "        gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "    else:\n",
    "        not_in_wiki += 1\n",
    "    lines.append(build_output(entry, claim, predicted, factuality))\n",
    "\n",
    "print(f'Not in wiki {not_in_wiki}')\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "fh.write(PROJECT_DIR.joinpath(f'dataset/evaluation/{identification}_pipeline.jsonl'), lines)"
   ],
   "id": "1091fb2ac5bad150",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [04:35<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in wiki 29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6465    0.9275    0.7619        69\n",
      "           1     0.8750    0.5000    0.6364        70\n",
      "\n",
      "    accuracy                         0.7122       139\n",
      "   macro avg     0.7607    0.7138    0.6991       139\n",
      "weighted avg     0.7616    0.7122    0.6987       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## factscore facts",
   "id": "1c1292678ba10fa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:23:49.063533Z",
     "start_time": "2024-07-18T15:18:20.402309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import PROJECT_DIR\n",
    "\n",
    "identification = 'german_dpr_factscore_finetuned'\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "factualities = []\n",
    "not_in_wiki = 0\n",
    "lines = []\n",
    "for entry in tqdm(dataset):\n",
    "    word = entry.get('word')\n",
    "    english_word = entry.get('english_word', word)\n",
    "    search_word = entry.get('document_search_word')\n",
    "    claim = f'{search_word}: {entry.get(\"english_claim\", entry[\"claim\"])}'\n",
    "    atomic_facts = entry['factscore_facts']\n",
    "    atomic_facts = atomic_facts.split('--;--') if atomic_facts else []\n",
    "\n",
    "    factuality = pipeline_base.verify3(word, claim, english_word, only_intro=True, split_facts=False, search_word=search_word, atomic_claims=atomic_facts)\n",
    "    factualities.append(factuality)\n",
    "    \n",
    "    predicted, in_wiki = get_prediction(factuality)\n",
    "    \n",
    "    if in_wiki:\n",
    "        pr_labels.append(predicted.to_factuality())\n",
    "        gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "    else:\n",
    "        not_in_wiki += 1\n",
    "    lines.append(build_output(entry, claim, predicted, factuality))\n",
    "\n",
    "print(f'Not in wiki {not_in_wiki}')\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "fh.write(PROJECT_DIR.joinpath(f'dataset/evaluation/{identification}_pipeline.jsonl'), lines)"
   ],
   "id": "1387e69b03579aa8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [05:28<00:00,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in wiki 29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5877    0.9710    0.7322        69\n",
      "           1     0.9200    0.3286    0.4842        70\n",
      "\n",
      "    accuracy                         0.6475       139\n",
      "   macro avg     0.7539    0.6498    0.6082       139\n",
      "weighted avg     0.7551    0.6475    0.6073       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:29:16.547002Z",
     "start_time": "2024-07-18T15:23:49.067282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import PROJECT_DIR\n",
    "\n",
    "identification = 'german_dpr_factscore_base'\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "factualities = []\n",
    "not_in_wiki = 0\n",
    "lines = []\n",
    "for entry in tqdm(dataset):\n",
    "    word = entry.get('word')\n",
    "    english_word = entry.get('english_word', word)\n",
    "    search_word = entry.get('document_search_word')\n",
    "    claim = f'{search_word}: {entry.get(\"english_claim\", entry[\"claim\"])}'\n",
    "    atomic_facts = entry['factscore_facts']\n",
    "    atomic_facts = atomic_facts.split('--;--') if atomic_facts else []\n",
    "\n",
    "    factuality = pipeline_base.verify3(word, claim, english_word, only_intro=True, split_facts=False, search_word=search_word, atomic_claims=atomic_facts)\n",
    "    factualities.append(factuality)\n",
    "    \n",
    "    predicted, in_wiki = get_prediction(factuality)\n",
    "    \n",
    "    if in_wiki:\n",
    "        pr_labels.append(predicted.to_factuality())\n",
    "        gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "    else:\n",
    "        not_in_wiki += 1\n",
    "    lines.append(build_output(entry, claim, predicted, factuality))\n",
    "\n",
    "print(f'Not in wiki {not_in_wiki}')\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))\n",
    "fh.write(PROJECT_DIR.joinpath(f'dataset/evaluation/{identification}_pipeline.jsonl'), lines)"
   ],
   "id": "b63b07f485dbb3e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [05:27<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in wiki 29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5877    0.9710    0.7322        69\n",
      "           1     0.9200    0.3286    0.4842        70\n",
      "\n",
      "    accuracy                         0.6475       139\n",
      "   macro avg     0.7539    0.6498    0.6082       139\n",
      "weighted avg     0.7551    0.6475    0.6073       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "62267080e641376c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
