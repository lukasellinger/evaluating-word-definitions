{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T16:21:55.736400Z",
     "start_time": "2024-07-08T16:21:41.966900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataset.def_dataset import DefinitionDataset, Fact\n",
    "from transformers import AutoTokenizer\n",
    "from models.evidence_selection_model import EvidenceSelectionModel\n",
    "from models.claim_verification_model import ClaimVerificationModel\n",
    "import torch\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification\n",
    "from general_utils.fever_scorer import fever_score\n",
    "from pipeline.pipeline import TestPipeline, WikiPipeline\n",
    "from general_utils.utils import build_fever_instance\n",
    "from general_utils.utils import convert_document_id_to_word\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ],
   "id": "f64dbae6394734e0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T16:22:16.333579Z",
     "start_time": "2024-07-08T16:22:09.156329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#selection_model_tokenizer = AutoTokenizer.from_pretrained('Snowflake/snowflake-arctic-embed-m-long')\n",
    "#selection_model_raw = AutoModel.from_pretrained('Snowflake/snowflake-arctic-embed-m-long', trust_remote_code=True, add_pooling_layer=False, safe_serialization=True)\n",
    "selection_model_name = 'lukasellinger/evidence_selection_model-v2'\n",
    "selection_model_tokenizer = AutoTokenizer.from_pretrained(selection_model_name)\n",
    "selection_model_raw = AutoModel.from_pretrained(selection_model_name, trust_remote_code=True, add_pooling_layer=False, safe_serialization=True)\n",
    "selection_model = EvidenceSelectionModel(selection_model_raw).to(device)\n",
    "\n",
    "#verification_model_name = 'MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7'\n",
    "verification_model_name = 'lukasellinger/claim_verification_model-v1'\n",
    "verification_model_tokenizer = AutoTokenizer.from_pretrained(verification_model_name)\n",
    "verification_model_raw = AutoModelForSequenceClassification.from_pretrained(verification_model_name)\n",
    "verification_model = ClaimVerificationModel(verification_model_raw).to(device)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:21:23.720273Z",
     "start_time": "2024-07-03T15:20:40.615331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"lukasellinger/fever_evidence_selection-v1\", cache_dir=None).get('dev')\n",
    "# dataset = DefinitionDataset(raw_dataset, tokenizer=None, model='claim_verification')\n",
    "print(raw_dataset.features)\n",
    "\n",
    "test_pipeline = TestPipeline(selection_model=selection_model,selection_model_tokenizer=selection_model_tokenizer, \n",
    "                             verification_model=verification_model, verification_model_tokenizer=verification_model_tokenizer)\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "fever_instances = []\n",
    "for entry in tqdm(raw_dataset):\n",
    "    word = entry.get('document_id')\n",
    "    fallback_word = convert_document_id_to_word(word)\n",
    "\n",
    "    output = test_pipeline.verify(word, entry['short_claim'], fallback_word, split_facts=False)\n",
    "    if output.get('factuality') == 1:\n",
    "        factuality = Fact.SUPPORTED\n",
    "    else:\n",
    "        factuality = Fact.NOT_SUPPORTED\n",
    "    pr_labels.append(factuality.to_factuality())\n",
    "\n",
    "    if entry['label'] == 'SUPPORTS':\n",
    "        label = Fact.SUPPORTED\n",
    "    else:\n",
    "        label = Fact.NOT_SUPPORTED\n",
    "    gt_labels.append(label.to_factuality())\n",
    "\n",
    "    evidence = entry['evidence_lines'].split(';')\n",
    "    #predicted_label = output.get('factualities')[0]  # TODO add atomic fact support\n",
    "    #predicted_evidence = output.get('evidences')\n",
    "    predicted_evidence = [(x, y) for (x, y, z) in output.get('evidences')]\n",
    "    fever_instance = build_fever_instance(label.name, evidence, entry['document_id'], factuality, predicted_evidence)\n",
    "    fever_instances.append(fever_instance)\n",
    "\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0))\n",
    "strict_score, label_accuracy, precision, recall, f1 = fever_score(fever_instances)\n",
    "\n",
    "print(strict_score)\n",
    "print(label_accuracy)\n",
    "print(precision)  # TP / TP + FP not too important, rather at least one TP than none\n",
    "print(recall)     # more important\n",
    "print(f1)"
   ],
   "id": "ef957373e0ab88e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='int64', id=None), 'claim': Value(dtype='string', id=None), 'short_claim': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'document_id': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'lines': Value(dtype='string', id=None), 'evidence_lines': Value(dtype='string', id=None), 'atomic_facts': Value(dtype='string', id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1978 [00:37<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T16:28:18.650278Z",
     "start_time": "2024-07-08T16:22:38.921540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lukasellinger/german_dpr_claim_verification_dissim-v1\").get('train')\n",
    "offline_wiki = 'lukasellinger/wiki_dump_2024-07-08'\n",
    "print(dataset.features)\n",
    "\n",
    "pipeline = WikiPipeline(selection_model=selection_model, selection_model_tokenizer=selection_model_tokenizer, word_lang='de', use_offline_wiki=offline_wiki)\n",
    "\n",
    "pr_labels = []\n",
    "gt_labels = []\n",
    "factualities = []\n",
    "not_in_wiki = 0\n",
    "for entry in tqdm(dataset):\n",
    "    word = entry.get('word')\n",
    "    english_word = entry.get('english_word', word)\n",
    "    search_word = entry.get('document_search_word')\n",
    "    claim = entry.get('english_claim', entry['claim'])\n",
    "    atomic_facts = entry['atomic_facts']\n",
    "    atomic_facts = atomic_facts.split('--;--') if atomic_facts else []\n",
    "    \n",
    "    factuality = pipeline.verify(word, claim, english_word, only_intro=True, split_facts=False, search_word=search_word)\n",
    "    factualities.append(factuality)\n",
    "    if factuality.get('factuality') == 1:\n",
    "        pr_labels.append(Fact.SUPPORTED.to_factuality())\n",
    "    elif factuality.get('factuality') == -1:\n",
    "        not_in_wiki += 1\n",
    "        continue\n",
    "    else:\n",
    "        pr_labels.append(Fact.NOT_SUPPORTED.to_factuality())\n",
    "    gt_labels.append(Fact[entry['label']].to_factuality())\n",
    "\n",
    "print(f'Not in wiki {not_in_wiki}')\n",
    "print(classification_report(gt_labels, pr_labels, zero_division=0))"
   ],
   "id": "aa4856295ab68dca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='int64', id=None), 'question': Value(dtype='string', id=None), 'claim': Value(dtype='string', id=None), 'english_claim': Value(dtype='string', id=None), 'fact': Value(dtype='string', id=None), 'word': Value(dtype='string', id=None), 'english_word': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'atomic_facts': Value(dtype='string', id=None), 'document_search_word': Value(dtype='string', id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [04:59<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in wiki 29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.91      0.79        69\n",
      "           1       0.88      0.61      0.72        70\n",
      "\n",
      "    accuracy                           0.76       139\n",
      "   macro avg       0.79      0.76      0.76       139\n",
      "weighted avg       0.79      0.76      0.76       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "61c5aa3f458ba50f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
