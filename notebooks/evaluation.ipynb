{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-17T20:07:06.586265Z",
     "start_time": "2024-07-17T20:07:06.507103Z"
    }
   },
   "source": [
    "from general_utils.reader import JSONLineReader\n",
    "from sklearn.metrics import classification_report\n",
    "from dataset.def_dataset import Fact\n",
    "from config import PROJECT_DIR"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:35:27.409916Z",
     "start_time": "2024-07-17T22:35:21.778100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"lukasellinger/german_dpr_claim_verification_dissim-v1\"\n",
    "dataset = load_dataset(dataset_name).get('train')\n",
    "outputs = JSONLineReader().read(PROJECT_DIR.joinpath('dataset/openai/output/german_dpr/output_german_dpr_factscore-gpt3_5-turbo-gtr.jsonl'))\n",
    "outputs = {d['id']: d for d in outputs}"
   ],
   "id": "a31f0196f3196fa8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:35:29.521760Z",
     "start_time": "2024-07-17T22:35:29.489894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exclude_not_in_wiki = True\n",
    "\n",
    "gt_labels = []\n",
    "pr_labels = []\n",
    "for entry in dataset:\n",
    "    if exclude_not_in_wiki and entry['in_wiki'] == 'No':\n",
    "        continue\n",
    "    output = outputs[entry['id']]\n",
    "    pr_labels.append(Fact[output['predicted']].to_factuality())\n",
    "    gt_labels.append(Fact[output['label']].to_factuality())"
   ],
   "id": "fc892c04b3536c1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:35:31.451238Z",
     "start_time": "2024-07-17T22:35:31.412939Z"
    }
   },
   "cell_type": "code",
   "source": "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))",
   "id": "57ffb9943127e47a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5312    0.9855    0.6904        69\n",
      "           1     0.9091    0.1429    0.2469        70\n",
      "\n",
      "    accuracy                         0.5612       139\n",
      "   macro avg     0.7202    0.5642    0.4686       139\n",
      "weighted avg     0.7215    0.5612    0.4670       139\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "repository = 'evaluating_factuality_word_definitions'\n",
    "\n",
    "%cd /content/drive/My Drive/{repository}"
   ],
   "id": "82121a98ee7c420c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from general_utils.utils import print_classification_report\n",
    "from pipeline.statement_verifier import ModelStatementVerifier\n",
    "from pipeline.evidence_selector import ModelEvidenceSelector\n",
    "from pipeline.translator import OpusMTTranslator\n",
    "from pipeline.sentence_connector import PhiSentenceConnector, ColonSentenceConnector\n",
    "from pipeline.evidence_fetcher import WikipediaEvidenceFetcher\n",
    "from pipeline.pipeline import Pipeline\n",
    "from pipeline.claim_splitter import DisSimSplitter, T5SplitRephraseSplitter, FactscoreSplitter"
   ],
   "id": "b7c7b2cf02eae43c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_selection_model = 'Snowflake/snowflake-arctic-embed-m-long'\n",
    "base_verification_model = 'MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7'\n",
    "\n",
    "finetuned_selection_model = 'lukasellinger/evidence_selection_model-v2'\n",
    "finetuned_verification_model = 'lukasellinger/claim_verification_model-v1'"
   ],
   "id": "40d038ee87be8aed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "datasets = {\n",
    "    'german_dpr_dataset': {\n",
    "        'dataset': load_dataset('lukasellinger/german_dpr_claim_verification_dissim-v1'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'german_dataset': {\n",
    "        'dataset': load_dataset('lukasellinger/german_claim_verification_dissim-v1'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'squad_dataset': {\n",
    "                'dataset': load_dataset('lukasellinger/squad_claim_verification_dissim-v1'),\n",
    "        'lang': 'en'\n",
    "    }\n",
    "}"
   ],
   "id": "26dff9eae25d4da9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_pipeline(pipeline, dataset, output_file_name=''):\n",
    "    outputs, report, not_in_wiki = pipeline.verify_test_dataset(dataset, output_file_name)\n",
    "    avg_claim_count = sum(len(output['atoms']) for output in outputs) / len(outputs) if outputs else 0  \n",
    "    print_classification_report(report, not_in_wiki, avg_claim_count)\n",
    "    return outputs"
   ],
   "id": "7ff307516b01c8e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Init models",
   "id": "bc4adf5dde45a3f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Translator\n",
    "translator = OpusMTTranslator()\n",
    "\n",
    "# Sentence Connectors\n",
    "colon_sentence_connector = ColonSentenceConnector()\n",
    "phi_sentence_connector = PhiSentenceConnector()\n",
    "\n",
    "# Evidence Fetcher\n",
    "offline_evid_fetcher = WikipediaEvidenceFetcher()\n",
    "online_evid_fetcher = WikipediaEvidenceFetcher(offline=False)\n",
    "\n",
    "pipeline_models = {\n",
    "    'base': {\n",
    "        'evid_selector': ModelEvidenceSelector(model_name=base_selection_model),\n",
    "        'stm_verifier': ModelStatementVerifier(model_name=base_verification_model)\n",
    "    },\n",
    "    'finetuned': {\n",
    "        'evid_selector': ModelEvidenceSelector(model_name=finetuned_selection_model),\n",
    "        'stm_verifier': ModelStatementVerifier(model_name=finetuned_verification_model)\n",
    "    }\n",
    "}"
   ],
   "id": "a90688f2c34bad97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finetuned vs Base (Best Setup)",
   "id": "b515e0bcde2b6a25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_base_name = \"{}\"\n",
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "    for name, models in pipeline_models.items():\n",
    "        print(f\"Evaluating {dataset_name} with {name}...\")\n",
    "        pipeline = Pipeline(translator=translator,\n",
    "                        sent_connector=phi_sentence_connector,\n",
    "                        claim_splitter=None,\n",
    "                        evid_fetcher=offline_evid_fetcher,\n",
    "                        evid_selector=models.get('evid_selector'),\n",
    "                        stm_verifier=models.get('stm_verifier'),\n",
    "                        lang=lang)\n",
    "        evaluate_pipeline(pipeline, dataset, output_file_base_name.format(name=name))"
   ],
   "id": "cfc5253e611023ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate different Claim Splitters",
   "id": "e876b9d09804e464"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "claim_splitters = {\n",
    "    'DisSimSplitter': DisSimSplitter(),\n",
    "    'T5SplitRephraseSplitter': T5SplitRephraseSplitter(),\n",
    "    'FactscoreSplitter': FactscoreSplitter()\n",
    "}"
   ],
   "id": "73ac75db95a6d25b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_base_name = \"{}\"\n",
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "    for name, splitter in claim_splitters.items():\n",
    "        print(f\"Evaluating {dataset_name} with {name}...\")\n",
    "        pipeline = Pipeline(translator=translator,\n",
    "                        sent_connector=phi_sentence_connector,\n",
    "                        claim_splitter=splitter,\n",
    "                        evid_fetcher=offline_evid_fetcher,\n",
    "                        evid_selector=pipeline_models['finetuned']['evid_selector'],\n",
    "                        stm_verifier=pipeline_models['finetuned']['stm_verifier'],\n",
    "                        lang=lang)\n",
    "        evaluate_pipeline(pipeline, dataset, output_file_base_name.format(name=name))"
   ],
   "id": "54be8e1ab3f695c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
