{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T18:33:50.347535Z",
     "start_time": "2024-07-29T18:33:45.888408Z"
    }
   },
   "source": [
    "from general_utils.reader import JSONLineReader\n",
    "from sklearn.metrics import classification_report\n",
    "from dataset.def_dataset import Fact\n",
    "from config import PROJECT_DIR"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:35:27.409916Z",
     "start_time": "2024-07-17T22:35:21.778100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"lukasellinger/german_dpr_claim_verification_dissim-v1\"\n",
    "dataset = load_dataset(dataset_name).get('train')\n",
    "outputs = JSONLineReader().read(PROJECT_DIR.joinpath('dataset/openai/output/german_dpr/output_german_dpr_factscore-gpt3_5-turbo-gtr.jsonl'))\n",
    "outputs = {d['id']: d for d in outputs}"
   ],
   "id": "a31f0196f3196fa8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:35:29.521760Z",
     "start_time": "2024-07-17T22:35:29.489894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exclude_not_in_wiki = True\n",
    "\n",
    "gt_labels = []\n",
    "pr_labels = []\n",
    "for entry in dataset:\n",
    "    if exclude_not_in_wiki and entry['in_wiki'] == 'No':\n",
    "        continue\n",
    "    output = outputs[entry['id']]\n",
    "    pr_labels.append(Fact[output['predicted']].to_factuality())\n",
    "    gt_labels.append(Fact[output['label']].to_factuality())"
   ],
   "id": "fc892c04b3536c1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T22:35:31.451238Z",
     "start_time": "2024-07-17T22:35:31.412939Z"
    }
   },
   "cell_type": "code",
   "source": "print(classification_report(gt_labels, pr_labels, zero_division=0, digits=4))",
   "id": "57ffb9943127e47a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5312    0.9855    0.6904        69\n",
      "           1     0.9091    0.1429    0.2469        70\n",
      "\n",
      "    accuracy                         0.5612       139\n",
      "   macro avg     0.7202    0.5642    0.4686       139\n",
      "weighted avg     0.7215    0.5612    0.4670       139\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "repository = 'evaluating_factuality_word_definitions'\n",
    "\n",
    "%cd /content/drive/My Drive/{repository}"
   ],
   "id": "82121a98ee7c420c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T18:49:47.495136Z",
     "start_time": "2024-07-29T18:49:34.388279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from config import PROJECT_DIR\n",
    "from general_utils.utils import print_classification_report\n",
    "from pipeline.statement_verifier import ModelStatementVerifier\n",
    "from pipeline.evidence_selector import ModelEvidenceSelector\n",
    "from pipeline.translator import OpusMTTranslator\n",
    "from pipeline.sentence_connector import PhiSentenceConnector, ColonSentenceConnector\n",
    "from pipeline.evidence_fetcher import WikipediaEvidenceFetcher\n",
    "from pipeline.pipeline import Pipeline\n",
    "from pipeline.claim_splitter import DisSimSplitter, T5SplitRephraseSplitter, FactscoreSplitter"
   ],
   "id": "b7c7b2cf02eae43c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T18:49:47.502421Z",
     "start_time": "2024-07-29T18:49:47.498147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_selection_model = 'Snowflake/snowflake-arctic-embed-m-long'\n",
    "base_verification_model = 'MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7'\n",
    "\n",
    "finetuned_selection_model = 'lukasellinger/evidence_selection_model-v2'\n",
    "finetuned_verification_model = 'lukasellinger/claim_verification_model-v1'"
   ],
   "id": "40d038ee87be8aed",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T18:50:04.018184Z",
     "start_time": "2024-07-29T18:49:49.608015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = {\n",
    "    'german_dpr-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german_dpr-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'german-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/german-claim_verification', split='test'),\n",
    "        'lang': 'de'\n",
    "    },\n",
    "    'squad-claim_verification': {\n",
    "        'dataset': load_dataset('lukasellinger/squad-claim_verification', split='test'),\n",
    "        'lang': 'en'\n",
    "    }\n",
    "}"
   ],
   "id": "26dff9eae25d4da9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:25:39.841379Z",
     "start_time": "2024-07-29T19:25:39.833053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_pipeline(pipeline: Pipeline, dataset, batch_size=4, output_file_name='', only_intro=True):\n",
    "    outputs, report, not_in_wiki = pipeline.verify_test_dataset(dataset, batch_size, output_file_name, only_intro)\n",
    "    total_claim_count = sum(len(entry['atoms']) for entry in outputs if entry.get('atoms'))\n",
    "    total = sum(1 for entry in outputs if entry.get('atoms'))\n",
    "    avg_claim_count = total_claim_count / total if total > 0 else 0\n",
    "    print_classification_report(report, not_in_wiki, avg_claim_count)\n",
    "    return outputs"
   ],
   "id": "7ff307516b01c8e9",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Init models",
   "id": "bc4adf5dde45a3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T18:50:42.156654Z",
     "start_time": "2024-07-29T18:50:04.028698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Translator\n",
    "translator = OpusMTTranslator()\n",
    "\n",
    "# Sentence Connectors\n",
    "colon_sentence_connector = ColonSentenceConnector()\n",
    "phi_sentence_connector = PhiSentenceConnector()\n",
    "\n",
    "# Evidence Fetcher\n",
    "offline_evid_fetcher = WikipediaEvidenceFetcher()\n",
    "online_evid_fetcher = WikipediaEvidenceFetcher(offline=False)\n",
    "\n",
    "pipeline_models = {\n",
    "    'base': {\n",
    "        'evid_selector': ModelEvidenceSelector(model_name=base_selection_model),\n",
    "        'stm_verifier': ModelStatementVerifier(model_name=base_verification_model)\n",
    "    },\n",
    "    'finetuned': {\n",
    "        'evid_selector': ModelEvidenceSelector(model_name=finetuned_selection_model),\n",
    "        'stm_verifier': ModelStatementVerifier(model_name=finetuned_verification_model)\n",
    "    }\n",
    "}"
   ],
   "id": "a90688f2c34bad97",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d326706dda93445eb75b40dbdabbdc5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finetuned vs Base (Best Setup)",
   "id": "b515e0bcde2b6a25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T21:15:44.546239Z",
     "start_time": "2024-07-29T19:26:03.201196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file_base_name = PROJECT_DIR.joinpath(\"data/evaluation/{dataset}_{model}.jsonl\")\n",
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "    for model_name, models in pipeline_models.items():\n",
    "        print(f\"Evaluating {dataset_name} with pipeline {model_name}...\")\n",
    "        pipeline = Pipeline(translator=translator,\n",
    "                        sent_connector=phi_sentence_connector,\n",
    "                        claim_splitter=None,\n",
    "                        evid_fetcher=offline_evid_fetcher,\n",
    "                        evid_selector=models.get('evid_selector'),\n",
    "                        stm_verifier=models.get('stm_verifier'),\n",
    "                        lang=lang)\n",
    "        evaluate_pipeline(pipeline, dataset, output_file_name=output_file_base_name.format(dataset=dataset_name, model_name=model_name))"
   ],
   "id": "cfc5253e611023ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating german_dpr_dataset with pipeline base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [04:41<00:00,  6.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Not in wikipedia: 29\n",
      "Avg claim count: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6818    0.8696    0.7643        69\n",
      "           1     0.8235    0.6000    0.6942        70\n",
      "\n",
      "    accuracy                         0.7338       139\n",
      "   macro avg     0.7527    0.7348    0.7293       139\n",
      "weighted avg     0.7532    0.7338    0.7290       139\n",
      "\n",
      "################################\n",
      "Evaluating german_dpr_dataset with pipeline finetuned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [04:28<00:00,  6.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Not in wikipedia: 29\n",
      "Avg claim count: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7792    0.8696    0.8219        69\n",
      "           1     0.8548    0.7571    0.8030        70\n",
      "\n",
      "    accuracy                         0.8129       139\n",
      "   macro avg     0.8170    0.8134    0.8125       139\n",
      "weighted avg     0.8173    0.8129    0.8124       139\n",
      "\n",
      "################################\n",
      "Evaluating german_dataset with pipeline base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [46:54<00:00, 15.81s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Not in wikipedia: 26\n",
      "Avg claim count: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5645    0.8772    0.6870       334\n",
      "           1     0.7515    0.3543    0.4816       350\n",
      "\n",
      "    accuracy                         0.6096       684\n",
      "   macro avg     0.6580    0.6158    0.5843       684\n",
      "weighted avg     0.6602    0.6096    0.5819       684\n",
      "\n",
      "################################\n",
      "Evaluating german_dataset with pipeline finetuned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [46:35<00:00, 15.71s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Not in wikipedia: 26\n",
      "Avg claim count: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5699    0.7934    0.6633       334\n",
      "           1     0.6849    0.4286    0.5272       350\n",
      "\n",
      "    accuracy                         0.6067       684\n",
      "   macro avg     0.6274    0.6110    0.5953       684\n",
      "weighted avg     0.6288    0.6067    0.5937       684\n",
      "\n",
      "################################\n",
      "Evaluating squad_dataset with pipeline base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:23<00:00,  5.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Not in wikipedia: 32\n",
      "Avg claim count: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.8571    0.8000        63\n",
      "           1     0.8333    0.7143    0.7692        63\n",
      "\n",
      "    accuracy                         0.7857       126\n",
      "   macro avg     0.7917    0.7857    0.7846       126\n",
      "weighted avg     0.7917    0.7857    0.7846       126\n",
      "\n",
      "################################\n",
      "Evaluating squad_dataset with pipeline finetuned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:37<00:00,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Not in wikipedia: 32\n",
      "Avg claim count: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.8889    0.8421        63\n",
      "           1     0.8750    0.7778    0.8235        63\n",
      "\n",
      "    accuracy                         0.8333       126\n",
      "   macro avg     0.8375    0.8333    0.8328       126\n",
      "weighted avg     0.8375    0.8333    0.8328       126\n",
      "\n",
      "################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Online Finetuned Pipeline (Best Setup)",
   "id": "35c70a72c490dc6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_base_name = PROJECT_DIR.joinpath(\"data/evaluation/{dataset}_finetuned_online.jsonl\")\n",
    "\n",
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "    print(f\"Evaluating {dataset_name}...\")\n",
    "    pipeline = Pipeline(translator=translator,\n",
    "                    sent_connector=phi_sentence_connector,\n",
    "                    claim_splitter=None,\n",
    "                    evid_fetcher=online_evid_fetcher,\n",
    "                    evid_selector=pipeline_models['finetuned']['evid_selector'],\n",
    "                    stm_verifier=pipeline_models['finetuned']['stm_verifier'],\n",
    "                    lang=lang)\n",
    "    evaluate_pipeline(pipeline, dataset, output_file_name=output_file_base_name.format(dataset=dataset_name))"
   ],
   "id": "48756de4e1d0b58c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate different Claim Splitters",
   "id": "e876b9d09804e464"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "claim_splitters = {\n",
    "    'DisSimSplitter': DisSimSplitter(),\n",
    "    'T5SplitRephraseSplitter': T5SplitRephraseSplitter(),\n",
    "    'FactscoreSplitter': FactscoreSplitter(),\n",
    "    'None': None\n",
    "}"
   ],
   "id": "73ac75db95a6d25b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this setting, the splits are not recalculated; instead, they are reused as they are already present in the dataset.",
   "id": "e10a3b3786578985"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_base_name = PROJECT_DIR.joinpath(\"data/evaluation/{dataset}_finetuned_{splitter}.jsonl\")\n",
    "\n",
    "for dataset_name, config in datasets.items():\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "    for name, splitter in claim_splitters.items():\n",
    "        print(f\"Evaluating {dataset_name} with claim splitter {name}...\")\n",
    "        pipeline = Pipeline(translator=translator,\n",
    "                        sent_connector=phi_sentence_connector,\n",
    "                        claim_splitter=splitter,\n",
    "                        evid_fetcher=offline_evid_fetcher,\n",
    "                        evid_selector=pipeline_models['finetuned']['evid_selector'],\n",
    "                        stm_verifier=pipeline_models['finetuned']['stm_verifier'],\n",
    "                        lang=lang)\n",
    "        evaluate_pipeline(pipeline, dataset, output_file_name=output_file_base_name.format(dataset=dataset_name, splitter=name))"
   ],
   "id": "54be8e1ab3f695c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Whole Wiki Page Run (Best setup)",
   "id": "ad649a80875a1c04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_base_name = PROJECT_DIR.joinpath(\"data/evaluation/{dataset}_finetuned_whole_page.jsonl\")\n",
    "max_intro_sent_indices = offline_evid_fetcher.get_max_intro_sent_idx()\n",
    "\n",
    "for dataset_name, config in datasets.items():\n",
    "    print(f\"Evaluating {dataset_name}...\")\n",
    "    dataset = config['dataset']\n",
    "    lang = config['lang']\n",
    "    \n",
    "    pipeline = Pipeline(translator=translator,\n",
    "                    sent_connector=phi_sentence_connector,\n",
    "                    claim_splitter=None,\n",
    "                    evid_fetcher=offline_evid_fetcher,\n",
    "                    evid_selector=pipeline_models['finetuned']['evid_selector'],\n",
    "                    stm_verifier=pipeline_models['finetuned']['stm_verifier'],\n",
    "                    lang=lang)\n",
    "    outputs = evaluate_pipeline(pipeline, dataset, output_file_name=output_file_base_name.format(dataset=dataset_name), only_intro=False)               "
   ],
   "id": "68275aae2ea98bfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate gold labels on Fever (FeverScore)\n",
   "id": "5b07f26f32fc5455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: need to create a Testpipeline",
   "id": "4ae3127d7ea42214"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for name, models in pipeline_models.items():\n",
    "    print(f\"Evaluating {dataset_name} with {name}...\")\n",
    "    pipeline = Pipeline(translator=translator,\n",
    "                    sent_connector=phi_sentence_connector,\n",
    "                    claim_splitter=None,\n",
    "                    evid_fetcher=offline_evid_fetcher,\n",
    "                    evid_selector=models.get('evid_selector'),\n",
    "                    stm_verifier=models.get('stm_verifier'),\n",
    "                    lang='en')\n",
    "    evaluate_pipeline(pipeline, dataset, output_file_base_name.format(name=name))"
   ],
   "id": "c8ebb8939b563e43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
